{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressed Sensing (CS) based ECG compressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "TO BE WRITTEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal of Project\n",
    "\n",
    "TO BE WRITTEN\n",
    "\n",
    "Roadmap:\n",
    "- reproducing idea from paper bla bla bla\n",
    "- in general: study (\"emulate\", not really) best solution for a CS-based compressor for ECG to be used with remote-ECG-devices, small, limited storage capability, limited computational power:\n",
    "    - __phase 1__ compute dictionary $\\Psi$ and measurement matrix $\\Phi$ before actually using the device to measure the patient ecg\n",
    "    - __phase 2__ pass $\\Psi$, $\\Phi$ to the device, take __already compressed measurements__ $y$ (we'll see that this is core idea of CS)\n",
    "    - __phase 3__ store only $y$, $\\Psi$, $\\Phi$ and send them back to _more computationally powerful system_ where recovery happens\n",
    "- paper focuses also on how such hardware is built, we will be more generic\n",
    "- exploit data from Physionet.org exactly like the paper did\n",
    "- test different dictionaries, both _fixed dictionaries_ (_DCT_, _DWT_, _KL_) and _adaptive dictionary learning_ (_MOM_, _K-SVD_)\n",
    "- test how dimension of measurement matrix $\\Phi$ is related to processing speed in __phase 2__\n",
    "- test different _recovery methods_ (\"classic\" _l1-minimization_, _LASSO_, _Greedy Algorithms_, _Smooth-L0_, _Baisis Pursuit_), always use newly developed __Kronecker technique__ \n",
    "- testing robustness to noise with _additive noise_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: sparsity and compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: metrics and choices\n",
    "\n",
    "TO BE WRITTEN\n",
    "\n",
    "Roadmap:\n",
    "- also here choices strongly based on paper bla bla bla\n",
    "- _Compression rate (CR)_ of 75%, obtained directly \"by construction\" in the process (through _measurement matrix_)\n",
    "- _complexity of algorithm in recording_ directly bind to _power consumption_, very relevant for remote-ecg-recorders\n",
    "- _emergency ecg_ , such in ambulatory require _processing speed_, both during sampling and in reconstruction\n",
    "    - both previous point are related to _optimal choice of measurement matrix_, as it will be explained later\n",
    "- _accuracy_ through quantitive metrics: _PRD_, _SNR_\n",
    "    - especially important to evaulate different dictionaries adn different methods of reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: measurement matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO BE WRITTEN\n",
    "\n",
    "Roadmap:\n",
    "- Relation between $\\Phi$ dimension and speed of \"sensing phase\" (phase 2)\n",
    "- Relation between randomicity and RIP property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _fixed dictionaries_ vs _adaptive dictionary learning_\n",
    "TO BE WRITTEN\n",
    "\n",
    "---\n",
    "\n",
    "### Fixed dictionaries\n",
    "\n",
    "#### DCT\n",
    "TO BE WRITTEN\n",
    "\n",
    "#### DWT\n",
    "TO BE WRITTEN\n",
    "\n",
    "#### KL\n",
    "TO BE WRITTEN\n",
    "\n",
    "---\n",
    "\n",
    "### Adaptive Dictionary Learning\n",
    "TO BE WRITTEN\n",
    "\n",
    "#### MOM\n",
    "TO BE WRITTEN\n",
    "\n",
    "#### K-SVD\n",
    "TO BE WRITTEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: reconstruction of the signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction methods\n",
    "\n",
    "(we already explained the maths before, here is practival parts)\n",
    "\n",
    "#### \"classic\" _l1-minimization_ problem\n",
    "TO BE WRITTEN\n",
    "\n",
    "#### LASSO\n",
    "TO BE WRITTEN\n",
    "\n",
    "#### Greedy Algorithms\n",
    "TO BE WRITTEN\n",
    "\n",
    "#### Smooth-L0\n",
    "TO BE WRITTEN\n",
    "\n",
    "#### Basis Pursuit\n",
    "TO BE WRITTEN\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kronecker Techinque\n",
    "\n",
    "TO BE WRITTEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code implementation (python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "__Pre-sampling phase__\n",
    "\n",
    "- Generate dictionary $\\Psi$\n",
    "- Generate Measurement Matrix $\\Phi$\n",
    "\n",
    "---\n",
    "__Sampling phase__\n",
    "- $y = \\Phi \\Psi s$, simulate what would happen on device (_compute block by block, later concatenate results_)\n",
    "\n",
    "---\n",
    "__Recovery phase__\n",
    "- Reconstruct signal\n",
    "\n",
    "---\n",
    "__Evaluate result__\n",
    "1. Sampling faster for smaller $\\Phi$?\n",
    "2. Which dictionary are the best?\n",
    "3. Which recovery method is best?\n",
    "4. Robustness to noise?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First step of __Pre-sampling phase__\n",
    "- Develop code that outputs a dictionary $\\Psi \\in \\mathbb{R}^{d \\times d}$\n",
    "- $d$ will be the block dimension. \n",
    "- Code will be able to create the dictionary with one of the methods\n",
    "    - Fixed dictionaries: DCT, DWT, KL\n",
    "    - Adaptive Dictionary Learning: MOM, K-SVD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pywt\n",
    "\n",
    "def create_wavelet_basis(N, wavelet_name='db4'):\n",
    "    \"\"\"\n",
    "    Create the wavelet basis matrix for a given signal dimension and wavelet type.\n",
    "\n",
    "    Parameters:\n",
    "    N (int): The dimension of the signal.\n",
    "    wavelet_name (str): The name of the wavelet (default is 'db4').\n",
    "\n",
    "    Returns:\n",
    "    waveletBasis (ndarray): The combined basis matrix, where the first N/2 rows are the\n",
    "                            approximation basis and the second N/2 rows are the detail basis.\n",
    "    \"\"\"\n",
    "    # Load the specified wavelet\n",
    "    wavelet = pywt.Wavelet(wavelet_name)\n",
    "    \n",
    "    # Extract the scaling (low-pass) and wavelet (high-pass) filter coefficients\n",
    "    h = wavelet.dec_lo\n",
    "    g = wavelet.dec_hi\n",
    "    \n",
    "    # Length of the filter\n",
    "    L = len(h)\n",
    "    \n",
    "    # Initialize matrices to store the basis vectors\n",
    "    Phi = np.zeros((N//2, N))\n",
    "    Psi = np.zeros((N//2, N))\n",
    "    \n",
    "    # Construct the approximation (Phi) and detail (Psi) basis matrices\n",
    "    for i in range(N//2):\n",
    "        Phi[i, i*2:i*2+L] = h\n",
    "        Psi[i, i*2:i*2+L] = g\n",
    "    \n",
    "    # Combine Phi and Psi into a single matrix\n",
    "    waveletBasis = np.vstack((Phi, Psi))\n",
    "    \n",
    "    return waveletBasis\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: measurement matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Second step of __Pre-sampling phase__\n",
    "- Simple code to produce a $\\Phi \\in \\mathbb{R}^{d/4 \\times d}$ measurement matrix\n",
    "- Coefficient of $\\Phi$ will be random (_Bernoulli_) extraction of $-1,1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: sampling phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Only step of __Sampling phase__ \n",
    "- Compute $y_{j} = \\Phi \\Psi s_{j}$ on each $j$-th block of signal of dimension $d$\n",
    "- Progressively attach new $y_{j}$ to previous unitil you obtain $y$ _compressed measurement_ (of whole signal $s$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Only step of __Recovery phase__\n",
    "- Code that will be able to use _one of the selected methods_ to reconstruct an apporoximation of original signal $s$ from the _compressed measurement_ $y$\n",
    "    - $l1$-minimization\n",
    "    - LASSO\n",
    "    - Greedy\n",
    "    - Smooth-L0\n",
    "    - Basis Pursuit\n",
    "- __All methods (if possible) must exploit Kronecker Technique__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data: MITâ€“BIH Arrhythmia Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "USE THE ONE IN THE PREVIOUS NOTEBOOK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: test for best dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test all dictionaries to show with big number of data which are the one to perform better (we expect DWT to be the best of fixed, and that adaptive are better than fixed in general)\n",
    "- __#Records:__ Test on __MULTIPLE patients__ records is a MUST, especially to show that adaptive are better \n",
    "- __#Dictionaries:__ Test all dictionaries, that's what we are doing ...\n",
    "- __#ReconstructionMethods:__ Test with __only one reconstruction method__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: test for best reconstruction method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test to find which recontruction method is the best\n",
    "- __#Records:__ Test on a __single patient__ record should be fine \n",
    "- __#Dictionaries:__ Test with __a single dictionary type__\n",
    "- __#ReconstructionMethods:__ Test with __all reconstruction methods__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: test for correct block dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test to show that __sampling phase process speed is inversly proportional to block dimension__\n",
    "- __#Records:__ Test on a __single patient__ record should be fine \n",
    "- __#Dictionaries:__ Test with __a single dictionary type__ (USE BEST!)\n",
    "- __#ReconstructionMethods:__ Test with __only one reconstruction method__ (USE BEST!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: test for noise robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test if robust to noise by adding noise to the signal\n",
    "- __#Records:__ Test on a __single patient__ record should be fine \n",
    "- __#Dictionaries:__ Test with __a single dictionary type__ (USE BEST!)\n",
    "- __#ReconstructionMethods:__ Test with __only one reconstruction method__ (USE BEST!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO BE WRITTEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO BE WRITTEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO BE WRITTEN?\n",
    "\n",
    "For instance explanation of what ecg is"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".namlVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
