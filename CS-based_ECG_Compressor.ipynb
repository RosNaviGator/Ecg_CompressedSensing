{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressed Sensing (CS) based ECG compressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "TO BE WRITTEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal of Project\n",
    "\n",
    "TO BE WRITTEN\n",
    "\n",
    "Roadmap:\n",
    "- reproducing idea from paper bla bla bla\n",
    "- in general: study (\"emulate\", not really) best solution for a CS-based compressor for ECG to be used with remote-ECG-devices, small, limited storage capability, limited computational power:\n",
    "    - __phase 1__ compute dictionary (only for adaptive dictionaries) $\\Psi$ and measurement matrix $\\Phi$ before actually using the device to measure the patient ecg\n",
    "    - __phase 2__ pass $\\Psi$, $\\Phi$ to the device, take __already compressed measurements__ $y$ (we'll see that this is core idea of CS)\n",
    "    - __phase 3__ store only $y$, $\\Psi$, $\\Phi$ and send them back to _more computationally powerful system_ where recovery happens\n",
    "- paper focuses also on how such hardware is built, we will be more generic\n",
    "- exploit data from Physionet.org exactly like the paper did\n",
    "- test different dictionaries, both _fixed dictionaries_ (_DCT_, _DWT_, _KL_) and _adaptive dictionary learning_ (_MOM_, _K-SVD_)\n",
    "- test how dimension of measurement matrix $\\Phi$ is related to processing speed in __phase 2__\n",
    "- test different _recovery methods_ (\"classic\" _l1-minimization_, _LASSO_, _Greedy Algorithms_, _Smooth-L0_, _Baisis Pursuit_), always use newly developed __Kronecker technique__ \n",
    "- testing robustness to noise with _additive noise_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: sparsity and compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Sparsity\n",
    "__Theoretical Sparsity__\n",
    "\n",
    "A signal $s \\in \\mathbb{R}^n$ is considered $k$-sparse if it has exactly $k$ non-zero elements, with $k \\ll n$. This means that $n-k$ elements of the signal are exactly zero.\n",
    "$$\n",
    "s = \\begin{pmatrix} s_1 \\\\ s_2 \\\\ \\vdots \\\\ s_n \\end{pmatrix}\n",
    "$$\n",
    "where exactly $k$ elements in $s$ are non-zero, and the remaining $n-k$ elements are zero.\n",
    "\n",
    "\n",
    "__Practical Sparsity__\n",
    "\n",
    "In real-world signals, _exact sparsity is rare_. Instead, signals are often _representable_ (see next section) as __approximately sparse__: only $k$ elements _of the sparse representation_ are significant and carry most of the signal's information, the remaining $n-k$ elements have small, negligible values. \n",
    "\n",
    "The difference lies in the fact that the $n-k$ coefficients are small but not exactly zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Representation of Signals\n",
    "\n",
    "\"Most natural signals, such as images and audio, are highly compressible. This compressibility means that, when the signal is written in an appropriate basis, only a few modes are active, thus reducing the number of values that must be stored for an accurate representation. In other words, a compressible signal $x \\in \\mathbb{R}^n$ may be written as a sparse vector $s \\in \\mathbb{R}^n$ in a transform basis $\\Psi \\in \\mathbb{C}^{n \\times n}$:\n",
    "\n",
    "$$\n",
    "x = \\Psi s.\n",
    "$$\n",
    "\n",
    "If the basis $\\Psi$ is generic, such as the Fourier or wavelet basis, then only the few active terms in $s$ are required to reconstruct the original signal $x$, reducing the data required to store or transmit the signal.\" [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classic Transformation-Based Compression\n",
    "\n",
    "A typical transformation-based compression algorithm involves the following steps:\n",
    "\n",
    "1. __Signal capture__: \n",
    "    Fully sense a whole __raw__ signal $x$ and store it. In this project $x$ are the _voltages_ measured by the ECG machine.\n",
    "2. __Transformation to a sparse domain__:\n",
    "    The signal $x$ is transformed to a sparse domain, basically we want to find the sparse vector $s \\in \\mathbb{R}^n$, that contain mostly negligible coefficients.\n",
    "\n",
    "    We exploit $\\Psi \\in \\mathbb{C}^{n \\times n}$ orthogonal basis matrix, also called __dictionary__. Being $\\Psi$ an orthonormal basis, it satisfies $\\Psi^H \\Psi = I$, where $\\Psi^H$ is the Hermitian conjugate (conjugate transpose) of $\\Psi$, and $I$ is the identity matrix. This implies that $\\Psi^{-1} = \\Psi^H$, making the transformation and its inverse straightforward.\n",
    "\n",
    "    Therefore, when $ \\Psi $ is an orthonormal basis, applying $ \\Psi^H $ to the signal effectively inverts the transformation applied by $ \\Psi $, we can use this to obtain sparse representation from original signal:\n",
    "\n",
    "    $$\n",
    "    s = \\Psi^H x\n",
    "    $$\n",
    "\n",
    "    __The use of transforms__:\n",
    "    On a mathematical note: $\\Psi$ is an orthonormal basis composed of functions like Fourier Function, Wavelet, and so on.\n",
    "    The actual computation of $s$ doesn't actually build a dictionary $\\Psi$ to invert and multiplicate to the signal. Instead it directly applies the _transform_ (e.g. FFT, DWT, DCT, ...) to the signal $x$, to immediately obtain _sparse representation_ $s$.\n",
    "\n",
    "3. __Sparsification__: \n",
    "    A fundamental concept is that a threshold is applied to the coefficients, retaining only those that are significant (i.e., above the threshold) and discarding the rest.\n",
    "\n",
    "    A more detailed view reveals that these steps can be performed using a wide range of techniques, depending on the transform employed, and equivalently, on the choice of dictionary.\n",
    "\n",
    "    _This will not be explored as it is not the subject of this project, it's a vast and intresting topic, Brunto&Kutz book in reference provide a good reference to explore more..._\n",
    "\n",
    "4. __Encoding__:\n",
    "    The retained coefficients and their positions are then encoded for storage or transmission. \n",
    "\n",
    "    _Another huge chapter that will not be explored here, again you can refer to the referenced book for more_\n",
    "\n",
    "__Complexity__\n",
    "\n",
    "Such methods can be _extremely_ effective, but they require a _thresholding/sparsification_ step, which introduces non-linearity and computational complexity. \n",
    "\n",
    "In the following is shown that CS-based methods can provide an alternative solution with different advantages...\n",
    "\n",
    "<center>\n",
    "    <img src=\"./.img/MethodsComparison.png\" alt=\"MethodsComparison.png\" width=\"600\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressed Sensing (CS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Mathematically, compressed sensing exploits the _sparsity of a signal_ in a __generic basis__ to achieve full signal reconstruction from surprisingly few measurements.\n",
    "\n",
    "If a __signal $x$ is k-sparse in $\\Psi$ (it's a requirement),__ then instead of measuring $x$ directly (n measurements) and then compressing, it is possible to collect dramatically fewer randomly chosen or compressed measurements and then solve for the non-zero elements of s in the transformed coordinate system.\" [2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measurement\n",
    "\n",
    "Instead of acquiring all $n$ samples, a reduced set of $m$ measurements is obtained directly by projecting the signal $x$  onto a measurement matrix $\\Phi$, storing a _compressed measurement_ $y$:\n",
    "\n",
    "$$\n",
    "y = \\Phi x\n",
    "$$\n",
    "\n",
    "\n",
    "where:\n",
    "- $x \\in \\mathbb{R}^n$ _real_ signal coming from sensors\n",
    "- $y \\in \\mathbb{R}^m$ _compressed measurement_\n",
    "- $\\Phi \\in \\mathbb{R}^{m \\times n}$ with $m \\ll n$ is the _measurement matrix_.\n",
    "\n",
    "__Key concept__:\n",
    "\n",
    "In the measurement phase the _sparse representation_ $s$ is __not__ computed, we directly apply the _measurement matrix_ to the _real_ signal $x$. \n",
    "\n",
    "$\\Phi$ does not simply \"select\" $m$ out of $n$ coefficients out of $x$. Instead, $\\Phi$ typically contains random or structured elements that ensure the measurements $y$ retain sufficient information to __later recover__ the sparse signal $s$. \n",
    "\n",
    "Although the _signal $x$ itself is not sparse in the time domain_, __compressed sensing theory exploits the fact that $s$ can be sparsely represented in some transform domain__ (e.g., wavelet or Fourier domain).\n",
    "\n",
    "_Measurement matrix topic is explored some chapters ahead_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovery\n",
    "\n",
    "With knowledge of $s \\in \\mathbb{R}^n$ _sparse representation_ of $x$ through $\\Psi$ _dictionary_, it is possible to recovery $x$ itself as previously shown with:\n",
    "$$\n",
    "x = \\Psi s\n",
    "$$\n",
    "\n",
    "Thus the goal of compressed sensing is to find the __sparsest__ vector $s$ that is consistent with:\n",
    "\n",
    "$$\n",
    "y = \\Phi x = \\Phi \\Psi s\n",
    "$$\n",
    "\n",
    "where (again):\n",
    "- $x \\in \\mathbb{R}^n$ _real_ signal coming from sensors\n",
    "- $y \\in \\mathbb{R}^m$ _compressed measurement_\n",
    "- $\\Psi \\in \\mathbb{R}^{n \\times n}$ is the _dictionary_ (same as explained in previous section)\n",
    "- $\\Phi \\in \\mathbb{R}^{m \\times n}$ with $m \\ll n$ is the _measurement matrix_.\n",
    "- $s \\in \\mathbb{R}^n$ is the _sparse representation_ of $x$ in $\\Psi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Non convex problem__\n",
    "\n",
    "\"Such system of equations is __under-determined__ since there are infinitely many consistent solution $s$. The __sparsest solution__ is the one that satisfies:\n",
    "\n",
    "$$\n",
    "\\hat{s} = \\arg_{s} \\min \\|s\\|_0 \\text{ subject to } y = \\Phi \\Psi \\alpha\n",
    "$$\n",
    "\n",
    "where $\\min \\|s\\|_0$ denotes the $\\ell_0$-pseudo-norm, given by the _non-zero entries_, also referred as the _cardinality_ of $s$.\n",
    "\n",
    "The optimization is non-convex, and in general, the solution can only be found with a brute-force search that is combinatorial in $n$ and $K$. In particular, all possible $K$-sparse vectors in $\\mathbb{R}^n$ must be checked; if the exact level of sparsity $K$ is unknown, the search is even broader. Because this search is combinatorial, solving such minimization is intractable for even moderately large $n$ and $K$, and the prospect of solving larger problems does not improve with Moore’s law of exponentially increasing computational power.\"[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convex equivalent problem__\n",
    "\n",
    "Fortunately, under certain conditions on the measurement matrix $\\Phi$, it is possible to relax the optimization to a convex $\\ell_1$-minimization.\n",
    "\n",
    "$$\n",
    "\\hat{s} = \\arg_{s} \\min \\|s\\|_1 \\text{ subject to } y = \\Phi \\Psi \\alpha\n",
    "$$\n",
    "\n",
    "__In the presence of noise__, the recovery problem is modified to:\n",
    "\n",
    "$$\n",
    "\\hat{s} = \\arg_{s} \\min \\|s\\|_1 \\text{ subject to } \\|y - \\Phi \\Psi s\\|_2 \\leq \\epsilon\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is a bound on the noise level.\n",
    "\n",
    "There are very specific conditions that must be met for the $\\ell_1$-minimization to converge with high probability to the sparsest solution of $\\ell_0$-minimization. They can be summarized as follows:\n",
    "- __Incoherence__: \n",
    "    A critical concept in compressed sensing is the _incoherence_ between the measurement matrix $\\Phi$ and the dictionary $\\Psi$. Incoherence refers to the property that ensures that the rows of $\\Phi$ are not too similar to the columns of $\\Psi$. This incoherence is vital because it allows the sparse information in the signal $x$ (which is represented in the domain of $\\Psi$) to be evenly spread across the measurements $y$. This spreading ensures that no single measurement in $y$ captures too much or too little information about the signal $x$, which is essential for accurate recovery of the sparse signal $s$ from the measurements $y$.\n",
    "\n",
    "- __Recoverability Condition:__ \n",
    "    A $K$-sparse signal $s \\in \\mathbb{R}^n$ can be properly recovered after Compressive Sensing (CS) if the number of measurements $m$ satisfies:\n",
    "\n",
    "    $$\n",
    "    m \\geq C K \\log\\left(\\frac{n}{K}\\right)\n",
    "    $$\n",
    "\n",
    "    where $C$ is a constant that depends on how __incoherent__ $\\Phi$ and $\\Psi$ are. This condition ensures that enough measurements are taken to accurately recover the sparse signal, accounting for both sparsity and the ambient dimension $n$.\n",
    "\n",
    "    The recoverability condition is a practical guideline that tells you how many measurements $m$ you need to take to ensure that a $k$-sparse signal $s \\in \\mathbb{R}^n$ can be recovered accurately. The $\\log\\left(\\frac{n}{k}\\right)$ term accounts for the dimensionality reduction that occurs when mapping an $n$-dimensional signal into an $m$-dimensional measurement space.\n",
    "\n",
    "\"Roughly speaking, these two conditions guarantee that the matrix $\\Phi |Psi$ acts as a unitary transformation on K-sparse vectors $s$, preserving relative distances between vectors and enabling almost certain signal reconstruction with $\\ell_1$ convex minimization. This is formulated precisely in terms of the restricted isometry property (RIP) that follows.\"[2]\n",
    "\n",
    "- __Restricted Isometry Property (RIP):__\n",
    "    \"The RIP is a property of the matrix $A = \\Phi \\Psi$ that provides a condition under which the matrix will behave well with respect to sparse signals. Specifically, for a matrix $A$ to satisfy the RIP of order $k$ with a constant $\\delta_k$, it must hold that:\n",
    "\n",
    "    $$\n",
    "    (1 - \\delta_k) \\|x\\|_2^2 \\leq \\|A x\\|_2^2 \\leq (1 + \\delta_k) \\|x\\|_2^2\n",
    "    $$\n",
    "\n",
    "    for all $k$-sparse vectors $x$. Here, $\\delta_k$ is the smallest constant such that this inequality holds, and it should be close to zero. This ensures that the matrix $A$ approximately preserves the Euclidean length (and hence the geometry) of all $k$-sparse signals, meaning the measurements are nearly isometric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: main aspects of study and evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspects relevant to the study\n",
    "\n",
    "__Work on signal block__\n",
    "\n",
    "ECG provide continuous data sampling, a record length can vary based on why it is being taken from few minutes, to hours, to days. This work addresses small devices, that will take a number of samples that can vary between 16 and 1024 as __signal block to compress__.\n",
    "\n",
    "__Compression ratio (CR)__\n",
    "\n",
    "\"Important factor for evaluating different methods. CR as follow \n",
    "$$\n",
    "CR(\\%) = 100 \\frac{n - m}{n}\n",
    "$$\n",
    "where $m$ and $n$ are the number of compressed and original samples, respectively. \"[1]\n",
    "\n",
    "__Compression algorithm’s complexity__\n",
    "\n",
    "Very relevant \"when we talk about limited and weak ECG-recorders. The power consumption usually has a linear relation with the complex-ity of systems. Supplying the power for 24-h ambulatory or remote ECG recorders is very important, that encourage \n",
    "us to focus on systems that have low power consumption.\"[1]\n",
    "\n",
    "The focus here is especially on _sampling phase_: one of the goal of the project will be to demonstrate, same as they did in the paper, that a smaller _measurement matrix_ will result in a _more efficient sampling phase_.\n",
    "\n",
    "__Processing speed__\n",
    "\n",
    "\"In emergency situations it will be important. Considering the ambulatory ECG recorders, whatever the data sooner to be presented to a physician, the next orders from a physician can be given sooner as well.\"\n",
    "\n",
    "Here it must be also taken into account the _reconstruction complexity_, in order to provide _usable_ ECG data, it's necessary to be fast both in acquirin and processing the data.\n",
    "\n",
    "_In this work are reproposed the same fundamental metrics and evaluation aspects proposed in the [1] Izadi, V., Shahri, P.K., & Ahani, H. (2020) paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics to Assess the Accuracy of Reconstructed Signal\n",
    "\n",
    "The accuracy of the reconstructed signal in ECG compression algorithms is typically evaluated using two common metrics: the Percentage Root Mean Square Difference (PRD) and Signal-to-Noise Ratio (SNR). These metrics are defined as follows:\n",
    "__Percentage Root Mean Square Difference (PRD)__\n",
    "The PRD is a measure of the difference between the original ECG signal and the reconstructed ECG signal. It is calculated using the following equation:\n",
    "\n",
    "$$\n",
    "\\text{PRD} = 100 \\times \\sqrt{\\frac{\\sum_{i=0}^{N-1} (x(n) - \\hat{x}(n))^2}{\\sum_{i=0}^{N-1} x(n)^2}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x(n)$ is the original ECG signal.\n",
    "- $\\hat{x}(n)$ is the reconstructed ECG signal.\n",
    "- $N$ is the length of the signal.\n",
    "\n",
    "__Signal-to-Noise Ratio (SNR)__\n",
    "The SNR is another measure used to assess the quality of the reconstructed signal. It is calculated from the PRD using the following equation:\n",
    "\n",
    "$$\n",
    "\\text{SNR} = -20 \\log_{10} \\left(\\frac{\\text{PRD}}{100}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Assessment Based on PRD and SNR\n",
    "\n",
    "Table 1 from the referenced paper classifies the quality of the reconstructed signal based on the PRD and corresponding SNR values:\n",
    "\n",
    "| Quality        | PRD Range      | SNR Range       |\n",
    "|----------------|----------------|-----------------|\n",
    "| Very Good      | 0% < PRD < 2%  | SNR > 33 dB     |\n",
    "| Good           | 2% < PRD < 9%  | 20 dB < SNR < 33 dB |\n",
    "| Undetermined   | PRD ≥ 9%       | SNR ≤ 20 dB     |\n",
    "\n",
    "This table indicates that when the PRD is less than 2%, the quality of the reconstructed signal can be categorized as \"Very Good.\" For PRD values between 2% and 9%, the quality is considered \"Good,\" and for PRD values above 9%, the quality of the reconstructed signal cannot be precisely determined. __In this study the same metric will be adopted__.\n",
    "\n",
    "_Table based on [1] Izadi, V., Shahri, P.K., & Ahani, H. (2020). A compressed-sensing-based compressor for ECG. *Biomedical Engineering Letters*, 10, 299–307. https://doi.org/10.1007/s13534-020-00148-7_\n",
    "\n",
    "_More information on how such measure was established in APPENDIX_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: measurement matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously mentioned ant _ECG device_ provide continuous data sampling for consectuive hours, for instance MIT-BIH Arrhythmia Database provides _records_ for each patient for about $30$ consecutive hours, sampled at $360$ $samples/second$. This means that each _record_ has about $650000$ samples.\n",
    "\n",
    "Exploiting _Compressed Sensing_ allows to store only a fraction of such data by immediately computing the _compress measurement_, this work won't delve into the __hardware__ specifics, the [1] Izadi, V., Shahri, P.K., & Ahani, H. (2020) paper provides a possible hardware implementation.\n",
    "\n",
    "What is important to understand in the present study is that on an ECG signal CS-based approach works on _groups of consecutive samples_ within a _record_, each _\"group\"_ is a __signal blocks__.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressing blocks of samples within signal\n",
    "\n",
    "For the whole signal we understood that CS performs:\n",
    "$$\n",
    "y = \\Phi x\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x \\in \\mathbb{R}^n$ _real_ signal coming from sensors\n",
    "- $y \\in \\mathbb{R}^m$ _compressed measurement_\n",
    "- $\\Phi \\in \\mathbb{R}^{m \\times n}$ with $m \\ll n$ is the _measurement matrix_.\n",
    "\n",
    "In the practical case \"the signal $x$\" becomes the single __signal block__ $y_{block} \\in \\mathbb{R}^d$, where $d$ is the _block size_.\n",
    "\n",
    "$$\n",
    "y_{block} = \\Phi_{p,d} \\cdot x_{block}\n",
    "$$\n",
    "\n",
    "Where $p \\ll d$, $\\Phi_{p,d}$ will reduce $d$ _original samples_ to $p$ samples that compose _compressed measurement_ __for that single block__. \n",
    "\n",
    "__Compressed measurement $y$ of the whole signal is then obtained by simply concatenating previous results__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How the measurement matrix is generated\n",
    "\n",
    "In the present work, elements of $\\Phi_{p,d}$ are drawn from a Bernoulli distribution, which is a discrete probability distribution.\n",
    "\n",
    "For each element $\\phi_{ij}$ of the matrix $\\Phi$, a random value is generated that is either $+1$ or $-1$ with equal probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How to Check that Restricted Isometry Property holds__\n",
    "\n",
    "As previously explained in the theoretical review the Restricted Isometry Property (RIP) is crucial in ensuring that compressed sensing can accurately recover sparse signals from a reduced set of measurements. \n",
    "\n",
    "However checking whether a specific matrix $A$ satisfies the RIP is computationally infeasible for large matrices because it would involve verifying this condition across all possible sparse vectors. \n",
    "\n",
    "Despite this difficulty, generating the measurement matrix $\\Phi$ randomly ensures that $A = \\Phi \\Psi$ is very likely to satisfy the RIP.__*__ This inherent randomness provides a strong theoretical basis for the effectiveness of compressed sensing without the need for direct verification of the RIP.[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How many measurement matrices?__\n",
    "\n",
    "The project aims to emulate what would be possible to do on a very small device with __limited computation capabilities and storage capacity__, to find the best methods in such system. \n",
    "\n",
    "Even though it's technically possible to use a different random _measurement matrix_ for each __signal block__, the idea is to use the same $\\Phi$ for the whole signal, as it would be advisable to do in a portable ECG device.\n",
    "\n",
    "Otherwise it'd be necessary to store a measurement matrix for each block, and reassociate it to such block during _recovery phase_. It would not be the best way to proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Choice of block dimension__\n",
    "\n",
    "One of the goals of this study is to demonstrate that a smaller value of $d$, hence a smaller _measurement matrix_ $\\Phi$ will result in a more efficient (time complexity-wise) compression process.\n",
    "\n",
    "In the next code block we choose a very small __block size__ $d$, in the last part of the project we will provide an actual study of the relation between the processing time and the __block size__, experiment with different matrices: $\\Phi_{4,16}, \\Phi_{8,32}, \\Phi_{16,64}, \\Phi_{32,128}, \\Phi_{64,256}, \\Phi_{128,512}, \\Phi_{256,1024}$\n",
    "\n",
    "__By construction the method will produce a 75% Compression Ratio__ due to the dimension choosen for $\\Phi_{p,d}$ because $p = d/4$ in all cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: reconstruction of the signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries\n",
    "\n",
    "#### _fixed dictionaries_ vs _adaptive dictionary learning_\n",
    "\n",
    "\"\n",
    "Decreasing  the  projection  matrixs  size  will  affect  the order of sparsity. There are two different classes of sparsifying bases: first class is fixed dictionaries such as wavelet transform dictionary or discrete cosine transform (DCT).\n",
    "The second class is adaptive dictionaries that usually present better sparse representation. There are various adaptive dictionary learning algorithms, such as the method of optimal direction  (MOD)  [19],  and  K  singular  value  decomposition (K-SVD) [20] which can present efficient sparsifying dictionary if the training set has been selected accurately. \n",
    "For the case of wearable ECG recorders that are used by a patient, after training a dictionary, the probability of major \n",
    "change in ECG data of patients is low; hence adaptive sparsifying  dictionary  methods  can  be  applied  to  produce  a  more efficient sparsifying dictionary. Since the sparsity has a direct relation with the quality of the reconstructed signal, it leads to compensate for the effect of decreasing the length of the projection matrix. In this work, adaptive dictionary learning is used for the ECG signal, and the result shows that it can be a well alternative to the fixed dictionaries used by previous researches.\"[1]\n",
    "\n",
    "This idea is reproposed in the present work.\n",
    "\n",
    "#### Fixed dictionaries\n",
    "In this work the _DCT_, _DWT_ __fixed dictionaries__ are utilized as a benchmark to test how dictionary learning can improve reconstruction.\n",
    "\n",
    "It's assumed that the reader possesses the necessary knowledge about the topic. This are well known methdos employed in the _signal compression \"world\"_.\n",
    "An overview is provided at the end of the document in the __Appendix__.\n",
    "\n",
    "\n",
    "#### Adaptive Dictionary Learning\n",
    "\n",
    "__MOM__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classic solution in CS is to solve the $\\ell_1$-minimization problem, equivalent to $\\ell_0$-minimization problem. In real cases is always a good idea to considere presence of noise, hence the problem is:\n",
    "$$\n",
    "\\hat{s} = \\arg_{s} \\min \\|s\\|_1 \\text{ subject to } \\|y - \\Phi \\Psi s\\|_2 \\leq \\epsilon\n",
    "$$\n",
    "where $\\epsilon$ is a bound on the noise level.\n",
    "\n",
    "_Many alternatives can be found, two are presented here. Other possibilities are introduced in the __Future Developements__ section._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__LASSO Compressed Sensing__\n",
    "\n",
    "Alternatively, the signal can be recovered using the LASSO formulation, which balances the $\\ell_1$ norm of the signal with the fidelity to the measurements:\n",
    "\n",
    "$$\n",
    "\\hat{s} = \\arg \\min \\left(\\frac{1}{2} \\|y - \\Phi \\Psi s\\|_2^2 + \\lambda \\|s\\|_1 \\right)\n",
    "$$\n",
    "\n",
    "Here, $\\lambda$ is a regularization parameter that controls the trade-off between the sparsity of the solution and the accuracy of the reconstruction. A larger $\\lambda$ emphasizes sparsity, while a smaller $\\lambda$ emphasizes data fidelity.\n",
    "\n",
    "Note that $LASSO$ formulation already accounts for noise using the $\\ell_2$ minimization on $\\|y - \\Phi \\Psi s\\|_2^2$, instead of enforcing the condition $y = \\Phi \\Psi s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Smooth-L0__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kronecker Techinque\n",
    "\n",
    "TO BE WRITTEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code implementation (python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "__Sampling phase__\n",
    "- $y = \\Phi x$, simulate what would happen on device (_compute block by block, later concatenate results_)\n",
    "\n",
    "---\n",
    "\n",
    "__Recovery phase__\n",
    "- Reconstruct $\\hat{s}$, approximation of $s$ _sparse representation_ of $x$\n",
    "- Use dictionary $\\Psi$ to retrieve $\\hat{x}$ approximation of $x$ \n",
    "\n",
    "--- \n",
    "\n",
    "__Evaluate result__\n",
    "1. Sampling faster for smaller $\\Phi$?\n",
    "2. Which dictionary are the best? (How fast, how accurate)\n",
    "3. With Kronecker vs without\n",
    "4. ~~Which recovery method is best? (How fast, how accurate)~~ only use Smooth-L0\n",
    "5. ~~Robustness to noise?~~\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: sampling phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement matrix $\\Phi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_measurement_matrix(rows, cols):\n",
    "    \"\"\"\n",
    "    Generates a random measurement matrix with the specified number of rows and columns.\n",
    "\n",
    "    Parameters:\n",
    "    - rows (int): The number of rows in the measurement matrix.\n",
    "    - cols (int): The number of columns in the measurement matrix.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The generated measurement matrix with shape (rows, cols).\n",
    "\n",
    "    Description:\n",
    "    This function generates a random measurement matrix with the specified number of rows and columns. \n",
    "    The measurement matrix is created by randomly choosing either -1 or 1 for each element in the matrix.\n",
    "    The resulting matrix is returned as a numpy array.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.random.choice([-1, 1], size=(rows, cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal Sampling $y = \\Phi x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_compressed_measurement(signal, signal_block_dimension):\n",
    "    \"\"\"\n",
    "    Computes the compressed measurement of a signal using compressed sensing.\n",
    "\n",
    "    Parameters:\n",
    "    - signal (numpy.ndarray): The input signal.\n",
    "    - signal_block_dimension (int): The dimension of each signal block.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The compressed measurement of the signal.\n",
    "    - numpy.ndarray: The measurement matrix used for compression.\n",
    "\n",
    "    Description:\n",
    "    This function computes the compressed measurement of a signal using compressed sensing.\n",
    "    It generates a measurement matrix with the specified number of columns based on the signal block dimension.\n",
    "    Then, it computes the compressed measurement block by block and concatenates the results to obtain the compressed measurement of the whole signal.\n",
    "    The resulting compressed measurement and measurement matrix are returned as numpy arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of blocks\n",
    "    num_blocks = len(signal) // signal_block_dimension\n",
    "\n",
    "    # To a achieve 75% compression rate: the number of rows in the measurement matrix is set to 1/4 of the signal block dimension\n",
    "    NUM_OF_ROWS = signal_block_dimension // 4\n",
    "\n",
    "    # Generate measurement matrix\n",
    "    measurement_matrix = generate_measurement_matrix( NUM_OF_ROWS, signal_block_dimension)\n",
    "\n",
    "    # Initialize the compressed measurement\n",
    "    compressed_measurement = np.empty(0)\n",
    "\n",
    "    # Compute the compressed measurement block by block\n",
    "    for i in range(num_blocks):\n",
    "        \n",
    "        # Get the current block of the signal\n",
    "        block = signal[i * signal_block_dimension : (i + 1) * signal_block_dimension]\n",
    "\n",
    "        # Compute the compressed measurement block\n",
    "        compressed_measurement_block = np.dot(measurement_matrix, block)\n",
    "\n",
    "        # Concatenate the compressed measurement block to the compressed measurement\n",
    "        compressed_measurement = np.concatenate((compressed_measurement, compressed_measurement_block))\n",
    "\n",
    "    return compressed_measurement, measurement_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: recovery phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only two things needed out of the _sampling phase_ are $y$ and $\\Phi$\n",
    "\n",
    "Next step is to solve:\n",
    "$$\n",
    "\\hat{s} = \\arg_{s} \\min \\|s\\|_1 \\text{ subject to } y = \\Phi \\Psi \\alpha\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix $\\Theta = \\Phi \\Psi$ for fixed dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's necessary to compute $\\Theta = \\Phi \\Psi$\n",
    "- It's not efficient to compute the whole base $\\Psi$, it's preferable a _matrix-free_ approach\n",
    "- We should allow choosing various __fixed dictionaries__\n",
    "- We need to store what transform we used in order to successfully reconstruct in later steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.fftpack import idct\n",
    "from pywt import wavedec, waverec\n",
    "\n",
    "def compute_theta(phi, transform_type='dwt', wavelet='db4'):\n",
    "    \"\"\"\n",
    "    Computes the matrix Theta = Phi * Psi using a matrix-free approach with the DCT or DWT as the basis for Psi.\n",
    "    Also returns the transform type and wavelet type (if applicable).\n",
    "\n",
    "    Parameters:\n",
    "    - phi (numpy.ndarray): The measurement matrix Phi of shape (m, n).\n",
    "    - transform_type (str): The type of transform to use. Can be 'dwt' (default) or 'dct'.\n",
    "    - wavelet (str): The type of wavelet to use for DWT. Default is 'db4'.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (Theta, transform_type, wavelet_type)\n",
    "      - Theta (numpy.ndarray): The matrix Theta of shape (m, n), where Theta = Phi * Psi.\n",
    "      - transform_type (str): A string indicating the transform used ('dct' or 'dwt').\n",
    "      - wavelet_type (str or None): The wavelet name if DWT is used; None if DCT is used.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the number of columns of Phi, equal to signal block dimension\n",
    "    n = phi.shape[1]\n",
    "\n",
    "    # Initialize Theta as a zero matrix of shape (m, n)\n",
    "    m = phi.shape[0]\n",
    "    theta = np.zeros((m, n))\n",
    "\n",
    "    # Set the wavelet_type to None initially\n",
    "    wavelet_type = None\n",
    "\n",
    "    # Iteratively compute each column of Theta\n",
    "    for ii in range(n):\n",
    "        # Create the unit vector ek\n",
    "        ek = np.zeros(n)\n",
    "        ek[ii] = 1\n",
    "\n",
    "        # Compute the corresponding column of Psi using the chosen transform\n",
    "        if transform_type == 'dct':\n",
    "            psi_column = idct(ek, norm='ortho')\n",
    "        elif transform_type == 'dwt':\n",
    "            # Decompose the unit vector in the wavelet domain\n",
    "            coeffs = wavedec(ek, wavelet, level=1)\n",
    "            # Reconstruct the unit vector (basis vector) in the time domain\n",
    "            psi_column = waverec(coeffs, wavelet)[:n]\n",
    "            wavelet_type = wavelet  # Set the wavelet_type to the specified wavelet\n",
    "\n",
    "        # Compute the corresponding column of Theta\n",
    "        theta[:, ii] = np.dot(phi, psi_column)\n",
    "\n",
    "    return theta, transform_type, wavelet_type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\ell_{1}$-minimization standard solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "from scipy.fftpack import idct\n",
    "from pywt import waverec\n",
    "\n",
    "def l1_minimization(y, Theta):\n",
    "    \"\"\"\n",
    "    Solves the L1-minimization problem to recover the sparse representation s from the compressed measurements y.\n",
    "    \n",
    "    Parameters:\n",
    "    y (numpy.ndarray): The compressed measurements.\n",
    "    Theta (numpy.ndarray): The combined matrix Theta (Phi * Psi) used during compression.\n",
    "    \n",
    "    Returns:\n",
    "    s_recovered (numpy.ndarray): The recovered sparse representation.\n",
    "    \"\"\"\n",
    "    n = Theta.shape[1]\n",
    "\n",
    "    # Objective function: Minimize the L1 norm of the sparse representation s\n",
    "    c = np.ones(2 * n)\n",
    "\n",
    "    # Equality constraints: Theta * s = y\n",
    "    A_eq = np.hstack([Theta, -Theta])\n",
    "    b_eq = y\n",
    "\n",
    "    # Variable bounds: s+ >= 0 and s- >= 0\n",
    "    bounds = [(0, None)] * (2 * n)\n",
    "\n",
    "    # Solve the linear program using linprog\n",
    "    result = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n",
    "\n",
    "    if result.success:\n",
    "        s_recovered = result.x[:n] - result.x[n:]\n",
    "        return s_recovered\n",
    "    else:\n",
    "        raise ValueError(\"L1-minimization did not converge\")\n",
    "\n",
    "\n",
    "def blockwise_l1_signal_reconstruction(y, Theta, transform_type, wavelet=None):\n",
    "    \"\"\"\n",
    "    Solves the L1-minimization problem block by block to find the sparsest s\n",
    "    that satisfies y = Theta * s, then inverts the transform to recover x.\n",
    "\n",
    "    Parameters:\n",
    "    - y (numpy.ndarray): The compressed measurements of the entire signal.\n",
    "    - Theta (numpy.ndarray): The measurement matrix Theta (Phi * Psi) for the blocks.\n",
    "    - transform_type (str): The type of transform used ('dct' or 'dwt').\n",
    "    - wavelet (str): The type of wavelet to use for DWT. Must be provided if 'dwt' is selected.\n",
    "\n",
    "    Returns:\n",
    "    - x_full (numpy.ndarray): The reconstructed signal x.\n",
    "    \"\"\"\n",
    "\n",
    "    if transform_type == 'dwt' and wavelet is None:\n",
    "        raise ValueError(\"Wavelet type must be provided when using 'dwt' as the transform_type.\")\n",
    "\n",
    "    num_blocks = len(y) // Theta.shape[0]\n",
    "    x_blocks = []\n",
    "\n",
    "    for i in range(num_blocks):\n",
    "        y_block = y[i * Theta.shape[0]: (i + 1) * Theta.shape[0]]\n",
    "        s_block = l1_minimization(y_block, Theta)\n",
    "\n",
    "        # Apply the inverse transform to recover the block x_block\n",
    "        if transform_type == 'dct':\n",
    "            x_block = idct(s_block, norm='ortho')\n",
    "        elif transform_type == 'dwt':\n",
    "            # Perform wavelet reconstruction\n",
    "            coeffs = [s_block] + [np.zeros_like(s_block) for _ in range(len(wavelet))]\n",
    "            x_block = waverec(coeffs, wavelet)[:len(s_block)]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown transform type: {transform_type}\")\n",
    "\n",
    "        x_blocks.append(x_block)\n",
    "\n",
    "    x_full = np.concatenate(x_blocks)\n",
    "    return x_full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: reconstructed signal evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_prd(original_signal, reconstructed_signal):\n",
    "    \"\"\"\n",
    "    Calculate the Percentage Root Mean Square Difference (PRD).\n",
    "    \n",
    "    Parameters:\n",
    "    original_signal (np.array): The original ECG signal.\n",
    "    reconstructed_signal (np.array): The reconstructed ECG signal.\n",
    "    \n",
    "    Returns:\n",
    "    float: The PRD value as a percentage.\n",
    "    \"\"\"\n",
    "    numerator = np.sum((original_signal - reconstructed_signal) ** 2)\n",
    "    denominator = np.sum(original_signal ** 2)\n",
    "    prd = 100 * np.sqrt(numerator / denominator)\n",
    "    return prd\n",
    "\n",
    "def calculate_snr(prd):\n",
    "    \"\"\"\n",
    "    Calculate the Signal-to-Noise Ratio (SNR) based on PRD.\n",
    "    \n",
    "    Parameters:\n",
    "    prd (float): The PRD value as a percentage.\n",
    "    \n",
    "    Returns:\n",
    "    float: The SNR value in dB.\n",
    "    \"\"\"\n",
    "    snr = -20 * np.log10(prd / 100)\n",
    "    return snr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: test for best dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test all dictionaries to show with big number of data which are the one to perform better (we expect DWT to be the best of fixed, and that adaptive are better than fixed in general)\n",
    "- __#Records:__ Test on __MULTIPLE patients__ records is a MUST, especially to show that adaptive are better \n",
    "- __#Dictionaries:__ Test all dictionaries, that's what we are doing ...\n",
    "- __#ReconstructionMethods:__ Test with __only one reconstruction method__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: test for best reconstruction method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test to find which recontruction method is the best\n",
    "- __#Records:__ Test on a __single patient__ record should be fine \n",
    "- __#Dictionaries:__ Test with __a single dictionary type__\n",
    "- __#ReconstructionMethods:__ Test with __all reconstruction methods__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: test for correct block dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test to show that __sampling phase process speed is inversly proportional to block dimension__\n",
    "- __#Records:__ Test on a __single patient__ record should be fine \n",
    "- __#Dictionaries:__ Test with __a single dictionary type__ (USE BEST!)\n",
    "- __#ReconstructionMethods:__ Test with __only one reconstruction method__ (USE BEST!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: test for noise robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test if robust to noise by adding noise to the signal\n",
    "- __#Records:__ Test on a __single patient__ record should be fine \n",
    "- __#Dictionaries:__ Test with __a single dictionary type__ (USE BEST!)\n",
    "- __#ReconstructionMethods:__ Test with __only one reconstruction method__ (USE BEST!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data: MIT–BIH Arrhythmia Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIT-BIH Arrhythmia Database\n",
    "\n",
    "__Source:__ [physionet.org](https://www.physionet.org/content/mitdb/1.0.0/)\n",
    "\n",
    "__Authors:__ George Moody, Roger Mark\n",
    "\n",
    "__Version:__ 1.0.0 (Feb. 24, 2005)\n",
    "\n",
    "__Citation Information__\n",
    "\n",
    "__Original publication:__\n",
    "Moody GB, Mark RG. *The impact of the MIT-BIH Arrhythmia Database*. IEEE Eng in Med and Biol 20(3):45-50 (May-June 2001). (PMID: 11446209)\n",
    "\n",
    "__Citation for PhysioNet:__\n",
    "Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., ... & Stanley, H. E. (2000). *PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals*. Circulation [Online]. 101 (23), pp. e215–e220.\n",
    "\n",
    "__Background__\n",
    "\n",
    "Since 1975, laboratories at Boston’s Beth Israel Hospital (now the Beth Israel Deaconess Medical Center) and at MIT have supported research into arrhythmia analysis and related subjects. One of the first major products of that effort was the MIT-BIH Arrhythmia Database, completed and distributed in 1980. The database was the first generally available set of standard test material for evaluating arrhythmia detectors and has been used for that purpose as well as for basic research into cardiac dynamics at more than 500 sites worldwide. Originally, the database was distributed on 9-track half-inch digital tape at 800 and 1600 bpi, and on quarter-inch IRIG-format FM analog tape. In August 1989, a CD-ROM version of the database was produced.\n",
    "\n",
    "__Data Description__\n",
    "\n",
    "The MIT-BIH Arrhythmia Database contains:\n",
    "\n",
    "- 48 half-hour excerpts of __two-channel ambulatory ECG recordings.__\n",
    "- Data obtained from 47 subjects (1975-1979).\n",
    "- 23 recordings chosen at random from a set of 4000 24-hour ambulatory ECG recordings from Boston’s Beth Israel Hospital (inpatients 60%, outpatients 40%).\n",
    "- 25 recordings selected to include less common but clinically significant arrhythmias.\n",
    "- __Digitized at 360 samples per second per channel with 11-bit resolution over a 10 mV range.__\n",
    "- Annotations by two or more cardiologists; disagreements resolved to obtain reference annotations (approx. 110,000 annotations).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the database (command worked on August 2024, otherwise look for the database on physionet.org)\n",
    "\n",
    "# This will ignore:\n",
    "# mitdbdir: html where data is presented and explained (go on website if intrested or download without \"--exclude\" option)\n",
    "# *.xws: files that are used to visualize data on a Physionet.org tool called \"LightWave\" (go on website if intrested or download without \"--reject\" option)\n",
    "\n",
    "#!wget -r -N -c -np --reject \"*.xws\" --exclude-directories=mitdbdir https://physionet.org/files/mitdb/1.0.0/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Retrieving the Data Path\n",
    "\n",
    "To read the MITDB (MIT-BIH Arrhythmia Database) data using the `wfdb` library, we first need to retrieve the data path. In this Jupyter Notebook, the data path is stored in the `data_folder` variable.\n",
    "We can use this path to locate the specific record we want to read. In this case, the record number is stored in the `record_number` variable.\n",
    "\n",
    "To construct the full path to the record, we can use the `os.path.join()` function as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the project root directory (assuming the notebook is in the root directory)\n",
    "project_root = os.getcwd()  # Gets the current working directory, which is the root in this case\n",
    "\n",
    "# Define the relative path to the data folder\n",
    "data_folder = os.path.join(project_root, 'physionet.org', 'files', 'mitdb', '1.0.0')\n",
    "\n",
    "# Specify the record number\n",
    "record_number = '100'  # Change this to the desired record number\n",
    "\n",
    "# Construct the full path to the record\n",
    "record_path = os.path.join(data_folder, record_number)\n",
    "\n",
    "\n",
    "# print record_path\n",
    "print(record_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the signal and annotations\n",
    "In the code above, we first import the `wfdb` module. Then, we use the `rdrecord()` function to load the signal from the specified `record_path`. We also use the `rdann()` function to load the annotations for the same record. \n",
    "\n",
    "After executing this code, the signal will be stored in the `record` variable, and the annotations will be stored in the `annotation` variable.\n",
    "\n",
    "We will then visualize the whole signal, it's unreadable on a 30 hours time frame, it's only to visually check if downloading went well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "\n",
    "# Load the signal and annotations\n",
    "record = wfdb.rdrecord(record_path)\n",
    "annotation = wfdb.rdann(record_name=record_path, extension='atr', shift_samps=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the signal with annotations, just to check if download went correctly\n",
    "wfdb.plot_wfdb(record=record, annotation=annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: moving data to Pandas dataframe\n",
    "Simply use the proper function from `wfdb` library\n",
    "\n",
    "__Annotation will not be ported to pandas dataframe__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = record.to_dataframe()\n",
    "\n",
    "print(df.head(15))\n",
    "print(\"\\n\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART II: visualize the ECG data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ECG wave consists of three main components: the P wave, the QRS complex, and the T wave.\n",
    "\n",
    "- The P wave represents atrial depolarization, which is the contraction of the atria. It is typically a small and smooth upward deflection.\n",
    "- The QRS complex represents ventricular depolarization, which is the contraction of the ventricles. It consists of three distinct waves: Q, R, and S. The Q wave is the first downward deflection, the R wave is the first upward deflection after the Q wave, and the S wave is the downward deflection after the R wave.\n",
    "- The T wave represents ventricular repolarization, which is the recovery of the ventricles. It is typically a smooth upward deflection.\n",
    "\n",
    "The typical duration of the P wave is around 80-100 milliseconds, the QRS complex lasts around 80-120 milliseconds, and the T wave lasts around 160-240 milliseconds.\n",
    "\n",
    "The distance between waves can vary depending on the heart rate. In a normal sinus rhythm, the distance between consecutive P waves (P-P interval) is usually consistent and represents the atrial rate. The distance between consecutive R waves (R-R interval) represents the ventricular rate. The normal range for the R-R interval is around 600-1000 milliseconds.\n",
    "\n",
    "When plotting hours-long ECG data, the waves become densely packed, making it difficult to interpret the waveform patterns. To overcome this, we can plot a specific interval of the ECG data by selecting a starting time and duration. This allows us to focus on a specific part of the ECG waveform and make it more interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be useful when deciding portion of data to work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_ecg_interval(df, start=0, duration=1, fs=360):\n",
    "    \"\"\"\n",
    "    Plot an interval of ECG data.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the ECG data with time as index.\n",
    "    - start: Starting time in seconds for the plot.\n",
    "    - duration: Duration in seconds of the interval to plot.\n",
    "    - fs: Sampling frequency (samples per second).\n",
    "    \"\"\"\n",
    "    start_sample = int(start * fs)\n",
    "    end_sample = int((start + duration) * fs)\n",
    "    \n",
    "    interval_df = df.iloc[start_sample:end_sample]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "    \n",
    "    # Plot MLII in the first subplot\n",
    "    axes[0].plot(interval_df.index, interval_df['MLII'], label='MLII')\n",
    "    axes[0].set_ylabel('Amplitude (mV)')\n",
    "    axes[0].set_title(f'ECG MLII Data from {start} to {start + duration} seconds')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Plot V5 in the second subplot\n",
    "    axes[1].plot(interval_df.index, interval_df['V5'], label='V5', color='orange')\n",
    "    axes[1].set_xlabel('Time (seconds)')\n",
    "    axes[1].set_ylabel('Amplitude (mV)')\n",
    "    axes[1].set_title(f'ECG V5 Data from {start} to {start + duration} seconds')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test single implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose portion of record to work with\n",
    "# starting sample\n",
    "start = 302\n",
    "# duration in seconds\n",
    "duration = 1024\n",
    "\n",
    "# Extract that portion of the signal\n",
    "ecg_signal = df.iloc[start:start + duration]\n",
    "\n",
    "# Plot \"duration\" number of seconds of ecg data starting at \"start\" seconds\n",
    "plot_ecg_interval(df, start=302, duration=1) # Change the start and duration values as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the compressed measurement of the ECG signal\n",
    "y,Phi = compute_compressed_measurement(ecg_signal['MLII'].values, 16)\n",
    "\n",
    "# Compute Theta\n",
    "Theta, transform_type, wavelet_type = compute_theta(Phi, transform_type='dct')\n",
    "\n",
    "# Reconstruct the signal using blockwise L1 minimization\n",
    "reconstructed_signal = blockwise_l1_signal_reconstruction(y, Theta, transform_type)\n",
    "\n",
    "# Calculate the PRD between the original and reconstructed signals\n",
    "prd = calculate_prd(ecg_signal['MLII'].values, reconstructed_signal)\n",
    "\n",
    "# Calculate the SNR from the PRD\n",
    "snr = calculate_snr(prd)\n",
    "\n",
    "# Print the PRD and SNR values\n",
    "print(f\"PRD: {prd:.2f}%\")\n",
    "print(f\"SNR: {snr:.2f} dB\")\n",
    "\n",
    "# Plot the original and reconstructed signals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ecg_signal['MLII'].values, label='Original Signal', color='blue')\n",
    "plt.plot(reconstructed_signal, label='Reconstructed Signal', linestyle='--', color='red')\n",
    "plt.title('Original vs Reconstructed ECG Signal')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Developements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test other possible reconstruction methdos\n",
    "\n",
    "__\"classic\" _l1-minimization_ problem__\n",
    "TO BE WRITTEN\n",
    "\n",
    "__LASSO__\n",
    "TO BE WRITTEN\n",
    "\n",
    "__Greedy Algorithms__\n",
    "TO BE WRITTEN\n",
    "\n",
    "__Basis Pursuit__\n",
    "TO BE WRITTEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test other dictionaries\n",
    "\n",
    "\n",
    "#### Fixed dictionaries\n",
    "\n",
    "__KL__\n",
    "TO BE WRITTEN\n",
    "\n",
    "\n",
    "#### Adaptive Dictionary Learning\n",
    "\n",
    "__MOM__\n",
    "TO BE WRITTEN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Direct quotation are enclosed in `\"...\"` and are followed by a reference number inside `[]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Izadi, V., Shahri, P.K., & Ahani, H. (2020). A compressed-sensing-based compressor for ECG. *Biomedical Engineering Letters*, 10, 299–307. https://doi.org/10.1007/s13534-020-00148-7\n",
    "\n",
    "[2] __Chapter 3.1 \"Sparsity and Compressed Sensing\" of the Book__:\n",
    "   Brunton, S. L., & Kutz, J. N. (2022). *Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control* (2nd ed.). Cambridge University Press.\n",
    "\n",
    "__Data__:\n",
    "\n",
    "Moody GB, Mark RG. *The impact of the MIT-BIH Arrhythmia Database*. IEEE Eng in Med and Biol 20(3):45-50 (May-June 2001). (PMID: 11446209)\n",
    "\n",
    "Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., ... & Stanley, H. E. (2000). *PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals*. Circulation [Online]. 101 (23), pp. e215–e220.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Cosine Transform (DCT)\n",
    "\n",
    "The Discrete Cosine Transform (DCT) is a transform similar to the Discrete Fourier Transform (DFT) but uses only real numbers and cosines. It is widely used in image and video compression (e.g., JPEG, MPEG) due to its properties that are particularly suitable for these applications.\n",
    "\n",
    "##### Overview of the Discrete Cosine Transform (DCT)\n",
    "\n",
    "The DCT represents a signal as a sum of cosine functions oscillating at different frequencies. It transforms a sequence of real numbers into a sequence of coefficients representing the signal in the frequency domain.\n",
    "\n",
    "#### Types of DCT\n",
    "\n",
    "There are several types of DCT, but the most commonly used are DCT-I, DCT-II, and DCT-III. The most frequently used variant in practical applications is DCT-II, often referred to simply as \"the DCT.\"\n",
    "\n",
    "__DCT-II (The Most Common DCT)__\n",
    "\n",
    "For a sequence of $N$ real numbers $x[n]$, where $n = 0, 1, \\ldots, N-1$, the DCT-II is defined as:\n",
    "\n",
    "$$\n",
    "X[k] = \\sum_{n=0}^{N-1} x[n] \\cos \\left[ \\frac{\\pi}{N} \\left( n + \\frac{1}{2} \\right) k \\right] \\quad \\text{for} \\quad k = 0, 1, \\ldots, N-1\n",
    "$$\n",
    "\n",
    "__Inverse DCT-II__\n",
    "\n",
    "The inverse DCT-II (often referred to as IDCT) is defined as:\n",
    "\n",
    "$$\n",
    "x[n] = \\frac{1}{N} \\left( \\frac{X[0]}{2} + \\sum_{k=1}^{N-1} X[k] \\cos \\left[ \\frac{\\pi}{N} \\left( n + \\frac{1}{2} \\right) k \\right] \\right) \\quad \\text{for} \\quad n = 0, 1, \\ldots, N-1\n",
    "$$\n",
    "\n",
    "__DCT-I__\n",
    "\n",
    "The DCT-I is defined for a sequence $x[n]$ of length $N$ as:\n",
    "\n",
    "$$\n",
    "X[k] = \\sum_{n=0}^{N-1} x[n] \\cos \\left( \\frac{\\pi}{N-1} nk \\right) \\quad \\text{for} \\quad k = 0, 1, \\ldots, N-1\n",
    "$$\n",
    "\n",
    "DCT-I is defined only for sequences of length $N \\geq 2$ and is less commonly used due to boundary conditions.\n",
    "\n",
    "__DCT-III__\n",
    "\n",
    "The DCT-III, often referred to as the inverse DCT of DCT-II, is defined as:\n",
    "\n",
    "$$\n",
    "x[n] = \\frac{1}{2} X[0] + \\sum_{k=1}^{N-1} X[k] \\cos \\left( \\frac{\\pi}{N} k \\left( n + \\frac{1}{2} \\right) \\right) \\quad \\text{for} \\quad n = 0, 1, \\ldots, N-1\n",
    "$$\n",
    "\n",
    "#### Properties of the DCT\n",
    "\n",
    "- __Orthogonality__: The cosine basis functions used in DCT are orthogonal.\n",
    "- __Real-Valued Output__: For real-valued input signals, the DCT output is also real-valued.\n",
    "- __Energy Compaction__: The DCT tends to concentrate the energy of the signal in a few low-frequency components, making it efficient for compression.\n",
    "\n",
    "#### Complexity of DCT\n",
    "\n",
    "__Direct Computation__\n",
    "\n",
    "The direct computation of DCT for a sequence of length $N$ involves $N$ multiplications and $N-1$ additions for each of the $N$ frequency components, resulting in a total complexity of:\n",
    "\n",
    "$$\n",
    "O(N^2)\n",
    "$$\n",
    "\n",
    "__Fast Algorithms for DCT__\n",
    "\n",
    "Fast algorithms, similar to the Fast Fourier Transform (FFT), reduce the computational complexity of the DCT to:\n",
    "\n",
    "$$\n",
    "O(N \\log N)\n",
    "$$\n",
    "\n",
    "These algorithms exploit symmetry properties and use divide-and-conquer approaches to achieve significant computational savings.\n",
    "\n",
    "__Energy Compaction and Low-Frequency Components in DCT__\n",
    "\n",
    "The Discrete Cosine Transform (DCT) has a key property known as energy compaction, where most of the signal's energy is concentrated in a few low-frequency components. This property is essential for efficient compression, as it allows significant data reduction while preserving the essential features of the original signal.\n",
    "\n",
    "In the DCT, the index $k$ represents the frequency component. Low values of $k$ correspond to low-frequency components, which represent slow variations in the signal, while high values of $k$ correspond to high-frequency components, representing rapid variations.\n",
    "\n",
    "__Frequency Interpretation__\n",
    "\n",
    "- $k = 0$: The basis function is a constant, representing the average value of the signal.\n",
    "- Low $k$: Represent slow variations, such as $\\cos \\left( \\frac{\\pi}{N} \\left( n + \\frac{1}{2} \\right) \\cdot 1 \\right)$.\n",
    "- High $k$: Represent rapid variations, such as $\\cos \\left( \\frac{\\pi}{N} \\left( n + \\frac{1}{2} \\right) \\cdot (N-1) \\right)$.\n",
    "\n",
    "__Energy Compaction__\n",
    "\n",
    "The DCT's ability to concentrate energy in low-frequency components means that for many natural signals, including images and audio, most of the significant information can be captured with only a few coefficients. This makes the DCT highly efficient for compression purposes, as the majority of high-frequency coefficients (which represent fine details and noise) can be quantized more coarsely or discarded without significantly affecting the perceived quality of the signal.\n",
    "\n",
    "#### Matrix Representation of the DCT\n",
    "\n",
    "The Discrete Cosine Transform (DCT) can be represented in a matrix form, which is particularly useful for understanding the transform as a linear operation. This approach involves the use of an orthonormal basis matrix formed by cosine functions.\n",
    "\n",
    "__DCT as a Matrix Product__\n",
    "\n",
    "Let $\\mathbf{x}$ be the input signal, which is a column vector of length $N$. The DCT of this signal can be expressed as a matrix-vector multiplication:\n",
    "\n",
    "$$\n",
    "\\mathbf{X} = \\mathbf{\\Psi} \\mathbf{x}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{X}$ is the vector of DCT coefficients, and $\\mathbf{\\Psi}$ is the $N \\times N$ DCT matrix whose elements are defined as:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Psi}[k,n] = \\cos \\left[ \\frac{\\pi}{N} \\left( n + \\frac{1}{2} \\right) k \\right] \\quad \\text{for} \\quad k, n = 0, 1, \\ldots, N-1\n",
    "$$\n",
    "\n",
    "This matrix $\\mathbf{\\Psi}$ forms an orthonormal basis for the space of real-valued signals of length $N$.\n",
    "\n",
    "__Inverse DCT as a Matrix Product__\n",
    "\n",
    "The inverse DCT (IDCT) can also be represented in matrix form. Given the DCT coefficients $\\mathbf{X}$, the original signal $\\mathbf{x}$ can be recovered as:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\mathbf{\\Psi}^\\top \\mathbf{X}\n",
    "$$\n",
    "\n",
    "Here, $\\mathbf{\\Psi}^\\top$ is the transpose of the DCT matrix $\\mathbf{\\Psi}$, not the conjugate transpose (Hermitian), since the DCT is a real-valued transform and $\\mathbf{\\Psi}$ is a real-valued matrix.\n",
    "\n",
    "__Orthogonality of the DCT Matrix__\n",
    "\n",
    "The matrix $\\mathbf{\\Psi}$ is orthonormal, meaning it satisfies:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Psi}^\\top \\mathbf{\\Psi} = \\mathbf{I}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{I}$ is the identity matrix. This property ensures that the DCT and IDCT operations are perfect inverses of each other, preserving the energy of the original signal in the frequency domain.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Wavelet Transform (DWT)\n",
    "\n",
    "The Discrete Wavelet Transform (DWT) is a transform used in signal processing and compression, offering advantages over the Discrete Fourier Transform (DFT) and Discrete Cosine Transform (DCT). The DWT provides a time-frequency representation of the signal, capturing both frequency and location information.\n",
    "\n",
    "#### Overview of DWT\n",
    "\n",
    "The DWT decomposes a signal into a set of wavelets, which are localized in both time and frequency. This allows for multi-resolution analysis, where different parts of the signal can be analyzed at different scales.\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "- __Wavelets__: Functions that efficiently represent data with sharp changes or edges, localized in time.\n",
    "- __Scaling and Translation__: Wavelets can be scaled (dilated) and translated (shifted) to capture different frequency components and their locations in the signal.\n",
    "- __Multi-Resolution Analysis__: DWT performs analysis at multiple resolutions, capturing both coarse and fine details of the signal.\n",
    "\n",
    "#### DWT Algorithm\n",
    "\n",
    "The DWT of a signal can be computed using recursive filtering and downsampling. The process involves two main steps: decomposition (analysis) and reconstruction (synthesis).\n",
    "\n",
    "__Decomposition (Analysis)__\n",
    "\n",
    "- __Filter Bank__: Apply a pair of filters to the signal: a low-pass filter (L) and a high-pass filter (H). The low-pass filter captures the approximation (low-frequency) components, while the high-pass filter captures the detail (high-frequency) components.\n",
    "- __Downsampling__: After filtering, the signal is downsampled by a factor of 2 (keeping every other sample) to reduce the data size.\n",
    "- __Recursive Decomposition__: The decomposition process is recursively applied to the low-pass filtered signal to create a multi-level decomposition.\n",
    "\n",
    "### Reconstruction (Synthesis)\n",
    "\n",
    "- __Upsampling__: The downsampled components are upsampled by a factor of 2 (inserting zeros between samples).\n",
    "- __Filter Bank__: Apply the synthesis filters (low-pass and high-pass) to the upsampled components.\n",
    "- __Combining__: The filtered components are combined to reconstruct the signal.\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "Given a signal $x[n]$:\n",
    "\n",
    "- __Approximation Coefficients (Low Frequency)__:\n",
    "  \n",
    "  $$\n",
    "  A_j[k] = \\sum_n x[n] \\cdot \\phi_{j,k}[n]\n",
    "  $$\n",
    "  \n",
    "  where $\\phi_{j,k}[n]$ are the scaling functions (low-pass).\n",
    "\n",
    "- __Detail Coefficients (High Frequency)__:\n",
    "  \n",
    "  $$\n",
    "  D_j[k] = \\sum_n x[n] \\cdot \\psi_{j,k}[n]\n",
    "  $$\n",
    "  \n",
    "  where $\\psi_{j,k}[n]$ are the wavelet functions (high-pass).\n",
    "\n",
    "#### Advantages of DWT\n",
    "\n",
    "- __Localization__: Wavelets are localized in both time and frequency, allowing DWT to capture transient features more effectively than DFT or DCT.\n",
    "- __Multi-Resolution Analysis__: DWT provides a hierarchical representation, enabling analysis at multiple resolutions and scales.\n",
    "- __Efficient Compression__: DWT often achieves better compression efficiency for images and signals with sharp changes or edges, as it can represent such features more compactly.\n",
    "\n",
    "#### Complexity of DWT\n",
    "\n",
    "__Direct Computation__\n",
    "\n",
    "The direct computation of DWT for a signal of length $N$ involves $O(N)$ operations per level of decomposition. For a full $J$-level decomposition, the total complexity is:\n",
    "\n",
    "$$\n",
    "O(N)\n",
    "$$\n",
    "\n",
    "__Fast Algorithms for DWT__\n",
    "\n",
    "Fast DWT algorithms, such as those based on recursive filtering and downsampling, also achieve a complexity of:\n",
    "\n",
    "$$\n",
    "O(N)\n",
    "$$\n",
    "\n",
    "These algorithms exploit the hierarchical structure of the wavelet transform to achieve efficient computation.\n",
    "\n",
    "__Matrix Representation of the DWT__\n",
    "\n",
    "The Discrete Wavelet Transform (DWT) can also be represented in matrix form, analogous to the matrix representation of the Discrete Cosine Transform (DCT). This approach allows us to see the DWT as a linear operation involving an orthonormal basis formed by wavelet functions.\n",
    "\n",
    "__DWT as a Matrix Product__\n",
    "\n",
    "Let $\\mathbf{x}$ be the input signal, which is a column vector of length $N$. The DWT of this signal can be expressed as a matrix-vector multiplication:\n",
    "\n",
    "$$\n",
    "\\mathbf{W} = \\mathbf{\\Phi} \\mathbf{x}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{W}$ is the vector of wavelet coefficients, and $\\mathbf{\\Phi}$ is the $N \\times N$ wavelet transform matrix. The matrix $\\mathbf{\\Phi}$ is constructed using wavelet functions (for high-frequency components) and scaling functions (for low-frequency components).\n",
    "\n",
    "__Inverse DWT as a Matrix Product__\n",
    "\n",
    "The inverse DWT (IDWT) can be represented in matrix form similarly. Given the wavelet coefficients $\\mathbf{W}$, the original signal $\\mathbf{x}$ can be recovered as:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = \\mathbf{\\Phi}^\\top \\mathbf{W}\n",
    "$$\n",
    "\n",
    "Here, $\\mathbf{\\Phi}^\\top$ is the transpose of the wavelet transform matrix $\\mathbf{\\Phi}$. Since the DWT is typically real-valued, we use the transpose rather than the conjugate transpose (Hermitian).\n",
    "\n",
    "__Orthogonality of the DWT Matrix__\n",
    "\n",
    "The matrix $\\mathbf{\\Phi}$ is orthonormal, which means it satisfies:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Phi}^\\top \\mathbf{\\Phi} = \\mathbf{I}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{I}$ is the identity matrix. This property ensures that the DWT and IDWT are perfect inverses of each other, preserving the energy of the original signal while transforming it into the wavelet domain.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Assessment Based on PRD and SNR\n",
    "\n",
    "Table 1 from the referenced paper classifies the quality of the reconstructed signal based on the PRD and corresponding SNR values:\n",
    "\n",
    "| Quality        | PRD Range      | SNR Range       |\n",
    "|----------------|----------------|-----------------|\n",
    "| Very Good      | 0% < PRD < 2%  | SNR > 33 dB     |\n",
    "| Good           | 2% < PRD < 9%  | 20 dB < SNR < 33 dB |\n",
    "| Undetermined   | PRD ≥ 9%       | SNR ≤ 20 dB     |\n",
    "\n",
    "This table indicates that when the PRD is less than 2%, the quality of the reconstructed signal can be categorized as \"Very Good.\" For PRD values between 2% and 9%, the quality is considered \"Good,\" and for PRD values above 9%, the quality of the reconstructed signal cannot be precisely determined. \n",
    "\n",
    "__Metric Based on Physician Qualitative Assessments__\n",
    "\n",
    "The classification of the PRD and SNR values into \"Very Good,\" \"Good,\" and \"Undetermined\" categories was established based on a study by Zigel et al., which is referenced in the paper. In this study, a link was established between the diagnostic distortion of ECG signals and the PRD metric. The researchers conducted qualitative assessments with physicians, who evaluated the diagnostic quality of reconstructed ECG signals at different PRD levels.\n",
    "\n",
    "The physicians' qualitative assessments provided a subjective but clinically relevant measure of how much distortion could be tolerated in the reconstructed signals before it began to interfere with accurate diagnosis. These evaluations were then correlated with specific PRD values, allowing the researchers to define thresholds where the signal quality was deemed acceptable or unacceptable for clinical use. For instance, a PRD of less than 2% was consistently associated with minimal diagnostic distortion, leading to its classification as \"Very Good.\" As PRD increased, the likelihood of clinically significant distortion also increased, which was reflected in the \"Good\" and \"Undetermined\" categories.\n",
    "\n",
    "This physician-based qualitative assessment was crucial in grounding the PRD and SNR metrics in practical clinical utility, ensuring that the numerical \n",
    "thresholds corresponded to meaningful diagnostic criteria.\n",
    "\n",
    "_Based on [1] Izadi, V., Shahri, P.K., & Ahani, H. (2020). A compressed-sensing-based compressor for ECG. *Biomedical Engineering Letters*, 10, 299–307. https://doi.org/10.1007/s13534-020-00148-7_\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".namlVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
