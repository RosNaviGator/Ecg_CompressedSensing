{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compressed Sensing (CS) based ECG compressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "TO BE WRITTEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal of Project\n",
    "\n",
    "TO BE WRITTEN\n",
    "\n",
    "Roadmap:\n",
    "- reproducing idea from paper bla bla bla\n",
    "- in general: study (\"emulate\", not really) best solution for a CS-based compressor for ECG to be used with remote-ECG-devices, small, limited storage capability, limited computational power:\n",
    "    - __phase 1__ compute dictionary (only for adaptive dictionaries) $\\Psi$ and measurement matrix $\\Phi$ before actually using the device to measure the patient ecg\n",
    "    - __phase 2__ pass $\\Psi$, $\\Phi$ to the device, take __already compressed measurements__ $y$ (we'll see that this is core idea of CS)\n",
    "    - __phase 3__ store only $y$, $\\Psi$, $\\Phi$ and send them back to _more computationally powerful system_ where recovery happens\n",
    "- paper focuses also on how such hardware is built, we will be more generic\n",
    "- exploit data from Physionet.org exactly like the paper did\n",
    "- test different dictionaries, both _fixed dictionaries_ (_DCT_, _DWT_, _KL_) and _adaptive dictionary learning_ (_MOM_, _K-SVD_)\n",
    "- test how dimension of measurement matrix $\\Phi$ is related to processing speed in __phase 2__\n",
    "- test different _recovery methods_ (\"classic\" _l1-minimization_, _LASSO_, _Greedy Algorithms_, _Smooth-L0_, _Baisis Pursuit_), always use newly developed __Kronecker technique__ \n",
    "- testing robustness to noise with _additive noise_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: sparsity and compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Sparsity\n",
    "__Theoretical Sparsity__\n",
    "\n",
    "A signal $s \\in \\mathbb{R}^n$ is considered $k$-sparse if it has exactly $k$ non-zero elements, with $k \\ll n$. This means that $n-k$ elements of the signal are exactly zero.\n",
    "$$\n",
    "s = \\begin{pmatrix} s_1 \\\\ s_2 \\\\ \\vdots \\\\ s_n \\end{pmatrix}\n",
    "$$\n",
    "where exactly $k$ elements in $s$ are non-zero, and the remaining $n-k$ elements are zero.\n",
    "\n",
    "\n",
    "__Practical Sparsity__\n",
    "\n",
    "In real-world signals, _exact sparsity is rare_. Instead, signals are often _representable_ (see next section) as __approximately sparse__: only $k$ elements _of the sparse representation_ are significant and carry most of the signal's information, the remaining $n-k$ elements have small, negligible values. \n",
    "\n",
    "The difference lies in the fact that the $n-k$ coefficients are small but not exactly zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Representation of Signals\n",
    "\n",
    "\"Most natural signals, such as images and audio, are highly compressible. This compressibility means that, when the signal is written in an appropriate basis, only a few modes are active, thus reducing the number of values that must be stored for an accurate representation. In other words, a compressible signal $x \\in \\mathbb{R}^n$ may be written as a sparse vector $s \\in \\mathbb{R}^n$ in a transform basis $\\Psi \\in \\mathbb{C}^{n \\times n}$:\n",
    "\n",
    "$$\n",
    "x = \\Psi s.\n",
    "$$\n",
    "\n",
    "If the basis $\\Psi$ is generic, such as the Fourier or wavelet basis, then only the few active terms in $s$ are required to reconstruct the original signal $x$, reducing the data required to store or transmit the signal.\" [2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classic Transformation-Based Compression\n",
    "\n",
    "A typical transformation-based compression algorithm involves the following steps:\n",
    "\n",
    "1. __Signal capture__: \n",
    "    Fully sense a whole __raw__ signal $x$ and store it. In this project $x$ are the _voltages_ measured by the ECG machine.\n",
    "2. __Transformation to a sparse domain__:\n",
    "    The signal $x$ is transformed to a sparse domain, basically we want to find the sparse vector $s \\in \\mathbb{R}^n$, that contain mostly negligible coefficients.\n",
    "\n",
    "    We exploit $\\Psi \\in \\mathbb{C}^{n \\times n}$ orthogonal basis matrix, also called __dictionary__. Being $\\Psi$ an orthonormal basis, it satisfies $\\Psi^H \\Psi = I$, where $\\Psi^H$ is the Hermitian conjugate (conjugate transpose) of $\\Psi$, and $I$ is the identity matrix. This implies that $\\Psi^{-1} = \\Psi^H$, making the transformation and its inverse straightforward.\n",
    "\n",
    "    Therefore, when $ \\Psi $ is an orthonormal basis, applying $ \\Psi^H $ to the signal effectively inverts the transformation applied by $ \\Psi $, we can use this to obtain sparse representation from original signal:\n",
    "\n",
    "    $$\n",
    "    s = \\Psi^H x\n",
    "    $$\n",
    "\n",
    "    __The use of transforms__:\n",
    "    On a mathematical note: $\\Psi$ is an orthonormal basis composed of functions like Fourier Function, Wavelet, and so on.\n",
    "    The actual computation of $s$ doesn't actually build a dictionary $\\Psi$ to invert and multiplicate to the signal. Instead it directly applies the _transform_ (e.g. FFT, DWT, DCT, ...) to the signal $x$, to immediately obtain _sparse representation_ $s$.\n",
    "\n",
    "3. __Sparsification__: \n",
    "    A fundamental concept is that a threshold is applied to the coefficients, retaining only those that are significant (i.e., above the threshold) and discarding the rest.\n",
    "\n",
    "    A more detailed view reveals that these steps can be performed using a wide range of techniques, depending on the transform employed, and equivalently, on the choice of dictionary.\n",
    "\n",
    "    _This will not be explored as it is not the subject of this project, it's a vast and intresting topic, Brunto&Kutz book in reference provide a good reference to explore more..._\n",
    "\n",
    "4. __Encoding__:\n",
    "    The retained coefficients and their positions are then encoded for storage or transmission. \n",
    "\n",
    "    _Another huge chapter that will not be explored here, again you can refer to the referenced book for more_\n",
    "\n",
    "__Complexity__\n",
    "\n",
    "Such methods can be _extremely_ effective, but they require a _thresholding/sparsification_ step, which introduces non-linearity and computational complexity. \n",
    "\n",
    "In the following is shown that CS-based methods can provide an alternative solution with different advantages...\n",
    "\n",
    "<center>\n",
    "    <img src=\"./.img/MethodsComparison.png\" alt=\"MethodsComparison.png\" width=\"600\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressed Sensing (CS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Mathematically, compressed sensing exploits the _sparsity of a signal_ in a __generic basis__ to achieve full signal reconstruction from surprisingly few measurements.\n",
    "\n",
    "If a __signal $x$ is k-sparse in $\\Psi$ (it's a requirement),__ then instead of measuring $x$ directly (n measurements) and then compressing, it is possible to collect dramatically fewer randomly chosen or compressed measurements and then solve for the non-zero elements of s in the transformed coordinate system.\" [2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measurement\n",
    "\n",
    "Instead of acquiring all $n$ samples, a reduced set of $m$ measurements is obtained directly by projecting the signal $x$  onto a measurement matrix $\\Phi$, storing a _compressed measurement_ $y$:\n",
    "\n",
    "$$\n",
    "y = \\Phi x\n",
    "$$\n",
    "\n",
    "\n",
    "where:\n",
    "- $x \\in \\mathbb{R}^n$ _real_ signal coming from sensors\n",
    "- $y \\in \\mathbb{R}^m$ _compressed measurement_\n",
    "- $\\Phi \\in \\mathbb{R}^{m \\times n}$ with $m \\ll n$ is the _measurement matrix_.\n",
    "\n",
    "__Key concept__:\n",
    "In the measurement phase the _sparse representation_ $s$ is __not__ computed, we directly apply the _measurement matrix_ to the _real_ signal $x$. \n",
    "\n",
    "$\\Phi$ does not simply \"select\" $m$ out of $n$ coefficients out of $x$. Instead, $\\Phi$ typically contains random or structured elements that ensure the measurements $y$ retain sufficient information to __later recover__ the sparse signal $s$. \n",
    "\n",
    "_Next chapter will explore measurement matrix topic_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovery\n",
    "\n",
    "With knowledge of $s \\in \\mathbb{R}^n$ _sparse representation_ of $x$ through $\\Psi$ _dictionary_, it is possible to recovery $x$ itself as previously shown with:\n",
    "$$\n",
    "x = \\Psi s\n",
    "$$\n",
    "\n",
    "Thus the goal of compressed sensing is to find the __sparsest__ vector $s$ that is consistent with:\n",
    "\n",
    "$$\n",
    "y = \\Phi x = \\Phi \\Psi s\n",
    "$$\n",
    "\n",
    "where (again):\n",
    "- $x \\in \\mathbb{R}^n$ _real_ signal coming from sensors\n",
    "- $y \\in \\mathbb{R}^m$ _compressed measurement_\n",
    "- $\\Psi \\in \\mathbb{R}^{n \\times n}$ is the _dictionary_ (same as explained in previous section)\n",
    "- $\\Phi \\in \\mathbb{R}^{m \\times n}$ with $m \\ll n$ is the _measurement matrix_.\n",
    "- $s \\in \\mathbb{R}^n$ is the _sparse representation_ of $x$ in $\\Psi$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Non convex problem__\n",
    "\n",
    "\"Such system of equations is __under-determined__ since there are infinitely many consistent solution $s$. The __sparsest solution__ is the one that satisfies:\n",
    "\n",
    "$$\n",
    "\\hat{s} = \\arg_{s} \\min \\|s\\|_0 \\text{ subject to } y = \\Phi \\Psi \\alpha\n",
    "$$\n",
    "\n",
    "where $\\min \\|s\\|_0$ denotes the $\\ell_0$-pseudo-norm, given by the _non-zero entries_, also referred as the _cardinality_ of $s$.\n",
    "\n",
    "The optimization is non-convex, and in general, the solution can only be found with a brute-force search that is combinatorial in $n$ and $K$. In particular, all possible $K$-sparse vectors in $\\mathbb{R}^n$ must be checked; if the exact level of sparsity $K$ is unknown, the search is even broader. Because this search is combinatorial, solving such minimization is intractable for even moderately large $n$ and $K$, and the prospect of solving larger problems does not improve with Moore’s law of exponentially increasing computational power.\"[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convex equivalent problem__\n",
    "\n",
    "Fortunately, under certain conditions on the measurement matrix $\\Phi$, it is possible to relax the optimization to a convex $\\ell_1$-minimization.\n",
    "\n",
    "$$\n",
    "\\hat{s} = \\arg_{s} \\min \\|s\\|_1 \\text{ subject to } y = \\Phi \\Psi \\alpha\n",
    "$$\n",
    "\n",
    "__In the presence of noise__, the recovery problem is modified to:\n",
    "\n",
    "$$\n",
    "\\hat{s} = \\arg_{s} \\min \\|s\\|_1 \\text{ subject to } \\|y - \\Phi \\Psi s\\|_2 \\leq \\epsilon\n",
    "$$\n",
    "\n",
    "where $\\epsilon$ is a bound on the noise level.\n",
    "\n",
    "There are very specific conditions that must be met for the $\\ell_1$-minimization to converge with high probability to the sparsest solution of $\\ell_0$-minimization. They can be summarized as follows:\n",
    "- __Incoherence__: \n",
    "    A critical concept in compressed sensing is the _incoherence_ between the measurement matrix $\\Phi$ and the dictionary $\\Psi$. Incoherence refers to the property that ensures that the rows of $\\Phi$ are not too similar to the columns of $\\Psi$. This incoherence is vital because it allows the sparse information in the signal $x$ (which is represented in the domain of $\\Psi$) to be evenly spread across the measurements $y$. This spreading ensures that no single measurement in $y$ captures too much or too little information about the signal $x$, which is essential for accurate recovery of the sparse signal $s$ from the measurements $y$.\n",
    "\n",
    "- __Recoverability Condition:__ \n",
    "    A __$K$-sparse__ signal $s \\in \\mathbb{R}^n$ can be properly recovered after Compressive Sensing (CS) if the number of measurements $m$ satisfies:\n",
    "\n",
    "    $$\n",
    "    m \\geq C K \\log\\left(\\frac{n}{K}\\right)\n",
    "    $$\n",
    "\n",
    "    where $C$ is a constant that depends on how __incoherent__ $\\Phi$ and $\\Psi$ are. This condition ensures that enough measurements are taken to accurately recover the sparse signal, accounting for both sparsity and the ambient dimension $n$.\n",
    "\n",
    "    The recoverability condition is a practical guideline that tells you how many measurements $m$ you need to take to ensure that a $k$-sparse signal $s \\in \\mathbb{R}^n$ can be recovered accurately. The $\\log\\left(\\frac{n}{k}\\right)$ term accounts for the dimensionality reduction that occurs when mapping an $n$-dimensional signal into an $m$-dimensional measurement space.\n",
    "\n",
    "\"Roughly speaking, these two conditions guarantee that the matrix $\\Phi |Psi$ acts as a unitary transformation on K-sparse vectors $s$, preserving relative distances between vectors and enabling almost certain signal reconstruction with $\\ell_1$ convex minimization. This is formulated precisely in terms of the restricted isometry property (RIP) that follows.\"[2]\n",
    "\n",
    "__Restricted Isometry Property (RIP):__\n",
    "\"The RIP is a property of the matrix $A = \\Phi \\Psi$ that provides a condition under which the matrix will behave well with respect to sparse signals. Specifically, for a matrix $A$ to satisfy the RIP of order $k$ with a constant $\\delta_k$, it must hold that:\n",
    "\n",
    "$$\n",
    "(1 - \\delta_k) \\|x\\|_2^2 \\leq \\|A x\\|_2^2 \\leq (1 + \\delta_k) \\|x\\|_2^2\n",
    "$$\n",
    "\n",
    "for all $k$-sparse vectors $x$. Here, $\\delta_k$ is the smallest constant such that this inequality holds, and it should be close to zero. This ensures that the matrix $A$ approximately preserves the Euclidean length (and hence the geometry) of all $k$-sparse signals, meaning the measurements are nearly isometric.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__LASSO Compressed Sensing__\n",
    "\n",
    "Alternatively, the signal can be recovered using the LASSO formulation, which balances the $\\ell_1$ norm of the signal with the fidelity to the measurements:\n",
    "\n",
    "$$\n",
    "\\hat{s} = \\arg \\min \\left(\\frac{1}{2} \\|y - \\Phi \\Psi s\\|_2^2 + \\lambda \\|s\\|_1 \\right)\n",
    "$$\n",
    "\n",
    "Here, $\\lambda$ is a regularization parameter that controls the trade-off between the sparsity of the solution and the accuracy of the reconstruction. A larger $\\lambda$ emphasizes sparsity, while a smaller $\\lambda$ emphasizes data fidelity.\n",
    "\n",
    "Note that $LASSO$ formulation already accounts for noise using the $\\ell_2$ minimization on $\\|y - \\Phi \\Psi s\\|_2^2$, instead of enforcing the condition $y = \\Phi \\Psi s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: main aspects of study and evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aspects relevant to the study\n",
    "\n",
    "__Work on signal block__\n",
    "\n",
    "ECG provide continuous data sampling, a record length can vary based on why it is being taken from few minutes, to hours, to days. This work addresses small devices, that will take a number of sample that can vary between 16 and 1024 as __signal block to compress__.\n",
    "\n",
    "__Compression ratio (CR)__\n",
    "\n",
    "\"Important factor for evaluating different methods. CR as follow \n",
    "$$\n",
    "CR(\\%) = 100 \\frac{n - m}{n}\n",
    "$$\n",
    "where $m$ and $n$ are the number of compressed and original samples, respectively. \"[1]\n",
    "\n",
    "__Compression algorithm’s complexity__\n",
    "\n",
    "Very relevant \"when we talk about limited and weak ECG-recorders. The power consumption usually has a linear relation with the complex-ity of systems. Supplying the power for 24-h ambulatory or remote ECG recorders is very important, that encourage \n",
    "us to focus on systems that have low power consumption.\"[1]\n",
    "\n",
    "The focus here is especially on _sampling phase_: one of the goal of the project will be to demonstrate, same as they did in the paper, that a smaller _measurement matrix_ will result in a _more efficient sampling phase_.\n",
    "\n",
    "__Processing speed__\n",
    "\n",
    "\"In emergency situations it will be important. Considering the ambulatory ECG recorders, whatever the data sooner to be presented to a physician, the next orders from a physician can be given sooner as well.\"\n",
    "\n",
    "Here it must be also taken into account the _reconstruction complexity_, in order to provide _usable_ ECG data, it's necessary to be fast both in acquirin and processing the data.\n",
    "\n",
    "_In this work are reproposed the same fundamental metrics and evaluation aspects proposed in the [1] Izadi, V., Shahri, P.K., & Ahani, H. (2020) paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics to Assess the Accuracy of Reconstructed Signal\n",
    "\n",
    "The accuracy of the reconstructed signal in ECG compression algorithms is typically evaluated using two common metrics: the Percentage Root Mean Square Difference (PRD) and Signal-to-Noise Ratio (SNR). These metrics are defined as follows:\n",
    "__Percentage Root Mean Square Difference (PRD)__\n",
    "The PRD is a measure of the difference between the original ECG signal and the reconstructed ECG signal. It is calculated using the following equation:\n",
    "\n",
    "$$\n",
    "\\text{PRD} = 100 \\times \\sqrt{\\frac{\\sum_{i=0}^{N-1} (x(n) - \\hat{x}(n))^2}{\\sum_{i=0}^{N-1} x(n)^2}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x(n)$ is the original ECG signal.\n",
    "- $\\hat{x}(n)$ is the reconstructed ECG signal.\n",
    "- $N$ is the length of the signal.\n",
    "\n",
    "__Signal-to-Noise Ratio (SNR)__\n",
    "The SNR is another measure used to assess the quality of the reconstructed signal. It is calculated from the PRD using the following equation:\n",
    "\n",
    "$$\n",
    "\\text{SNR} = -20 \\log_{10} \\left(\\frac{\\text{PRD}}{100}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Assessment Based on PRD and SNR\n",
    "\n",
    "Table 1 from the referenced paper classifies the quality of the reconstructed signal based on the PRD and corresponding SNR values:\n",
    "\n",
    "| Quality        | PRD Range      | SNR Range       |\n",
    "|----------------|----------------|-----------------|\n",
    "| Very Good      | 0% < PRD < 2%  | SNR > 33 dB     |\n",
    "| Good           | 2% < PRD < 9%  | 20 dB < SNR < 33 dB |\n",
    "| Undetermined   | PRD ≥ 9%       | SNR ≤ 20 dB     |\n",
    "\n",
    "This table indicates that when the PRD is less than 2%, the quality of the reconstructed signal can be categorized as \"Very Good.\" For PRD values between 2% and 9%, the quality is considered \"Good,\" and for PRD values above 9%, the quality of the reconstructed signal cannot be precisely determined. __In this study the same metric will be adopted__.\n",
    "\n",
    "_Table based on [1] Izadi, V., Shahri, P.K., & Ahani, H. (2020). A compressed-sensing-based compressor for ECG. *Biomedical Engineering Letters*, 10, 299–307. https://doi.org/10.1007/s13534-020-00148-7_\n",
    "\n",
    "_More information on how such measure was established in APPENDIX_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: measurement matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO BE WRITTEN\n",
    "\n",
    "Roadmap:\n",
    "- Relation between $\\Phi$ dimension and speed of \"sensing phase\" (phase 2)\n",
    "- Relation between randomicity and RIP property"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _fixed dictionaries_ vs _adaptive dictionary learning_\n",
    "TO BE WRITTEN\n",
    "\n",
    "---\n",
    "\n",
    "### Fixed dictionaries\n",
    "\n",
    "#### DCT\n",
    "TO BE WRITTEN\n",
    "\n",
    "#### DWT\n",
    "TO BE WRITTEN\n",
    "\n",
    "#### KL\n",
    "TO BE WRITTEN\n",
    "\n",
    "---\n",
    "\n",
    "### Adaptive Dictionary Learning\n",
    "TO BE WRITTEN\n",
    "\n",
    "#### MOM\n",
    "TO BE WRITTEN\n",
    "\n",
    "#### K-SVD\n",
    "TO BE WRITTEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the signal and annotations\n",
    "In the code above, we first import the `wfdb` module. Then, we use the `rdrecord()` function to load the signal from the specified `record_path`. We also use the `rdann()` function to load the annotations for the same record. \n",
    "\n",
    "After executing this code, the signal will be stored in the `record` variable, and the annotations will be stored in the `annotation` variable.\n",
    "\n",
    "We will then visualize the whole signal, it's unreadable on a 30 hours time frame, it's only we will check if loading went correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: reconstruction of the signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction methods\n",
    "\n",
    "(we already explained the maths before, here is practival parts)\n",
    "\n",
    "#### \"classic\" _l1-minimization_ problem\n",
    "TO BE WRITTEN\n",
    "\n",
    "#### LASSO\n",
    "TO BE WRITTEN\n",
    "\n",
    "#### Greedy Algorithms\n",
    "TO BE WRITTEN\n",
    "\n",
    "#### Smooth-L0\n",
    "TO BE WRITTEN\n",
    "\n",
    "#### Basis Pursuit\n",
    "TO BE WRITTEN\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kronecker Techinque\n",
    "\n",
    "TO BE WRITTEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code implementation (python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "__Pre-sampling phase__\n",
    "\n",
    "- Generate dictionary $\\Psi$\n",
    "- Generate Measurement Matrix $\\Phi$\n",
    "\n",
    "---\n",
    "__Sampling phase__\n",
    "- $y = \\Phi \\Psi s$, simulate what would happen on device (_compute block by block, later concatenate results_)\n",
    "\n",
    "---\n",
    "__Recovery phase__\n",
    "- Reconstruct signal\n",
    "\n",
    "---\n",
    "__Evaluate result__\n",
    "1. Sampling faster for smaller $\\Phi$?\n",
    "2. Which dictionary are the best?\n",
    "3. Which recovery method is best?\n",
    "4. Robustness to noise?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: measurement matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: sampling phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: reconstructed signal evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_prd(original_signal, reconstructed_signal):\n",
    "    \"\"\"\n",
    "    Calculate the Percentage Root Mean Square Difference (PRD).\n",
    "    \n",
    "    Parameters:\n",
    "    original_signal (np.array): The original ECG signal.\n",
    "    reconstructed_signal (np.array): The reconstructed ECG signal.\n",
    "    \n",
    "    Returns:\n",
    "    float: The PRD value as a percentage.\n",
    "    \"\"\"\n",
    "    numerator = np.sum((original_signal - reconstructed_signal) ** 2)\n",
    "    denominator = np.sum(original_signal ** 2)\n",
    "    prd = 100 * np.sqrt(numerator / denominator)\n",
    "    return prd\n",
    "\n",
    "def calculate_snr(prd):\n",
    "    \"\"\"\n",
    "    Calculate the Signal-to-Noise Ratio (SNR) based on PRD.\n",
    "    \n",
    "    Parameters:\n",
    "    prd (float): The PRD value as a percentage.\n",
    "    \n",
    "    Returns:\n",
    "    float: The SNR value in dB.\n",
    "    \"\"\"\n",
    "    snr = -20 * np.log10(prd / 100)\n",
    "    return snr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: test for best dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test all dictionaries to show with big number of data which are the one to perform better (we expect DWT to be the best of fixed, and that adaptive are better than fixed in general)\n",
    "- __#Records:__ Test on __MULTIPLE patients__ records is a MUST, especially to show that adaptive are better \n",
    "- __#Dictionaries:__ Test all dictionaries, that's what we are doing ...\n",
    "- __#ReconstructionMethods:__ Test with __only one reconstruction method__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: test for best reconstruction method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test to find which recontruction method is the best\n",
    "- __#Records:__ Test on a __single patient__ record should be fine \n",
    "- __#Dictionaries:__ Test with __a single dictionary type__\n",
    "- __#ReconstructionMethods:__ Test with __all reconstruction methods__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: test for correct block dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test to show that __sampling phase process speed is inversly proportional to block dimension__\n",
    "- __#Records:__ Test on a __single patient__ record should be fine \n",
    "- __#Dictionaries:__ Test with __a single dictionary type__ (USE BEST!)\n",
    "- __#ReconstructionMethods:__ Test with __only one reconstruction method__ (USE BEST!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: test for noise robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test if robust to noise by adding noise to the signal\n",
    "- __#Records:__ Test on a __single patient__ record should be fine \n",
    "- __#Dictionaries:__ Test with __a single dictionary type__ (USE BEST!)\n",
    "- __#ReconstructionMethods:__ Test with __only one reconstruction method__ (USE BEST!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data: MIT–BIH Arrhythmia Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIT-BIH Arrhythmia Database\n",
    "\n",
    "__Source:__ [physionet.org](https://www.physionet.org/content/mitdb/1.0.0/)\n",
    "\n",
    "__Authors:__ George Moody, Roger Mark\n",
    "\n",
    "__Version:__ 1.0.0 (Feb. 24, 2005)\n",
    "\n",
    "__Citation Information__\n",
    "\n",
    "__Original publication:__\n",
    "Moody GB, Mark RG. *The impact of the MIT-BIH Arrhythmia Database*. IEEE Eng in Med and Biol 20(3):45-50 (May-June 2001). (PMID: 11446209)\n",
    "\n",
    "__Citation for PhysioNet:__\n",
    "Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., ... & Stanley, H. E. (2000). *PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals*. Circulation [Online]. 101 (23), pp. e215–e220.\n",
    "\n",
    "__Background__\n",
    "\n",
    "Since 1975, laboratories at Boston’s Beth Israel Hospital (now the Beth Israel Deaconess Medical Center) and at MIT have supported research into arrhythmia analysis and related subjects. One of the first major products of that effort was the MIT-BIH Arrhythmia Database, completed and distributed in 1980. The database was the first generally available set of standard test material for evaluating arrhythmia detectors and has been used for that purpose as well as for basic research into cardiac dynamics at more than 500 sites worldwide. Originally, the database was distributed on 9-track half-inch digital tape at 800 and 1600 bpi, and on quarter-inch IRIG-format FM analog tape. In August 1989, a CD-ROM version of the database was produced.\n",
    "\n",
    "__Data Description__\n",
    "\n",
    "The MIT-BIH Arrhythmia Database contains:\n",
    "\n",
    "- 48 half-hour excerpts of __two-channel ambulatory ECG recordings.__\n",
    "- Data obtained from 47 subjects (1975-1979).\n",
    "- 23 recordings chosen at random from a set of 4000 24-hour ambulatory ECG recordings from Boston’s Beth Israel Hospital (inpatients 60%, outpatients 40%).\n",
    "- 25 recordings selected to include less common but clinically significant arrhythmias.\n",
    "- __Digitized at 360 samples per second per channel with 11-bit resolution over a 10 mV range.__\n",
    "- Annotations by two or more cardiologists; disagreements resolved to obtain reference annotations (approx. 110,000 annotations).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the database (command worked on August 2024, otherwise look for the database on physionet.org)\n",
    "\n",
    "# This will ignore:\n",
    "# mitdbdir: html where data is presented and explained (go on website if intrested or download without \"--exclude\" option)\n",
    "# *.xws: files that are used to visualize data on a Physionet.org tool called \"LightWave\" (go on website if intrested or download without \"--reject\" option)\n",
    "\n",
    "#!wget -r -N -c -np --reject \"*.xws\" --exclude-directories=mitdbdir https://physionet.org/files/mitdb/1.0.0/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Retrieving the Data Path\n",
    "\n",
    "To read the MITDB (MIT-BIH Arrhythmia Database) data using the `wfdb` library, we first need to retrieve the data path. In this Jupyter Notebook, the data path is stored in the `data_folder` variable.\n",
    "We can use this path to locate the specific record we want to read. In this case, the record number is stored in the `record_number` variable.\n",
    "\n",
    "To construct the full path to the record, we can use the `os.path.join()` function as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the project root directory (assuming the notebook is in the root directory)\n",
    "project_root = os.getcwd()  # Gets the current working directory, which is the root in this case\n",
    "\n",
    "# Define the relative path to the data folder\n",
    "data_folder = os.path.join(project_root, 'physionet.org', 'files', 'mitdb', '1.0.0')\n",
    "\n",
    "# Specify the record number\n",
    "record_number = '100'  # Change this to the desired record number\n",
    "\n",
    "# Construct the full path to the record\n",
    "record_path = os.path.join(data_folder, record_number)\n",
    "\n",
    "\n",
    "# print record_path\n",
    "print(record_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the signal and annotations\n",
    "In the code above, we first import the `wfdb` module. Then, we use the `rdrecord()` function to load the signal from the specified `record_path`. We also use the `rdann()` function to load the annotations for the same record. \n",
    "\n",
    "After executing this code, the signal will be stored in the `record` variable, and the annotations will be stored in the `annotation` variable.\n",
    "\n",
    "We will then visualize the whole signal, it's unreadable on a 30 hours time frame, it's only to visually check if downloading went well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "\n",
    "# Load the signal and annotations\n",
    "record = wfdb.rdrecord(record_path)\n",
    "annotation = wfdb.rdann(record_name=record_path, extension='atr', shift_samps=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the signal with annotations, just to check if download went correctly\n",
    "wfdb.plot_wfdb(record=record, annotation=annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: moving data to Pandas dataframe\n",
    "Simply use the proper function from `wfdb` library\n",
    "\n",
    "__Annotation will not be ported to pandas dataframe__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = record.to_dataframe()\n",
    "\n",
    "print(df.head(15))\n",
    "print(\"\\n\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART II: visualize the ECG data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ECG wave consists of three main components: the P wave, the QRS complex, and the T wave.\n",
    "\n",
    "- The P wave represents atrial depolarization, which is the contraction of the atria. It is typically a small and smooth upward deflection.\n",
    "- The QRS complex represents ventricular depolarization, which is the contraction of the ventricles. It consists of three distinct waves: Q, R, and S. The Q wave is the first downward deflection, the R wave is the first upward deflection after the Q wave, and the S wave is the downward deflection after the R wave.\n",
    "- The T wave represents ventricular repolarization, which is the recovery of the ventricles. It is typically a smooth upward deflection.\n",
    "\n",
    "The typical duration of the P wave is around 80-100 milliseconds, the QRS complex lasts around 80-120 milliseconds, and the T wave lasts around 160-240 milliseconds.\n",
    "\n",
    "The distance between waves can vary depending on the heart rate. In a normal sinus rhythm, the distance between consecutive P waves (P-P interval) is usually consistent and represents the atrial rate. The distance between consecutive R waves (R-R interval) represents the ventricular rate. The normal range for the R-R interval is around 600-1000 milliseconds.\n",
    "\n",
    "When plotting hours-long ECG data, the waves become densely packed, making it difficult to interpret the waveform patterns. To overcome this, we can plot a specific interval of the ECG data by selecting a starting time and duration. This allows us to focus on a specific part of the ECG waveform and make it more interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_ecg_interval(df, start=0, duration=1, fs=360):\n",
    "    \"\"\"\n",
    "    Plot an interval of ECG data.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the ECG data with time as index.\n",
    "    - start: Starting time in seconds for the plot.\n",
    "    - duration: Duration in seconds of the interval to plot.\n",
    "    - fs: Sampling frequency (samples per second).\n",
    "    \"\"\"\n",
    "    start_sample = int(start * fs)\n",
    "    end_sample = int((start + duration) * fs)\n",
    "    \n",
    "    interval_df = df.iloc[start_sample:end_sample]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "    \n",
    "    # Plot MLII in the first subplot\n",
    "    axes[0].plot(interval_df.index, interval_df['MLII'], label='MLII')\n",
    "    axes[0].set_ylabel('Amplitude (mV)')\n",
    "    axes[0].set_title(f'ECG MLII Data from {start} to {start + duration} seconds')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Plot V5 in the second subplot\n",
    "    axes[1].plot(interval_df.index, interval_df['V5'], label='V5', color='orange')\n",
    "    axes[1].set_xlabel('Time (seconds)')\n",
    "    axes[1].set_ylabel('Amplitude (mV)')\n",
    "    axes[1].set_title(f'ECG V5 Data from {start} to {start + duration} seconds')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be useful when deciding portion of data to work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot \"duration\" number of seconds of ecg data starting at \"start\" seconds\n",
    "plot_ecg_interval(df, start=302, duration=1) # Change the start and duration values as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO BE WRITTEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Direct quotation are enclosed in `\"...\"` and are followed by a reference number inside `[]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Izadi, V., Shahri, P.K., & Ahani, H. (2020). A compressed-sensing-based compressor for ECG. *Biomedical Engineering Letters*, 10, 299–307. https://doi.org/10.1007/s13534-020-00148-7\n",
    "\n",
    "[2] __Chapter 3.1 \"Sparsity and Compressed Sensing\" of the Book__:\n",
    "   Brunton, S. L., & Kutz, J. N. (2022). *Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control* (2nd ed.). Cambridge University Press.\n",
    "\n",
    "__Data__:\n",
    "\n",
    "Moody GB, Mark RG. *The impact of the MIT-BIH Arrhythmia Database*. IEEE Eng in Med and Biol 20(3):45-50 (May-June 2001). (PMID: 11446209)\n",
    "\n",
    "Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., ... & Stanley, H. E. (2000). *PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals*. Circulation [Online]. 101 (23), pp. e215–e220.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO BE WRITTEN?\n",
    "\n",
    "For instance explanation of what ecg is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality Assessment Based on PRD and SNR\n",
    "\n",
    "Table 1 from the referenced paper classifies the quality of the reconstructed signal based on the PRD and corresponding SNR values:\n",
    "\n",
    "| Quality        | PRD Range      | SNR Range       |\n",
    "|----------------|----------------|-----------------|\n",
    "| Very Good      | 0% < PRD < 2%  | SNR > 33 dB     |\n",
    "| Good           | 2% < PRD < 9%  | 20 dB < SNR < 33 dB |\n",
    "| Undetermined   | PRD ≥ 9%       | SNR ≤ 20 dB     |\n",
    "\n",
    "This table indicates that when the PRD is less than 2%, the quality of the reconstructed signal can be categorized as \"Very Good.\" For PRD values between 2% and 9%, the quality is considered \"Good,\" and for PRD values above 9%, the quality of the reconstructed signal cannot be precisely determined. \n",
    "\n",
    "__Metric Based on Physician Qualitative Assessments__\n",
    "\n",
    "The classification of the PRD and SNR values into \"Very Good,\" \"Good,\" and \"Undetermined\" categories was established based on a study by Zigel et al., which is referenced in the paper. In this study, a link was established between the diagnostic distortion of ECG signals and the PRD metric. The researchers conducted qualitative assessments with physicians, who evaluated the diagnostic quality of reconstructed ECG signals at different PRD levels.\n",
    "\n",
    "The physicians' qualitative assessments provided a subjective but clinically relevant measure of how much distortion could be tolerated in the reconstructed signals before it began to interfere with accurate diagnosis. These evaluations were then correlated with specific PRD values, allowing the researchers to define thresholds where the signal quality was deemed acceptable or unacceptable for clinical use. For instance, a PRD of less than 2% was consistently associated with minimal diagnostic distortion, leading to its classification as \"Very Good.\" As PRD increased, the likelihood of clinically significant distortion also increased, which was reflected in the \"Good\" and \"Undetermined\" categories.\n",
    "\n",
    "This physician-based qualitative assessment was crucial in grounding the PRD and SNR metrics in practical clinical utility, ensuring that the numerical \n",
    "thresholds corresponded to meaningful diagnostic criteria.\n",
    "\n",
    "_Based on [1] Izadi, V., Shahri, P.K., & Ahani, H. (2020). A compressed-sensing-based compressor for ECG. *Biomedical Engineering Letters*, 10, 299–307. https://doi.org/10.1007/s13534-020-00148-7_\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".namlVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
