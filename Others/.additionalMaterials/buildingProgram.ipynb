{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script provides utility functions for formatted printing of NumPy matrices and saving matrices to CSV files.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# system imports\n",
    "import os\n",
    "\n",
    "# third party imports\n",
    "import numpy as np\n",
    "import wfdb\n",
    "\n",
    "\n",
    "def printFormatted(matrix, decimals=4):\n",
    "    \"\"\"\n",
    "    Prints the matrix with formatted elements aligned in columns for improved readability.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    matrix : numpy array\n",
    "        The matrix to be printed.\n",
    "    decimals : int, optional (default=4)\n",
    "        The number of decimal places for formatting the elements.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    None\n",
    "        This function does not return any value; it prints the formatted matrix directly to the console.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The function aligns columns based on the maximum width needed for the formatted elements, ensuring the matrix is displayed neatly.\n",
    "    - This function is useful for visual inspection of numerical matrices, especially those with varying magnitudes.\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> matrix = np.array([[1.234567, 123.456789], [0.0001234, 1.2345]])\n",
    "    >>> print('Classic print:')\n",
    "    >>> print(matrix)\n",
    "    Classic print:\n",
    "    [[1.2345670e+00 1.2345679e+02]\n",
    "     [1.2340000e-04 1.2345000e+00]]\n",
    "     \n",
    "    >>> print('\\nFormatted print:')\n",
    "    >>> printFormatted(matrix, decimals=4)\n",
    "         1.2346  123.4568\n",
    "         0.0001    1.2345\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Determine the maximum width needed to keep alignment\n",
    "    max_width = max(len(f'{value:.{decimals}f}') for row in matrix for value in row)\n",
    "\n",
    "    # Create a formatted string for each element in the matrix, ensuring alignment\n",
    "    formatted_matrix = '\\n'.join([' '.join([f'{value:>{max_width}.{decimals}f}' for value in row]) for row in matrix])\n",
    "\n",
    "    # Print the formatted matrix\n",
    "    print(formatted_matrix)\n",
    "\n",
    "\n",
    "def py_test_csv(array):\n",
    "    \"\"\"\n",
    "    Save a numpy array as a CSV file in ./debugCsvPy/py_test.csv\n",
    "\n",
    "    Parameters:\n",
    "    array (numpy.ndarray): The input array to be saved as a CSV file.\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    output_dir = 'debugCsvPy'  # Directory where CSV files will be stored\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    py_dict_path = os.path.join(output_dir, 'py_test.csv')\n",
    "    np.savetxt(py_dict_path, array, delimiter=',', fmt='%.6f')\n",
    "\n",
    "\n",
    "def load_signal_from_wfdb(record_name, duration_minutes=None):\n",
    "    \"\"\"\n",
    "    Load an ECG signal from the MIT-BIH Arrhythmia Database using wfdb.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    record_name : str\n",
    "        The record name (e.g., '100' for record 100) from the MIT-BIH Arrhythmia Database.\n",
    "    duration_minutes : int, optional\n",
    "        The duration of the signal to load in minutes. If None, loads the entire signal.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray: The loaded ECG signal (MLII lead).\n",
    "    \"\"\"\n",
    "    # Load the record from the PhysioNet MIT-BIH dataset online\n",
    "    record = wfdb.rdrecord(f'{record_name}', pn_dir='mitdb', channels=[0])  # Load channel 0 (MLII)\n",
    "    fs = record.fs  # Get the sampling frequency\n",
    "\n",
    "    # print fs\n",
    "    print(f\"Sampling frequency: {fs} Hz\")\n",
    "\n",
    "    # print record units\n",
    "    print(f\"Units: {record.units}\")\n",
    "\n",
    "    # Convert from mV to muV\n",
    "    record.p_signal[:, 0] *= 1000\n",
    "    \n",
    "    # Convert the number of minutes to samples\n",
    "    if duration_minutes is not None:\n",
    "        num_samples = int(fs * duration_minutes)\n",
    "        signal = record.p_signal[:num_samples, 0]  # Load specified duration\n",
    "    else:\n",
    "        signal = record.p_signal[:, 0]  # Load entire signal\n",
    "\n",
    "    return signal, record_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"\n",
    "This script provides functions to calculate the Signal-to-Noise Ratio (SNR) between \n",
    "an original and a reconstructed signal, and to plot these signals together, displaying \n",
    "the SNR. It also includes functionality to save the plotted signals to a specified directory.\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def calculate_snr(signal, recovered_signal):\n",
    "    \"\"\"\n",
    "    Calculates the Signal-to-Noise Ratio (SNR) between the original signal and the recovered signal.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : numpy.ndarray\n",
    "        The original signal.\n",
    "    recovered_signal : numpy.ndarray\n",
    "        The recovered signal after some processing or recovery algorithm.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    snr : float\n",
    "        The Signal-to-Noise Ratio (SNR) in decibels (dB).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The SNR is calculated as 20 * log10(norm(original_signal) / norm(original_signal - recovered_signal)).\n",
    "    - A higher SNR value indicates a better recovery, with less error relative to the original signal.\n",
    "    \"\"\"\n",
    "    error = recovered_signal - signal\n",
    "    snr = 20 * np.log10(np.linalg.norm(signal) / np.linalg.norm(error))\n",
    "    \n",
    "    return snr\n",
    "\n",
    "\n",
    "\n",
    "def plot_signals(original_signal, reconstructed_signal, snr=None, original_name=\"Original Signal\", \n",
    "                 reconstructed_name=\"Reconstructed Signal\", save_path=None, filename=None,\n",
    "                 start_pct=0.0, num_samples=None, show_snr_box=False):\n",
    "    \"\"\"\n",
    "    Plots a section of the original signal and the reconstructed signal on the same plot with the given names,\n",
    "    displays the Signal-to-Noise Ratio (SNR) in a text box if enabled, and saves the plot to a specified directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    original_signal : numpy.ndarray\n",
    "        The original signal to be plotted.\n",
    "    \n",
    "    reconstructed_signal : numpy.ndarray\n",
    "        The reconstructed signal to be plotted.\n",
    "    \n",
    "    snr : float, optional (default=None)\n",
    "        The Signal-to-Noise Ratio to display. If None, it will be computed using the original and reconstructed signals.\n",
    "    \n",
    "    original_name : str, optional (default=\"Original Signal\")\n",
    "        The name to display for the original signal in the plot.\n",
    "    \n",
    "    reconstructed_name : str, optional (default=\"Reconstructed Signal\")\n",
    "        The name to display for the reconstructed signal in the plot.\n",
    "    \n",
    "    save_path : str, optional\n",
    "        The directory path where the plot should be saved. If None, the plot will not be saved.\n",
    "    \n",
    "    filename : str, optional\n",
    "        The name of the file to save the plot as. If None and save_path is provided, a default name will be generated.\n",
    "    \n",
    "    start_pct : float, optional (default=0.0)\n",
    "        The percentage (between 0 and 1) of the way through the signal to start plotting. For example, 0.5 means start \n",
    "        from the halfway point of the signals.\n",
    "    \n",
    "    num_samples : int, optional (default=None)\n",
    "        The number of samples to plot from the start point. If None, it will plot to the end of the signals.\n",
    "    \n",
    "    show_snr_box : bool, optional (default=True)\n",
    "        Whether to display the SNR value in a text box on the plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the signals have the same length\n",
    "    if len(original_signal) != len(reconstructed_signal):\n",
    "        raise ValueError(\"The original signal and the reconstructed signal must have the same length.\")\n",
    "    \n",
    "    # Calculate the start index based on percentage\n",
    "    start_idx = int(start_pct * len(original_signal))\n",
    "    \n",
    "    # Determine the end index based on num_samples\n",
    "    if num_samples is not None:\n",
    "        end_idx = start_idx + num_samples\n",
    "    else:\n",
    "        end_idx = len(original_signal)\n",
    "    \n",
    "    # Ensure that the end index does not exceed the signal length\n",
    "    end_idx = min(end_idx, len(original_signal))\n",
    "\n",
    "    # Slice the signals to the selected section\n",
    "    original_signal_section = original_signal[start_idx:end_idx]\n",
    "    reconstructed_signal_section = reconstructed_signal[start_idx:end_idx]\n",
    "    \n",
    "    # Calculate SNR if not provided\n",
    "    if snr is None and show_snr_box:\n",
    "        snr = calculate_snr(original_signal_section, reconstructed_signal_section)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(original_signal_section, label=original_name, color='blue', linewidth=1.5)\n",
    "    plt.plot(reconstructed_signal_section, label=reconstructed_name, color='red', linestyle='--', linewidth=1.5)\n",
    "    \n",
    "    # Title and labels\n",
    "    plt.title(f\"{original_name} vs {reconstructed_name} (Section: {start_pct*100:.1f}% - {num_samples} samples)\")\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Amplitude')\n",
    "    \n",
    "    # Add a legend in the upper-right corner with a white background\n",
    "    plt.legend(loc='upper right', frameon=True, facecolor='white')\n",
    "    \n",
    "    # Display SNR in a text box in the top-left corner if show_snr_box is True\n",
    "    if show_snr_box and snr is not None:\n",
    "        plt.text(0.05, 0.95, f'SNR: {snr:.2f} dB', transform=plt.gca().transAxes,\n",
    "                fontsize=12, verticalalignment='top', horizontalalignment='left',\n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "    # Grid and show plot\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save the plot if a save path is provided\n",
    "    if save_path is not None:\n",
    "        # Ensure the save directory exists\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        # Use provided filename or generate a default one\n",
    "        if filename is None:\n",
    "            filename = f\"{original_name}_vs_{reconstructed_name}_section.png\"\n",
    "        \n",
    "        # Define the file path to save the plot\n",
    "        file_path = os.path.join(save_path, filename)\n",
    "        plt.savefig(file_path)\n",
    "        print(f\"Plot saved to {file_path}\")\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurement matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_DBBD_matrix(M, N):\n",
    "    \"\"\"\n",
    "    Generates a deterministic Diagonally Blocked Block Diagonal (DBBD) matrix.\n",
    "\n",
    "    A DBBD matrix is a type of block diagonal matrix where each block is a square diagonal matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    M : int\n",
    "        Number of rows in the matrix.\n",
    "    N : int\n",
    "        Number of columns in the matrix. Should be a multiple of M.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A : numpy.ndarray\n",
    "        The generated DBBD matrix of shape (M, N).\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If `N` is not a multiple of `M`.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> generate_DBDD_matrix(3, 9)\n",
    "    array([[1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
    "           [0., 0., 0., 1., 1., 1., 0., 0., 0.],\n",
    "           [0., 0., 0., 0., 0., 0., 1., 1., 1.]])\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if N % M != 0:\n",
    "        raise ValueError(\"N should be a multiple of M.\")\n",
    "    \n",
    "    Phi = np.zeros((M, N))\n",
    "    m = N // M\n",
    "    \n",
    "    for i in range(M):\n",
    "        Phi[i, i*m:(i+1)*m] = 1\n",
    "\n",
    "    return Phi\n",
    "\n",
    "\n",
    "def generate_random_matrix(M, N, matrix_type='gaussian'):\n",
    "    \"\"\"\n",
    "    Generates a random matrix based on the specified type.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    M : int\n",
    "        Number of rows in the matrix.\n",
    "    N : int\n",
    "        Number of columns in the matrix.\n",
    "    matrix_type : str, optional (default='gaussian')\n",
    "        The type of random matrix to generate. Options are:\n",
    "        - 'gaussian': A matrix with entries drawn from a normal distribution scaled by 1/M.\n",
    "        - 'scaled_binary': A matrix with binary entries (±0.5), scaled by 1/sqrt(M).\n",
    "        - 'unscaled_binary': A matrix with binary entries (±1), with no scaling.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A : numpy.ndarray\n",
    "        The generated random matrix of shape (M, N).\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If `matrix_type` is not one of the supported types.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> generate_random_matrix(2, 3, matrix_type='gaussian')\n",
    "    array([[ 0.01, -0.02,  0.03],\n",
    "           [-0.04,  0.05, -0.06]])\n",
    "\n",
    "    >>> generate_random_matrix(2, 3, matrix_type='scaled_binary')\n",
    "    array([[-0.5,  0. , -0.5],\n",
    "           [ 0.5, -0.5,  0. ]])\n",
    "    \n",
    "    >>> generate_random_matrix(2, 3, matrix_type='unscaled_binary')\n",
    "    array([[ 1., -1.,  1.],\n",
    "           [-1.,  1., -1.]])\n",
    "    \"\"\"\n",
    "    if matrix_type == 'gaussian':\n",
    "        A = ((1/M)**2) * np.random.randn(M, N)\n",
    "\n",
    "    elif matrix_type == 'scaled_binary':\n",
    "        A = np.random.binomial(1, 0.5, size=(M, N)) - 0.5\n",
    "        A = (1/np.sqrt(M)) * A\n",
    "\n",
    "    elif matrix_type == 'unscaled_binary':\n",
    "        A = np.random.binomial(1, 0.5, size=(M, N)) * 2 - 1\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported matrix type. Choose either 'gaussian', 'scaled_binary', or 'unscaled_binary'.\")\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script provides utility functions for generating a Discrete Cosine Transform (DCT) \n",
    "orthonormal basis matrix and for testing various properties of matrices such as independence \n",
    "of columns, normalization, and coherence.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import scipy.fftpack as fftpack\n",
    "import pywt\n",
    "\n",
    "def dct_dictionary(N):\n",
    "    \"\"\"\n",
    "    Generates a Discrete Cosine Transform (DCT) orthonormal basis matrix.\n",
    "\n",
    "    The DCT basis is commonly used in signal processing and data compression. \n",
    "    It transforms a signal into a sum of cosine functions oscillating at different frequencies. \n",
    "    The resulting matrix can be used for orthogonal transformations of signals.\n",
    "    \n",
    "    DCT basis is sparifying.\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    N : int\n",
    "        The size of the dictionary (i.e., the length of the signal).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict_matrix : numpy.ndarray\n",
    "        The generated DCT dictionary matrix of shape (N, N), where each column represents \n",
    "        a DCT basis vector.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> generate_dct_dictionary(4)\n",
    "    array([[ 0.5       ,  0.5       ,  0.5       ,  0.5       ],\n",
    "           [ 0.65328148,  0.27059805, -0.27059805, -0.65328148],\n",
    "           [ 0.5       , -0.5       , -0.5       ,  0.5       ],\n",
    "           [ 0.27059805, -0.65328148,  0.65328148, -0.27059805]])\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate a DCT basis dictionary\n",
    "    dict_matrix = fftpack.dct(np.eye(N), norm='ortho')    \n",
    "    return dict_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive dictionary learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Orthogonal Matching Pursuit (OMP) algorithm for sparse coding.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def OMP(dictio, sig, max_coeff):\n",
    "    \"\"\"\n",
    "    Orthogonal Matching Pursuit (OMP) algorithm for sparse coding.\n",
    "\n",
    "    This function implements the OMP algorithm, which is used to find the sparse\n",
    "    representation of a signal over a given dictionary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dictio : numpy.ndarray\n",
    "        The dictionary to use for sparse coding. It should be a matrix of size (n x K), \n",
    "        where n is the signal dimension and K is the number of atoms in the dictionary.\n",
    "        (its columns MUST be normalized).\n",
    "    \n",
    "    sig : numpy.ndarray\n",
    "        The signals to represent using the dictionary. \n",
    "        It should be a matrix of size (n x N), where N is the number of signals.\n",
    "    \n",
    "    max_coeff : int\n",
    "        The maximum number of coefficients to use for representing each signal.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    s : numpy.ndarray\n",
    "        The sparse representation of the signals over the dictionary.\n",
    "        It should be a matrix of size (K x N).\n",
    "    \"\"\"\n",
    "\n",
    "    [n, p] = sig.shape\n",
    "    [_, key] = dictio.shape\n",
    "    s = np.zeros((key, p))\n",
    "    for k in range(p):\n",
    "        x = sig[:, k]\n",
    "        residual = x.copy()\n",
    "        indx = np.array([], dtype=int)\n",
    "        current_atoms = np.empty((n, 0))\n",
    "        norm_x = np.linalg.norm(x)\n",
    "        for j in range(max_coeff):\n",
    "            proj = dictio.T @ residual\n",
    "            pos = np.argmax(np.abs(proj))\n",
    "            indx = np.append(indx, pos)\n",
    "            # Update selected atoms matrix\n",
    "            current_atoms = np.column_stack((current_atoms, dictio[:, pos]))\n",
    "            # Solve least squares problem using QR decomposition for stability\n",
    "            q, r = np.linalg.qr(current_atoms)\n",
    "            a = np.linalg.solve(r, q.T @ x)\n",
    "            residual = x - current_atoms @ a\n",
    "            # Break if norm of residual is suff small (relative to original signal)\n",
    "            if np.linalg.norm(residual) < 1e-6 * norm_x:\n",
    "                break\n",
    "        temp = np.zeros((key,))\n",
    "        temp[indx] = a\n",
    "        s[:, k] = temp\n",
    "\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MOD (Method of Optimal Directions) algorithm for dictionary learning with improved numerical stability.\n",
    "\"\"\"\n",
    "\n",
    "# system imports\n",
    "import os\n",
    "\n",
    "# third party imports\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.linalg import solve\n",
    "\n",
    "\n",
    "\n",
    "def I_findDistanceBetweenDictionaries(original, new):\n",
    "    \"\"\"\n",
    "    Calculates the distance between two dictionaries.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    original : numpy.ndarray\n",
    "        The original dictionary.\n",
    "\n",
    "    new : numpy.ndarray\n",
    "        The new dictionary.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    catchCounter : int\n",
    "        The number of elements that satisfy the condition errorOfElement < 0.01.\n",
    "    totalDistances : float\n",
    "        The sum of all errorOfElement values.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # first: all the columns in the original start with positive values\n",
    "    catchCounter = 0\n",
    "    totalDistances = 0\n",
    "\n",
    "    for i in range(new.shape[1]):\n",
    "        new[:,i] = np.sign(new[0,i]) * new[:,i]\n",
    "\n",
    "    for i in range(original.shape[1]):\n",
    "        d = np.sign(original[0,i]) * original[:,i]\n",
    "        distances = np.sum(new - np.tile(d, (1, new.shape[1])), axis=0)\n",
    "        index = np.argmin(distances)\n",
    "        errorOfElement = 1 - np.abs(new[:,index].T @ d)\n",
    "        totalDistances += errorOfElement\n",
    "        catchCounter += errorOfElement < 0.01\n",
    "\n",
    "    ratio = catchCounter / original.shape[1]\n",
    "    return ratio, totalDistances\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def MOD(data, parameters):\n",
    "    \"\"\"\n",
    "    Method of Optimal Directions (MOD) algorithm for dictionary learning .\n",
    "\n",
    "    The MOD algorithm is a method for learning a dictionary for sparse representation of signals.\n",
    "    It iteratively updates the dictionary to best represent the input data with sparse coefficients\n",
    "    using the Orthogonal Matching Pursuit (OMP) algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        An (n x N) matrix containing N signals, each of dimension n.\n",
    "    \n",
    "    parameters : dict\n",
    "        A dictionary containing the parameters for the MOD algorithm:\n",
    "            - K : int\n",
    "                The number of dictionary elements (columns) to train.\n",
    "            \n",
    "            - num_iterations : int\n",
    "                The number of iterations to perform for dictionary learning.\n",
    "            \n",
    "            - initialization_method : str\n",
    "                Method to initialize the dictionary. Options are:\n",
    "                * 'DataElements' - Initializes the dictionary using the first K data signals.\n",
    "                * 'GivenMatrix' - Initializes the dictionary using a provided matrix \n",
    "                  (requires 'initial_dictionary' key).\n",
    "\n",
    "            - initial_dictionary : numpy.ndarray, optional\n",
    "                The initial dictionary matrix to use if 'initialization_method' is \n",
    "                set to 'GivenMatrix'. It should be of size (n x K).\n",
    "\n",
    "            - L : int\n",
    "                The number of non-zero coefficients to use in OMP for sparse\n",
    "                representation of each signal.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dictionary : numpy.ndarray\n",
    "        The trained dictionary of size (n x K), where each column is a dictionary element.\n",
    "\n",
    "    coef_matrix : numpy.ndarray\n",
    "        The coefficient matrix of size (K x N), representing the sparse representation\n",
    "        of the input data using the trained dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the number of signals is smaller than the dictionary size\n",
    "    if data.shape[1] < parameters['K']:\n",
    "        print(\"MOD: number of training signals is smaller than the dictionary size. Returning trivial solution...\")\n",
    "        dictionary = data[:, :data.shape[1]]\n",
    "        coef_matrix = np.eye(data.shape[1])  # Trivial coefficients\n",
    "        return dictionary, coef_matrix\n",
    "\n",
    "    # Initialize dictionary based on the specified method\n",
    "    if parameters['initialization_method'] == 'DataElements':\n",
    "        dictionary = data[:, :parameters['K']]\n",
    "    elif parameters['initialization_method'] == 'GivenMatrix':\n",
    "        if 'initial_dictionary' not in parameters:\n",
    "            raise ValueError(\"initial_dictionary parameter is required when \"\n",
    "                             \"initialization_method is set to 'GivenMatrix'.\")\n",
    "        dictionary = parameters['initial_dictionary']\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Invalid value for initialization_method. Choose 'DataElements' or 'GivenMatrix'.\")\n",
    "\n",
    "    # Convert to float64 for precision\n",
    "    dictionary = dictionary.astype(np.float64)\n",
    "\n",
    "    # Normalize dictionary columns and avoid division by zero\n",
    "    column_norms = np.linalg.norm(dictionary, axis=0)\n",
    "    column_norms[column_norms < 1e-10] = 1  # Prevent division by zero\n",
    "    dictionary /= column_norms\n",
    "\n",
    "    # Ensure positive first elements\n",
    "    dictionary *= np.sign(dictionary[0, :])\n",
    "\n",
    "    prev_dictionary = dictionary.copy()\n",
    "\n",
    "    # Run MOD algorithm\n",
    "    for iter_num in range(parameters['num_iterations']):\n",
    "        # Step 1: Sparse coding using OMP\n",
    "        coef_matrix = OMP(dictionary, data, parameters['L'])\n",
    "\n",
    "        # Step 2: Update the dictionary\n",
    "        regularization_term = 1e-7 * sp.eye(coef_matrix.shape[0])\n",
    "        matrix_a = coef_matrix @ coef_matrix.T + regularization_term.toarray()\n",
    "\n",
    "        # Use pinv for numerical stability: \"lstsq\" or \"regularizations\" could also work\n",
    "        dictionary = data @ coef_matrix.T @ np.linalg.pinv(matrix_a)\n",
    "\n",
    "\n",
    "        # Normalize dictionary columns and avoid division by zero\n",
    "        column_norms = np.linalg.norm(dictionary, axis=0)\n",
    "        column_norms[column_norms < 1e-10] = 1  # Prevent division by zero\n",
    "        dictionary /= column_norms\n",
    "\n",
    "        # Ensure positive first elements\n",
    "        dictionary *= np.sign(dictionary[0, :])\n",
    "\n",
    "        # Convergence check\n",
    "        if np.linalg.norm(dictionary - prev_dictionary) < 1e-5:\n",
    "            print(f\"MOD converged after {iter_num + 1} iterations.\")\n",
    "            break\n",
    "\n",
    "        prev_dictionary = dictionary.copy()\n",
    "\n",
    "    return dictionary, coef_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "K-SVD algorithm for dictionary learning and sparse coding using Orthogonal Matching Pursuit (OMP).\n",
    "Includes functions for updating dictionary elements, handling singular value decomposition (SVD)\n",
    "for vectors, and clearing redundant dictionary elements.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "\n",
    "def svds_vector(v):\n",
    "    \"\"\"\n",
    "    Handle SVD for a vector or a 2D matrix with one dimension equal to 1.\n",
    "    \"\"\"\n",
    "    v = np.asarray(v)\n",
    "    \n",
    "    if v.ndim == 1:\n",
    "        v = v.reshape(-1, 1)\n",
    "    elif v.ndim == 2 and (v.shape[0] == 1 or v.shape[1] == 1):\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a vector or a 2D array with one dimension equal to 1.\")\n",
    "    \n",
    "    s = np.linalg.norm(v)\n",
    "    if s > 0:\n",
    "        u = v / s\n",
    "    else:\n",
    "        u = np.zeros_like(v)\n",
    "    \n",
    "    vt = np.array([[1]])\n",
    "\n",
    "    return u, s, vt\n",
    "\n",
    "def I_findBetterDictionaryElement(data, dictionary, j, coeff_matrix, numCoefUsed=1):\n",
    "    \"\"\"\n",
    "    Update the j-th dictionary element.\n",
    "    \"\"\"\n",
    "    relevantDataIndices = np.nonzero(coeff_matrix[j, :])[0]\n",
    "    if relevantDataIndices.size == 0:\n",
    "        errorMat = data - dictionary @ coeff_matrix\n",
    "        errorNormVec = np.sum(errorMat ** 2, axis=0)\n",
    "        i = np.argmax(errorNormVec)\n",
    "        betterDictionaryElement = data[:, i] / np.linalg.norm(data[:, i])\n",
    "        betterDictionaryElement *= np.sign(betterDictionaryElement[0])\n",
    "        coeff_matrix[j, :] = 0\n",
    "        newVectAdded = 1\n",
    "        return betterDictionaryElement, coeff_matrix, newVectAdded\n",
    "    \n",
    "    newVectAdded = 0\n",
    "    tmpCoefMatrix = coeff_matrix[:, relevantDataIndices]\n",
    "    tmpCoefMatrix[j, :] = 0\n",
    "    errors = data[:, relevantDataIndices] - dictionary @ tmpCoefMatrix\n",
    "\n",
    "    if np.min(errors.shape) <= 1:\n",
    "        u, s, vt = svds_vector(errors)\n",
    "        betterDictionaryElement = u\n",
    "        singularValue = s\n",
    "        betaVector = vt\n",
    "    else:\n",
    "        u, s, vt = svds(errors, k=1)\n",
    "        betterDictionaryElement = u[:, 0]\n",
    "        singularValue = s[0]\n",
    "        betaVector = vt[0, :]\n",
    "\n",
    "    coeff_matrix[j, relevantDataIndices] = singularValue * betaVector.T\n",
    "\n",
    "    return betterDictionaryElement, coeff_matrix, newVectAdded\n",
    "\n",
    "def I_clearDictionary(dictionary, coeff_matrix, data):\n",
    "    \"\"\"\n",
    "    Clear or replace redundant dictionary elements.\n",
    "    \"\"\"\n",
    "    T2 = 0.99\n",
    "    T1 = 3\n",
    "    K = dictionary.shape[1]\n",
    "    Er = np.sum((data - dictionary @ coeff_matrix) ** 2, axis=0)\n",
    "    G = dictionary.T @ dictionary\n",
    "    G -= np.diag(np.diag(G))\n",
    "    for jj in range(K):\n",
    "        if np.max(G[jj, :]) > T2 or np.count_nonzero(np.abs(coeff_matrix[jj, :]) > 1e-7) <= T1:\n",
    "            pos = np.argmax(Er)\n",
    "            Er[pos] = 0\n",
    "            dictionary[:, jj] = data[:, pos] / np.linalg.norm(data[:, pos])\n",
    "            G = dictionary.T @ dictionary\n",
    "            G -= np.diag(np.diag(G))\n",
    "    return dictionary\n",
    "\n",
    "def KSVD(data, param):\n",
    "    \"\"\"\n",
    "    K-SVD algorithm for dictionary learning.\n",
    "    \"\"\"\n",
    "    if param['preserve_dc_atom'] > 0:\n",
    "        fixedDictElem = np.zeros((data.shape[0], 1))  \n",
    "        fixedDictElem[:data.shape[0], 0] = 1 / np.sqrt(data.shape[0])\n",
    "    else:\n",
    "        fixedDictElem = np.empty((0, 0))\n",
    "\n",
    "    if data.shape[1] < param['K']:\n",
    "        print('KSVD: number of training data is smaller than the dictionary size. Trivial solution...')\n",
    "        dictionary = data[:, :data.shape[1]]\n",
    "        coef_matrix = np.eye(data.shape[1])\n",
    "        return dictionary, coef_matrix\n",
    "    \n",
    "    dictionary = np.zeros((data.shape[0], param['K']), dtype=np.float64)    \n",
    "    if param['initialization_method'] == 'DataElements':\n",
    "        dictionary[:, :param['K'] - param['preserve_dc_atom']] = \\\n",
    "            data[:, :param['K'] - param['preserve_dc_atom']]\n",
    "    elif param['initialization_method'] == 'GivenMatrix':\n",
    "        dictionary[:, :param['K'] - param['preserve_dc_atom']] = \\\n",
    "            param['initial_dictionary'][:, :param['K'] - param['preserve_dc_atom']]\n",
    "\n",
    "    if param['preserve_dc_atom']:\n",
    "        tmpMat = np.linalg.lstsq(dictionary + 1e-7 * np.eye(dictionary.shape[1]), fixedDictElem, rcond=None)[0]\n",
    "        dictionary -= fixedDictElem @ tmpMat\n",
    "\n",
    "    column_norms = np.sqrt(np.sum(dictionary ** 2, axis=0))\n",
    "    column_norms[column_norms < 1e-10] = 1\n",
    "    dictionary /= column_norms\n",
    "    dictionary *= np.sign(dictionary[0, :])\n",
    "\n",
    "    for iterNum in range(param['num_iterations']):\n",
    "        coef_matrix = OMP(\n",
    "            np.hstack((fixedDictElem, dictionary)) if fixedDictElem.size > 0 else dictionary,\n",
    "            data,\n",
    "            param['L']\n",
    "        )\n",
    "        \n",
    "        rand_perm = np.random.permutation(dictionary.shape[1])\n",
    "        for j in rand_perm:\n",
    "            betterDictElem, coef_matrix, newVectAdded = I_findBetterDictionaryElement(\n",
    "                data,\n",
    "                np.hstack((fixedDictElem, dictionary)) if fixedDictElem.size > 0 else dictionary,\n",
    "                j + fixedDictElem.shape[1],\n",
    "                coef_matrix,\n",
    "                param['L']\n",
    "            )\n",
    "\n",
    "            dictionary[:, j] = betterDictElem.ravel()\n",
    "            if param['preserve_dc_atom']:\n",
    "                tmpCoeff = np.linalg.lstsq(betterDictElem + 1e-7, fixedDictElem, rcond=None)[0]\n",
    "                dictionary[:, j] -= fixedDictElem @ tmpCoeff\n",
    "                dictionary[:, j] /= np.linalg.norm(dictionary[:, j])\n",
    "\n",
    "        dictionary = I_clearDictionary(dictionary, coef_matrix[fixedDictElem.shape[1]:, :], data)\n",
    "\n",
    "    dictionary = np.hstack((fixedDictElem, dictionary)) if fixedDictElem.size > 0 else dictionary\n",
    "    \n",
    "    return dictionary, coef_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## ------------------------------------------------------------------------------------------------\n",
    "## REST OF THE FUNCTIONS ARE FOR TESTING PURPOSES\n",
    "## ------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "def compute_independent_columns(A, tol=1e-10):\n",
    "    \"\"\"\n",
    "    Computes the independent columns of a matrix using the QR decomposition.\n",
    "\n",
    "    The function identifies independent columns of a given matrix `A` by performing a QR \n",
    "    decomposition. It selects columns corresponding to non-zero diagonal elements of the \n",
    "    `R` matrix, which are considered linearly independent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : numpy.ndarray\n",
    "        The matrix for which to compute the independent columns.\n",
    "    tol : float, optional (default=1e-10)\n",
    "        The tolerance value for considering diagonal elements of `R` as non-zero.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ind_cols : numpy.ndarray\n",
    "        A matrix containing the independent columns of `A`.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The QR decomposition is used to determine the rank of the matrix `A`.\n",
    "    - Columns corresponding to non-zero diagonal elements of the `R` matrix are considered independent.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "    >>> compute_independent_columns(A)\n",
    "    array([[1, 2],\n",
    "           [4, 5],\n",
    "           [7, 8]])\n",
    "    \"\"\"\n",
    "    # Perform the QR decomposition\n",
    "    Q, R = np.linalg.qr(A)\n",
    "\n",
    "    # Find the independent columns based on the rank of R\n",
    "    rank = np.sum(np.abs(np.diagonal(R)) > tol)\n",
    "    ind_cols = A[:, :rank]\n",
    "\n",
    "    return ind_cols\n",
    "\n",
    "def check_normalization(A):\n",
    "    \"\"\"\n",
    "    Checks if the columns of a matrix are normalized (i.e., each column has a unit norm).\n",
    "\n",
    "    The function calculates the norm of each column in the matrix `A` and checks if all \n",
    "    column norms are close to 1.0, which indicates normalization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : numpy.ndarray\n",
    "        The matrix to check for normalization.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    is_normalized : bool\n",
    "        True if all columns of `A` are normalized, False otherwise.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> A = np.array([[1, 0], [0, 1]])\n",
    "    >>> check_normalization(A)\n",
    "    True\n",
    "    \"\"\"\n",
    "    column_norms = np.linalg.norm(A, axis=0)\n",
    "    is_normalized = np.allclose(column_norms, 1.0)\n",
    "    return is_normalized\n",
    "\n",
    "\n",
    "def compute_coherence(matrix):\n",
    "    \"\"\"\n",
    "    Computes the coherence of the given matrix.\n",
    "\n",
    "    Coherence is a measure of the maximum correlation between any two columns of a matrix. \n",
    "    It is useful in various applications, such as signal processing and compressed sensing, \n",
    "    to assess the degree of similarity between different columns of the matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : numpy.ndarray\n",
    "        An N x M matrix where coherence is to be calculated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    coherence : float\n",
    "        The coherence of the matrix, defined as the maximum absolute value of the off-diagonal \n",
    "        elements in the Gram matrix of the column-normalized input matrix.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> matrix = np.array([[1, 0], [0, 1]])\n",
    "    >>> compute_coherence(matrix)\n",
    "    0.0\n",
    "    \"\"\"\n",
    "    # Normalize the columns of the matrix\n",
    "    normalized_matrix = matrix / np.linalg.norm(matrix, axis=0, keepdims=True)\n",
    "    \n",
    "    # Compute the Gram matrix (inner products between all pairs of columns)\n",
    "    gram_matrix = np.dot(normalized_matrix.T, normalized_matrix)\n",
    "    \n",
    "    # Remove the diagonal elements (which are all 1's) to only consider distinct columns\n",
    "    np.fill_diagonal(gram_matrix, 0)\n",
    "    \n",
    "    # Compute the coherence as the maximum absolute value of the off-diagonal elements\n",
    "    coherence = np.max(np.abs(gram_matrix))\n",
    "    \n",
    "    return coherence\n",
    "\n",
    "\n",
    "def check_matrix_properties(A):\n",
    "    \"\"\"\n",
    "    Checks various properties of a matrix.\n",
    "\n",
    "    The function checks if the matrix `A` is full rank, if its columns and rows are normalized,\n",
    "    and computes the coherence of the matrix. It also prints the results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : numpy.ndarray\n",
    "        The matrix to check.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> A = np.array([[1, 2], [3, 4]])\n",
    "    >>> check_matrix_properties(A)\n",
    "    \"\"\"\n",
    "    # Check if the matrix is full rank\n",
    "    is_full_rank = np.linalg.matrix_rank(A) == min(A.shape)\n",
    "\n",
    "    # Check if the columns are normalized\n",
    "    is_columns_normalized = check_normalization(A)\n",
    "\n",
    "    # Check if the rows are normalized\n",
    "    is_rows_normalized = check_normalization(A.T)\n",
    "\n",
    "    # Compute the coherence of the matrix\n",
    "    coherence = compute_coherence(A)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Is full rank:\", is_full_rank)\n",
    "    print(\"Are columns normalized:\", is_columns_normalized)\n",
    "    print(\"Are rows normalized:\", is_rows_normalized)\n",
    "    print(\"Coherence:\", coherence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovery Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SL0 Algorithm Implementation\n",
    "\n",
    "This file contains an implementation of the Smoothed L0 (SL0) algorithm for sparse signal recovery.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def SL0(y, A, sigma_min, sigma_decrease_factor=0.5, mu_0=2, L=3, A_pinv=None, showProgress=False):\n",
    "    \"\"\"\n",
    "    Returns the sparsest vector `s` that satisfies the underdetermined system of \n",
    "    linear equations `A @ s = y`, using the Smoothed L0 (SL0) algorithm.\n",
    "\n",
    "    Requires:\n",
    "    --------\n",
    "    - numpy as np\n",
    "    \n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    y : numpy array\n",
    "        The observed vector (Mx1), where M is the number of rows in `A`.\n",
    "    \n",
    "    A : numpy array\n",
    "        The measurement matrix (MxN), which should be 'wide', meaning it has more \n",
    "        columns than rows (N > M). The number of rows in `A` must match the length \n",
    "        of `y`.\n",
    "    \n",
    "    sigma_min : float\n",
    "        The minimum value of `sigma`, which determines the stopping criterion for \n",
    "        the algorithm. It should be chosen based on the noise level or desired \n",
    "        accuracy.\n",
    "    \n",
    "    sigma_decrease_factor : float, optional (default=0.5)\n",
    "        The factor by which `sigma` is decreased in each iteration. This should be \n",
    "        a positive value less than 1. Smaller values lead to quicker reduction of \n",
    "        `sigma`, possibly at the cost of accuracy for less sparse signals.\n",
    "    \n",
    "    mu_0 : float, optional (default=2)\n",
    "        The scaling factor for `mu`, where `mu = mu_0 * sigma^2`. This parameter \n",
    "        influences the convergence rate of the algorithm.\n",
    "    \n",
    "    L : int, optional (default=3)\n",
    "        The number of iterations for the inner loop (steepest descent). Increasing \n",
    "        `L` can improve the precision of the result but also increases computational \n",
    "        cost.\n",
    "    \n",
    "    A_pinv : numpy array, optional\n",
    "        The precomputed pseudoinverse of the matrix `A`. If not provided, it will be \n",
    "        calculated within the function as `np.linalg.pinv(A)`. Providing this value \n",
    "        is beneficial if the function is called repeatedly with the same `A`.\n",
    "    \n",
    "    showProgress : bool, optional (default=False)\n",
    "        If `True`, the function prints the current value of `sigma` during each \n",
    "        iteration, which helps monitor the convergence process.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    s : numpy array\n",
    "        The estimated sparse signal (Nx1) that best satisfies the equation `A @ s = y`.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The algorithm works by iteratively reducing `sigma` in a geometric sequence, \n",
    "      starting with `sigma = 2 * max(abs(s))` and ending with `sigma_min`. At each \n",
    "      step, the function adjusts `s` to minimize the L0-norm by smoothing it using \n",
    "      a Gaussian kernel.\n",
    "    \n",
    "    - The choice of `sigma_min` is crucial: for noiseless cases, a smaller `sigma_min` \n",
    "      yields a sparser solution; for noisy cases, `sigma_min` should be a few times \n",
    "      the standard deviation of the noise in `s`.\n",
    "\n",
    "    - If `A_pinv` is precomputed and passed as an argument, the function becomes \n",
    "      more efficient, especially in scenarios where it is called repeatedly with the \n",
    "      same `A`.\n",
    "\n",
    "      \n",
    "      References:\n",
    "      ----------\n",
    "    - Original authors (MATLAB): Massoud Babaie-Zadeh, Hossein Mohimani, 4 August 2008.\n",
    "    - Web-page: http://ee.sharif.ir/~SLzero\n",
    "\n",
    "    - Ported to python: RosNaviGator https://github.com/RosNaviGator, 2024\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "    if A_pinv is None:\n",
    "        A_pinv = np.linalg.pinv(A)\n",
    "        \n",
    "    # Initialize the variables\n",
    "    s = A_pinv @ y\n",
    "    sigma = 2 * max(np.abs(s))\n",
    "\n",
    "    # Define lambda function for delta\n",
    "    OurDelta = lambda s, sigma: s * np.exp(-s**2 / sigma**2)\n",
    " \n",
    "    # Main loop\n",
    "    while sigma > sigma_min:\n",
    "        for i in range(L):\n",
    "            delta = OurDelta(s, sigma)\n",
    "            s = s - mu_0 * delta\n",
    "            s = s - A_pinv @ (A @ s - y)\n",
    "        \n",
    "        if showProgress:\n",
    "            print(f'sigma: {sigma}')\n",
    "\n",
    "        sigma = sigma * sigma_decrease_factor\n",
    "\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressed Sensing class (higher level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "class compressedSensing:\n",
    "    def __init__(self, signal, BLOCK_LEN=16, CR=4, matrix_type='gaussian'):\n",
    "        \"\"\"\n",
    "        Constructor for the compressedSensing class.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        signal : numpy.ndarray\n",
    "            The input signal, must be a valid array of real numbers.\n",
    "        BLOCK_LEN : int\n",
    "            The number of rows in the measurement matrix Phi. Default is 16.\n",
    "        CR : int, optional (default=4)\n",
    "            Compression ratio (controls the number of rows in Phi). Must be a positive integer and BLOCK_LEN / CR > 1.\n",
    "        matrix_type : str, optional (default='gaussian')\n",
    "            Type of the matrix to generate ('gaussian', 'DBBD', etc.).\n",
    "        \"\"\"\n",
    "        # Save original parameters\n",
    "        self.ORIGINAL_BLOCK_LEN = BLOCK_LEN\n",
    "        self.CR = CR\n",
    "        self.matrix_type = matrix_type\n",
    "\n",
    "        # Check if signal is valid\n",
    "        if signal is None:\n",
    "            raise ValueError(\"A signal must be provided.\")\n",
    "        \n",
    "        # Ensure signal is a vector-like structure (array or list of real numbers)\n",
    "        if not (isinstance(signal, (list, np.ndarray)) and np.issubdtype(np.array(signal).dtype, np.number)):\n",
    "            raise ValueError(\"The signal must be a valid array or list of numerical values.\")\n",
    "        \n",
    "        self.signal = np.array(signal)  # Convert to numpy array if it isn't already\n",
    "\n",
    "        # Check that BLOCK_LEN and CR are valid\n",
    "        if not isinstance(BLOCK_LEN, int) or BLOCK_LEN <= 0:\n",
    "            raise ValueError(\"BLOCK_LEN must be a positive integer.\")\n",
    "        if not isinstance(CR, int) or CR <= 2:\n",
    "            raise ValueError(\"CR must be a positive integer greater than 2.\")\n",
    "        if BLOCK_LEN % CR != 0 or BLOCK_LEN // CR <= 1:\n",
    "            raise ValueError(\"BLOCK_LEN must be divisible by CR, and BLOCK_LEN / CR must be greater than 1.\")\n",
    "        \n",
    "        self.BLOCK_LEN = BLOCK_LEN\n",
    "        self.COMP_LEN = BLOCK_LEN // CR  # Compression length (number of rows in Phi)\n",
    "\n",
    "        # Generate measurement matrix Phi based on the specified type\n",
    "        if matrix_type == 'DBBD':\n",
    "            self.Phi = generate_DBBD_matrix(self.COMP_LEN, self.BLOCK_LEN)\n",
    "        else:\n",
    "            self.Phi = generate_random_matrix(self.COMP_LEN, self.BLOCK_LEN, matrix_type=matrix_type)\n",
    "\n",
    "        # Save original Phi and block length for later resets\n",
    "        self.original_phi = self.Phi\n",
    "\n",
    "        # Initialize other attributes\n",
    "        self.clear()  # Initialize/reset all other attributes to their original state\n",
    "\n",
    "\n",
    "    def clear(self):\n",
    "        \"\"\"\n",
    "        Resets the class to its state after instantiation.\n",
    "        Clears all attributes and brings parameters back to their original state.\n",
    "        \"\"\"\n",
    "        # Clear attributes obtained after the constructor\n",
    "        self.BLOCK_LEN = self.ORIGINAL_BLOCK_LEN\n",
    "        self.COMP_LEN = self.BLOCK_LEN // self.CR\n",
    "        self.Phi = self.original_phi\n",
    "        self.training_set = None\n",
    "        self.training_matrix = None\n",
    "        self.reconstructed_signal = None\n",
    "        self.Y = None\n",
    "        self.theta = None\n",
    "        self.theta_pinv = None\n",
    "        self.coeff_matrix = None\n",
    "        self.is_kron = False\n",
    "\n",
    "\n",
    "\n",
    "    def divide_signal(self, training_percentage):\n",
    "        \"\"\"\n",
    "        Divides the signal into a training set and a test set based on the given percentage.\n",
    "        \"\"\"\n",
    "        training_size = int(training_percentage * len(self.signal))\n",
    "        \n",
    "        # Calculate the time duration in hours and minutes for both training and testing sets\n",
    "        training_minutes = training_size / 360\n",
    "        testing_minutes = (len(self.signal) - training_size) / 360\n",
    "        \n",
    "        training_hours = int(training_minutes // 60)\n",
    "        training_minutes = int(training_minutes % 60)\n",
    "        \n",
    "        testing_hours = int(testing_minutes // 60)\n",
    "        testing_minutes = int(testing_minutes % 60)\n",
    "\n",
    "        # Print the duration for training and testing sets in hours and minutes\n",
    "        print(f\"Training set duration: {training_hours} hour(s) and {training_minutes} minute(s)\")\n",
    "        print(f\"Testing set duration: {testing_hours} hour(s) and {testing_minutes} minute(s)\")\n",
    "        \n",
    "        # Define the training and test sets\n",
    "        self.training_set = self.signal[:training_size]\n",
    "        self.test_set = self.signal[training_size:]\n",
    "\n",
    "        # Ensure the test set size is a multiple of BLOCK_LEN by truncating the test set\n",
    "        test_size = len(self.test_set)\n",
    "        test_size_multiple = (test_size // self.BLOCK_LEN) * self.BLOCK_LEN\n",
    "        self.test_set = self.test_set[:test_size_multiple]\n",
    "\n",
    "        # Ensure the training set size is a multiple of BLOCK_LEN\n",
    "        num_cols = training_size // self.BLOCK_LEN\n",
    "        if num_cols < self.BLOCK_LEN:\n",
    "            warnings.warn(\"The number of samples (columns) in the training matrix is shorter than \"\n",
    "                        \"the number of rows, which can cause issues with dictionary learning.\")\n",
    "\n",
    "        # Reshape the training set using Fortran-style ordering ('F')\n",
    "        self.training_matrix = self.training_set[:num_cols * self.BLOCK_LEN].reshape(self.BLOCK_LEN, num_cols, order='F')\n",
    "\n",
    "        # print training matrix shape\n",
    "        print(f\"Training matrix shape: {self.training_matrix.shape}\")\n",
    "\n",
    "\n",
    "    def compress_test_set(self):\n",
    "        \"\"\"\n",
    "        Compresses the test set using the original measurement matrix (original_phi).\n",
    "        Raises an error if the test set has already been compressed.\n",
    "        \"\"\"\n",
    "        if self.Y is not None:\n",
    "            raise RuntimeError(\"Test set has already been compressed. Recompression is not allowed.\")\n",
    "        \n",
    "        if self.test_set is None:\n",
    "            raise ValueError(\"Test set not defined. Please divide the signal before compressing.\")\n",
    "\n",
    "        M, N = self.original_phi.shape  # Use original_phi instead of Phi\n",
    "        SIGNAL_BLOCKS = len(self.test_set) // N\n",
    "        self.Y = np.zeros((M, SIGNAL_BLOCKS))\n",
    "\n",
    "        # Sampling phase: Compress signal block-wise\n",
    "        for i in range(SIGNAL_BLOCKS):\n",
    "            self.Y[:, i] = self.original_phi @ self.test_set[i * N: (i + 1) * N]  # Use original_phi here\n",
    "\n",
    "\n",
    "\n",
    "    def Y_kron(self):\n",
    "        \"\"\"\n",
    "        Reshapes the compressed signal Y into its Kronecker version by concatenating KRON_FACT consecutive columns.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'KRON_FACT') or self.KRON_FACT is None:\n",
    "            raise ValueError(\"KRON_FACT has not been set. Please activate the Kronecker method before reshaping Y.\")\n",
    "        \n",
    "        # Generate Y_kron from Y by concatenating KRON_FACT consecutive columns\n",
    "        M, SIGNAL_BLOCKS = self.Y.shape\n",
    "        SIGNAL_BLOCKS_KRON = len(self.test_set) // self.BLOCK_LEN  # BLOCK_LEN must be already \"the kronecker one\"\n",
    "        temp_y = np.zeros((M * self.KRON_FACT, SIGNAL_BLOCKS_KRON))\n",
    "\n",
    "        for i in range(SIGNAL_BLOCKS_KRON):\n",
    "            temp_y[:, i] = self.Y[:, i * self.KRON_FACT: (i + 1) * self.KRON_FACT].flatten(order='F')\n",
    "        \n",
    "        self.Y = temp_y\n",
    "\n",
    "\n",
    "    def kronecker_activate(self, KRON_FACT):\n",
    "        \"\"\"\n",
    "        Activates Kronecker compression, adjusting BLOCK_LEN and reprocessing Phi and Y accordingly.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'is_kron') and self.is_kron:\n",
    "            raise ValueError(\"Kronecker compression has already been activated. Cannot activate again.\")\n",
    "\n",
    "        if self.Y is None:\n",
    "            raise ValueError(\"Y has not been computed. Please compress the signal before activating the kronecker method.\")\n",
    "\n",
    "        self.dictionary, self.coeff_matrix = None, None  # Clear dictionary and coefficients\n",
    "        \n",
    "        self.KRON_FACT = KRON_FACT\n",
    "        self.BLOCK_LEN = self.BLOCK_LEN * self.KRON_FACT\n",
    "\n",
    "        # Compute Kronecker product for Phi\n",
    "        self.Phi = np.kron(np.eye(self.KRON_FACT), self.Phi)\n",
    "\n",
    "        # Reprocess the training set if it exists\n",
    "        if self.training_set is not None:\n",
    "            training_size = len(self.training_set)\n",
    "            num_cols = training_size // self.BLOCK_LEN\n",
    "            if num_cols < self.BLOCK_LEN:\n",
    "                warnings.warn(\"The number of samples (columns) in the training matrix is shorter than \"\n",
    "                            \"the number of rows, which can cause issues with dictionary learning.\")\n",
    "            \n",
    "            self.training_matrix = self.training_set[:num_cols * self.BLOCK_LEN].reshape(self.BLOCK_LEN, num_cols, order='F')\n",
    "\n",
    "            # print training matrix shape\n",
    "            print(f\"KRONECKER ACTIVATE Training matrix shape: {self.training_matrix.shape}\")\n",
    "        \n",
    "        self.Y_kron()\n",
    "\n",
    "        # Set the is_kron flag to True\n",
    "        self.is_kron = True\n",
    "\n",
    "\n",
    "    def generate_dictionary(self, dictionary_type='dct', mod_params=None, ksvd_params=None):\n",
    "        \"\"\"\n",
    "        Generates the dictionary based on the specified type. Supports DCT, MOD, and K-SVD.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        dictionary_type : str\n",
    "            The type of dictionary to generate ('dct', 'mod', 'ksvd').\n",
    "        mod_params : dict, optional\n",
    "            Dictionary of parameters for MOD algorithm if using MOD, should include `redundancy`.\n",
    "        ksvd_params : dict, optional\n",
    "            Dictionary of parameters for K-SVD algorithm if using K-SVD, should include `redundancy`.\n",
    "        \"\"\"\n",
    "        if dictionary_type == 'dct':\n",
    "            self.dictionary = dct_dictionary(self.BLOCK_LEN)\n",
    "        elif dictionary_type == 'mod':\n",
    "            if self.training_matrix is None:\n",
    "                raise ValueError(\"Training matrix not defined. Please divide the signal before running MOD.\")\n",
    "            if mod_params is None:\n",
    "                raise ValueError(\"MOD parameters not provided.\")\n",
    "            \n",
    "            # Compute 'K' using redundancy factor and BLOCK_LEN\n",
    "            mod_params['K'] = mod_params['redundancy'] * self.BLOCK_LEN\n",
    "            \n",
    "            # Run MOD algorithm with training matrix and mod_params\n",
    "            self.dictionary, self.coeff_matrix = MOD(self.training_matrix, mod_params)\n",
    "        elif dictionary_type == 'ksvd':\n",
    "            if self.training_matrix is None:\n",
    "                raise ValueError(\"Training matrix not defined. Please divide the signal before running K-SVD.\")\n",
    "            if ksvd_params is None:\n",
    "                raise ValueError(\"K-SVD parameters not provided.\")\n",
    "            \n",
    "            # Compute 'K' using redundancy factor and BLOCK_LEN\n",
    "            ksvd_params['K'] = ksvd_params['redundancy'] * self.BLOCK_LEN\n",
    "            \n",
    "            # Run K-SVD algorithm with training matrix and ksvd_params\n",
    "            self.dictionary, self.coeff_matrix = KSVD(self.training_matrix, ksvd_params)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported dictionary type. Use 'dct', 'mod', or 'ksvd'.\")\n",
    "\n",
    "    def recover_signal(self, sl0_params=None):\n",
    "        \"\"\"\n",
    "        Recovers the original signal using the SL0 algorithm, after compressing.\n",
    "        \"\"\"\n",
    "        if self.Y is None:\n",
    "            raise ValueError(\"Test set has not been compressed. Please compress the signal first.\")\n",
    "        if self.dictionary is None:\n",
    "            raise ValueError(\"Dictionary has not been generated. Please generate a dictionary before recovery.\")\n",
    "        \n",
    "        M, N = self.Phi.shape\n",
    "        SIGNAL_BLOCKS = self.Y.shape[1]\n",
    "        reconstructed_signal = np.zeros(N * SIGNAL_BLOCKS)\n",
    "\n",
    "        # Precompute theta and theta_pinv\n",
    "        self.theta = self.Phi @ self.dictionary\n",
    "        self.theta_pinv = np.linalg.pinv(self.theta)\n",
    "\n",
    "        # Set default SL0 parameters and update with user-provided values\n",
    "        default_sl0_params = {\n",
    "            'sigma_min': 1e-4,\n",
    "            'sigma_decrease_factor': 0.5,\n",
    "            'mu_0': 2,\n",
    "            'L': 3,\n",
    "            'showProgress': False\n",
    "        }\n",
    "        \n",
    "        if sl0_params is not None:\n",
    "            default_sl0_params.update(sl0_params)\n",
    "\n",
    "        # SL0 recovery for each block\n",
    "        for i in range(SIGNAL_BLOCKS):\n",
    "            y = self.Y[:, i]\n",
    "\n",
    "            # SL0: Sparse reconstruction using the parameters\n",
    "            xp = SL0(\n",
    "                y, self.theta,\n",
    "                sigma_min=default_sl0_params['sigma_min'],\n",
    "                sigma_decrease_factor=default_sl0_params['sigma_decrease_factor'],\n",
    "                mu_0=default_sl0_params['mu_0'],\n",
    "                L=default_sl0_params['L'],\n",
    "                A_pinv=self.theta_pinv,\n",
    "                showProgress=default_sl0_params['showProgress']\n",
    "            )\n",
    "\n",
    "            # Recovery Phase: Reconstruct the original signal\n",
    "            reconstructed_signal[i * N : (i + 1) * N] = self.dictionary @ xp\n",
    "\n",
    "        # Store the reconstructed signal as an attribute\n",
    "        self.reconstructed_signal = reconstructed_signal\n",
    "\n",
    "\n",
    "\n",
    "    def plot_reconstructed_vs_original(self, save_path=None, filename=None, start_pct=0.0, num_samples=None, \n",
    "                                   reconstructed_label=\"Reconstructed Signal\", show_snr_box=False):\n",
    "        \"\"\"\n",
    "        Plots the original test set against the reconstructed signal, with an option to display SNR in the plot.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        save_path : str, optional\n",
    "            The directory path where the plot should be saved. If None, the plot will not be saved.\n",
    "        \n",
    "        filename : str, optional\n",
    "            The name of the file to save the plot as. If None and save_path is provided, a default name will be generated.\n",
    "        \n",
    "        start_pct : float, optional (default=0.0)\n",
    "            The percentage (between 0 and 1) of the way through the signal to start plotting. \n",
    "        \n",
    "        num_samples : int, optional (default=None)\n",
    "            The number of samples to plot from the start point. If None, it will plot to the end of the signals.\n",
    "        \n",
    "        reconstructed_label : str, optional (default=\"Reconstructed Signal\")\n",
    "            The name to display for the reconstructed signal in the plot.\n",
    "        \n",
    "        show_snr_box : bool, optional (default=False)\n",
    "            Whether to display the SNR value in a text box on the plot.\n",
    "        \"\"\"\n",
    "        if self.reconstructed_signal is None:\n",
    "            raise ValueError(\"Reconstructed signal not found. Please call recover_signal() first.\")\n",
    "        \n",
    "        if self.test_set is None:\n",
    "            raise ValueError(\"Test set not found. Please divide the signal before plotting.\")\n",
    "\n",
    "        # Check if the lengths of the signals match\n",
    "        if len(self.test_set) != len(self.reconstructed_signal):\n",
    "            warnings.warn(\"The original and reconstructed signals have different lengths. \"\n",
    "                        \"They will both be plotted up to the length of the shorter signal.\")\n",
    "\n",
    "        # Calculate the start index based on percentage\n",
    "        total_samples = min(len(self.test_set), len(self.reconstructed_signal))\n",
    "        start_idx = int(start_pct * total_samples)\n",
    "\n",
    "        # Determine the end index based on num_samples or plot till the end if num_samples is None\n",
    "        if num_samples is None:\n",
    "            end_idx = total_samples\n",
    "        else:\n",
    "            end_idx = min(start_idx + num_samples, total_samples)\n",
    "\n",
    "        # Slice the signals for plotting\n",
    "        original_signal_section = self.test_set[start_idx:end_idx]\n",
    "        reconstructed_signal_section = self.reconstructed_signal[start_idx:end_idx]\n",
    "\n",
    "        # Calculate SNR between the original test set and the reconstructed signals\n",
    "        snr = calculate_snr(original_signal_section, reconstructed_signal_section) if show_snr_box else None\n",
    "\n",
    "        # Plot the selected section of the signals and display SNR\n",
    "        plot_signals(\n",
    "            original_signal_section, \n",
    "            reconstructed_signal_section, \n",
    "            snr=snr, \n",
    "            original_name=\"Original Signal\",  # Fixed label for original signal\n",
    "            reconstructed_name=reconstructed_label,  # Custom label for reconstructed signal\n",
    "            save_path=save_path, \n",
    "            filename=filename,\n",
    "            start_pct=start_pct,\n",
    "            num_samples=len(original_signal_section),  # Update plot title with actual number of samples being plotted\n",
    "            show_snr_box=show_snr_box  # Use the SNR box toggle\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_measurement_matrix(self):\n",
    "        \"\"\"Retrieves the measurement matrix Phi.\"\"\"\n",
    "        return self.Phi\n",
    "\n",
    "    def get_compressed_signal(self):\n",
    "        \"\"\"Retrieves the compressed signal Y.\"\"\"\n",
    "        return self.Y\n",
    "\n",
    "    def get_dictionary(self):\n",
    "        \"\"\"Retrieves the generated dictionary.\"\"\"\n",
    "        return self.dictionary\n",
    "\n",
    "    def get_coeff_matrix(self):\n",
    "        \"\"\"Retrieves the coefficient matrix from MOD or K-SVD algorithm.\"\"\"\n",
    "        if self.coeff_matrix is None:\n",
    "            raise ValueError(\"The coefficient matrix has not been generated yet. Call generate_dictionary() with MOD or K-SVD first.\")\n",
    "        return self.coeff_matrix\n",
    "\n",
    "    def get_reconstructed_signal(self):\n",
    "        \"\"\"Retrieves the reconstructed signal after applying SL0.\"\"\"\n",
    "        if self.reconstructed_signal is None:\n",
    "            raise ValueError(\"The signal has not been reconstructed yet. Call recover_signal() first.\")\n",
    "        return self.reconstructed_signal\n",
    "\n",
    "    def get_original_signal(self):\n",
    "        \"\"\"Retrieves the original test signal that was passed to the class.\"\"\"\n",
    "        return self.test_set\n",
    "\n",
    "    def get_theta(self):\n",
    "        \"\"\"Retrieves theta (Phi @ dictionary).\"\"\"\n",
    "        if self.theta is None:\n",
    "            raise ValueError(\"theta has not been computed yet. Call recover_signal() first.\")\n",
    "        return self.theta\n",
    "\n",
    "    def get_theta_pinv(self):\n",
    "        \"\"\"Retrieves theta_pinv (pseudoinverse of Phi @ dictionary).\"\"\"\n",
    "        if self.theta_pinv is None:\n",
    "            raise ValueError(\"theta_pinv has not been computed yet. Call recover_signal() first.\")\n",
    "        return self.theta_pinv\n",
    "    \n",
    "    def get_test_set(self):\n",
    "        \"\"\"Retrieves the test set.\"\"\"\n",
    "        if self.test_set is None:\n",
    "            raise ValueError(\"Test set not defined. Please divide the signal before retrieving.\")\n",
    "        return self.test_set\n",
    "\n",
    "    def get_training_set(self):\n",
    "        \"\"\"Retrieves the training set.\"\"\"\n",
    "        if self.training_set is None:\n",
    "            raise ValueError(\"Training set not defined. Please divide the signal before retrieving.\")\n",
    "        return self.training_set\n",
    "\n",
    "    def get_snr(self):\n",
    "        \"\"\"\n",
    "        Computes and returns the Signal-to-Noise Ratio (SNR) between the original test set and the reconstructed signal.\n",
    "        \"\"\"\n",
    "        if self.test_set is None:\n",
    "            raise ValueError(\"Test set not found. Please divide the signal before computing SNR.\")\n",
    "        \n",
    "        if self.reconstructed_signal is None:\n",
    "            raise ValueError(\"Reconstructed signal not found. Please call recover_signal() first.\")\n",
    "        \n",
    "        # Ensure both signals have the same length by truncating to the shorter one\n",
    "        total_samples = min(len(self.test_set), len(self.reconstructed_signal))\n",
    "        \n",
    "        original_signal_section = self.test_set[:total_samples]\n",
    "        reconstructed_signal_section = self.reconstructed_signal[:total_samples]\n",
    "        \n",
    "        return calculate_snr(original_signal_section, reconstructed_signal_section)\n",
    "\n",
    "    def extract_model(self):\n",
    "        \"\"\"\n",
    "        Extracts the current state of the model for future recovery.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        model : dict\n",
    "            A dictionary containing 'phi', 'dict', 'is_kron', and 'original_phi'.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'phi': self.Phi,\n",
    "            'dict': self.dictionary,\n",
    "            'is_kron': self.is_kron,  # Use the is_kron flag directly\n",
    "            'original_phi': self.original_phi  # Always return the original version of Phi\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main test script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "\n",
    "# Toggle for showing min and max lines\n",
    "show_min_max = True  # Set this to False if you don't want to show min/max lines\n",
    "\n",
    "# Number of repetitions, REPS=1 will plot the results, REPS>1 will plot averages\n",
    "REPS = 0\n",
    "\n",
    "# Define matrix type (can be changed)\n",
    "matrix_type = 'unscaled_binary'  # Use 'DBBD', 'gaussian', 'scaled_binary', 'unscaled_binary'\n",
    "\n",
    "# MOD and K-SVD dictionary learning parameters\n",
    "mod_params = {\n",
    "    'redundancy': 1,  # This will translate to 'K = redundancy * BLOCK_LEN'\n",
    "    'num_iterations': 10,\n",
    "    'initialization_method': 'DataElements',  # Use 'DataElements' for initialization\n",
    "    'L': 4  # Number of non-zero coefficients to use in OMP\n",
    "}\n",
    "\n",
    "ksvd_params = {\n",
    "    'redundancy': 1,  # This will translate to 'K = redundancy * BLOCK_LEN'\n",
    "    'num_iterations': 10,\n",
    "    'initialization_method': 'DataElements',  # Use 'DataElements' for initialization\n",
    "    'L': 4,  # Number of non-zero coefficients to use in OMP\n",
    "    'preserve_dc_atom': 0  # Whether to preserve DC atom (0 for no)\n",
    "}\n",
    "\n",
    "# SL0 algorithm parameters\n",
    "sl0_params = {\n",
    "    'sigma_min': 1e-3,\n",
    "    'sigma_decrease_factor': 0.5,\n",
    "    'mu_0': 2,\n",
    "    'L': 3,\n",
    "    'showProgress': False\n",
    "}\n",
    "\n",
    "# Load the signal using scipy.io (from 100m.mat)\n",
    "mat_file = '100m.mat'\n",
    "data = scipy.io.loadmat(mat_file)\n",
    "signal = data['val'][0]  # Assuming the signal is stored in 'val' key\n",
    "signal = signal[360 * 0: 360 * 60 * 2]  # Example duration of 2 minutes\n",
    "\n",
    "# Initialize SNR accumulators and min/max values\n",
    "snr_dct_total = snr_mod_total = snr_ksvd_total = 0\n",
    "snr_dct_min = snr_mod_min = snr_ksvd_min = float('inf')\n",
    "snr_dct_max = snr_mod_max = snr_ksvd_max = float('-inf')\n",
    "snr_dct_kron_total = snr_mod_kron_total = snr_ksvd_kron_total = 0\n",
    "snr_dct_kron_min = snr_mod_kron_min = snr_ksvd_kron_min = float('inf')\n",
    "snr_dct_kron_max = snr_mod_kron_max = snr_ksvd_kron_max = float('-inf')\n",
    "\n",
    "# SNR box toggle (set to True by default)\n",
    "show_snr_box = True  # Set this to False if you don't want to display the SNR on the plots\n",
    "\n",
    "# Loop through the number of repetitions\n",
    "for rep in range(REPS):\n",
    "    print(f\"Iteration {rep} just started\")\n",
    "\n",
    "    # Instantiate the class (no need to re-load the signal)\n",
    "    cs = compressedSensing(signal=signal, matrix_type=matrix_type)\n",
    "\n",
    "    # Divide the signal\n",
    "    cs.divide_signal(training_percentage=0.45)\n",
    "\n",
    "    # Compress the test set\n",
    "    cs.compress_test_set()\n",
    "\n",
    "    # ----------------- Without Kronecker Compression -----------------\n",
    "\n",
    "    # ----------------- DCT-Based Dictionary Recovery -----------------\n",
    "    cs.generate_dictionary(dictionary_type='dct')\n",
    "    cs.recover_signal(sl0_params=sl0_params)  # Pass SL0 parameters here\n",
    "    snr_dct = cs.get_snr()\n",
    "    snr_dct_total += snr_dct\n",
    "    snr_dct_min = min(snr_dct_min, snr_dct)\n",
    "    snr_dct_max = max(snr_dct_max, snr_dct)\n",
    "\n",
    "    # If REPS == 1, plot the detailed reconstruction vs original for DCT\n",
    "    if REPS == 1:\n",
    "        cs.plot_reconstructed_vs_original(\n",
    "            start_pct=0.0,\n",
    "            num_samples=None,  # Plot all samples\n",
    "            reconstructed_label=f\"Reconstructed Signal (DCT) on 100m.mat\",\n",
    "            show_snr_box=show_snr_box  # Use the SNR box toggle\n",
    "        )\n",
    "\n",
    "    # ----------------- MOD-Based Dictionary Recovery -----------------\n",
    "    cs.generate_dictionary(dictionary_type='mod', mod_params=mod_params)\n",
    "    cs.recover_signal(sl0_params=sl0_params)  # Pass SL0 parameters here\n",
    "    snr_mod = cs.get_snr()\n",
    "    snr_mod_total += snr_mod\n",
    "    snr_mod_min = min(snr_mod_min, snr_mod)\n",
    "    snr_mod_max = max(snr_mod_max, snr_mod)\n",
    "\n",
    "    # If REPS == 1, plot the detailed reconstruction vs original for MOD\n",
    "    if REPS == 1:\n",
    "        cs.plot_reconstructed_vs_original(\n",
    "            start_pct=0.0,\n",
    "            num_samples=None,  # Plot all samples\n",
    "            reconstructed_label=f\"Reconstructed Signal (MOD) on 100m.mat\",\n",
    "            show_snr_box=show_snr_box  # Use the SNR box toggle\n",
    "        )\n",
    "\n",
    "    # ----------------- K-SVD-Based Dictionary Recovery -----------------\n",
    "    cs.generate_dictionary(dictionary_type='ksvd', ksvd_params=ksvd_params)\n",
    "    cs.recover_signal(sl0_params=sl0_params)  # Pass SL0 parameters here\n",
    "    snr_ksvd = cs.get_snr()\n",
    "    snr_ksvd_total += snr_ksvd\n",
    "    snr_ksvd_min = min(snr_ksvd_min, snr_ksvd)\n",
    "    snr_ksvd_max = max(snr_ksvd_max, snr_ksvd)\n",
    "\n",
    "    # If REPS == 1, plot the detailed reconstruction vs original for KSVD\n",
    "    if REPS == 1:\n",
    "        cs.plot_reconstructed_vs_original(\n",
    "            start_pct=0.0,\n",
    "            num_samples=None,  # Plot all samples\n",
    "            reconstructed_label=f\"Reconstructed Signal (KSVD) on 100m.mat\",\n",
    "            show_snr_box=show_snr_box  # Use the SNR box toggle\n",
    "        )\n",
    "\n",
    "    # ----------------- Activate Kronecker Compression -----------------\n",
    "    KRON_FACT = 8  # Example Kronecker factor\n",
    "    cs.kronecker_activate(KRON_FACT)\n",
    "\n",
    "    # ----------------- DCT-Based Dictionary Recovery with Kronecker -----------------\n",
    "    cs.generate_dictionary(dictionary_type='dct')\n",
    "    cs.recover_signal(sl0_params=sl0_params)  # Pass SL0 parameters here\n",
    "    snr_dct_kron = cs.get_snr()\n",
    "    snr_dct_kron_total += snr_dct_kron\n",
    "    snr_dct_kron_min = min(snr_dct_kron_min, snr_dct_kron)\n",
    "    snr_dct_kron_max = max(snr_dct_kron_max, snr_dct_kron)\n",
    "\n",
    "    # If REPS == 1, plot the detailed reconstruction vs original for DCT + Kronecker\n",
    "    if REPS == 1:\n",
    "        cs.plot_reconstructed_vs_original(\n",
    "            start_pct=0.0,\n",
    "            num_samples=None,  # Plot all samples\n",
    "            reconstructed_label=f\"Reconstructed Signal (DCT + Kronecker) on 100m.mat\",\n",
    "            show_snr_box=show_snr_box  # Use the SNR box toggle\n",
    "        )\n",
    "\n",
    "    # ----------------- MOD-Based Dictionary Recovery with Kronecker -----------------\n",
    "    cs.generate_dictionary(dictionary_type='mod', mod_params=mod_params)\n",
    "    cs.recover_signal(sl0_params=sl0_params)  # Pass SL0 parameters here\n",
    "    snr_mod_kron = cs.get_snr()\n",
    "    snr_mod_kron_total += snr_mod_kron\n",
    "    snr_mod_kron_min = min(snr_mod_kron_min, snr_mod_kron)\n",
    "    snr_mod_kron_max = max(snr_mod_kron_max, snr_mod_kron)\n",
    "\n",
    "    # If REPS == 1, plot the detailed reconstruction vs original for MOD + Kronecker\n",
    "    if REPS == 1:\n",
    "        cs.plot_reconstructed_vs_original(\n",
    "            start_pct=0.0,\n",
    "            num_samples=None,  # Plot all samples\n",
    "            reconstructed_label=f\"Reconstructed Signal (MOD + Kronecker) on 100m.mat\",\n",
    "            show_snr_box=show_snr_box  # Use the SNR box toggle\n",
    "        )\n",
    "\n",
    "    # ----------------- K-SVD-Based Dictionary Recovery with Kronecker -----------------\n",
    "    cs.generate_dictionary(dictionary_type='ksvd', ksvd_params=ksvd_params)\n",
    "    cs.recover_signal(sl0_params=sl0_params)  # Pass SL0 parameters here\n",
    "    snr_ksvd_kron = cs.get_snr()\n",
    "    snr_ksvd_kron_total += snr_ksvd_kron\n",
    "    snr_ksvd_kron_min = min(snr_ksvd_kron_min, snr_ksvd_kron)\n",
    "    snr_ksvd_kron_max = max(snr_ksvd_kron_max, snr_ksvd_kron)\n",
    "\n",
    "    # If REPS == 1, plot the detailed reconstruction vs original for KSVD + Kronecker\n",
    "    if REPS == 1:\n",
    "        cs.plot_reconstructed_vs_original(\n",
    "            start_pct=0.0,\n",
    "            num_samples=None,  # Plot all samples\n",
    "            reconstructed_label=f\"Reconstructed Signal (KSVD + Kronecker) on 100m.mat\",\n",
    "            show_snr_box=show_snr_box  # Use the SNR box toggle\n",
    "        )\n",
    "\n",
    "# For REPS > 1, plot the histogram\n",
    "if REPS > 1:\n",
    "    # Compute average, min, and max values for each method\n",
    "    avg_snrs = [\n",
    "        snr_dct_total / REPS,\n",
    "        snr_mod_total / REPS,\n",
    "        snr_ksvd_total / REPS,\n",
    "        snr_dct_kron_total / REPS,\n",
    "        snr_mod_kron_total / REPS,\n",
    "        snr_ksvd_kron_total / REPS\n",
    "    ]\n",
    "\n",
    "    snr_min = {\n",
    "        'dct': snr_dct_min,\n",
    "        'mod': snr_mod_min,\n",
    "        'ksvd': snr_ksvd_min,\n",
    "        'dct_kron': snr_dct_kron_min,\n",
    "        'mod_kron': snr_mod_kron_min,\n",
    "        'ksvd_kron': snr_ksvd_kron_min\n",
    "    }\n",
    "\n",
    "    snr_max = {\n",
    "        'dct': snr_dct_max,\n",
    "        'mod': snr_mod_max,\n",
    "        'ksvd': snr_ksvd_max,\n",
    "        'dct_kron': snr_dct_kron_max,\n",
    "        'mod_kron': snr_mod_kron_max,\n",
    "        'ksvd_kron': snr_ksvd_kron_max\n",
    "    }\n",
    "\n",
    "    # Labels for each method\n",
    "    labels = [\n",
    "        f\"{matrix_type}-DCT\", f\"{matrix_type}-MOD\", f\"{matrix_type}-KSVD\",\n",
    "        f\"{matrix_type}-DCT-KRON\", f\"{matrix_type}-MOD-KRON\", f\"{matrix_type}-KSVD-KRON\"\n",
    "    ]\n",
    "\n",
    "    # Create a bar chart\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(labels, avg_snrs, color='teal')\n",
    "\n",
    "    # Add min and max lines if toggle is True\n",
    "    if show_min_max:\n",
    "        for bar, label in zip(bars, labels):\n",
    "            method_key = label.split(\"-\")[1].lower()  # Extract the dictionary type (e.g., 'dct', 'mod', 'ksvd')\n",
    "            if \"kron\" in label.lower():\n",
    "                method_key += \"_kron\"  # Add the '_kron' suffix for Kronecker methods\n",
    "            \n",
    "            # Get min and max values\n",
    "            min_val = snr_min[method_key]\n",
    "            max_val = snr_max[method_key]\n",
    "            \n",
    "            # Get the bar center and width\n",
    "            bar_center = bar.get_x() + bar.get_width() / 2\n",
    "            \n",
    "            # Plot min and max lines with bright green and red\n",
    "            plt.plot([bar_center - bar.get_width()/4, bar_center + bar.get_width()/4], [min_val, min_val], color='red', lw=2)  # Min line (bright red)\n",
    "            plt.plot([bar_center - bar.get_width()/4, bar_center + bar.get_width()/4], [max_val, max_val], color='lime', lw=2)  # Max line (bright green)\n",
    "\n",
    "    # Labels and titles\n",
    "    plt.xlabel('Methods')\n",
    "    plt.ylabel('Average SNR (dB)')\n",
    "    plt.title(f'Average SNR for {matrix_type} on 100m.mat with Min/Max Values' if show_min_max else f'Average SNR for {matrix_type} on 100m.mat')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    output_folder = Path('outputs')\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save the plot with the matrix type in the filename\n",
    "    output_file = output_folder / f'snr_histogram_with_min_max_{matrix_type}_100m.png'\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_file)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Histogram saved to '{output_file}'\")\n",
    "\n",
    "    # Save CSV with results\n",
    "    csv_output_file = output_folder / f'snr_results_{matrix_type}_100m.csv'\n",
    "    results_data = {\n",
    "        'Method': labels,\n",
    "        'Average SNR': avg_snrs,\n",
    "        'Min SNR': [snr_min[method_key] for method_key in ['dct', 'mod', 'ksvd', 'dct_kron', 'mod_kron', 'ksvd_kron']],\n",
    "        'Max SNR': [snr_max[method_key] for method_key in ['dct', 'mod', 'ksvd', 'dct_kron', 'mod_kron', 'ksvd_kron']]\n",
    "    }\n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    results_df.to_csv(csv_output_file, index=False)\n",
    "    \n",
    "    print(f\"CSV saved to '{csv_output_file}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for best MOD and KSVD parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "if False:\n",
    "\n",
    "    # Number of repetitions for averaging\n",
    "    N = 1\n",
    "\n",
    "    # Fixed SL0 parameters (from optimal values)\n",
    "    sl0_params_dict = {\n",
    "        'dct': {'sigma_min': 1e-3, 'sigma_decrease_factor': 0.5, 'mu_0': 3, 'L': 3},\n",
    "        'mod': {'sigma_min': 1e-3, 'sigma_decrease_factor': 0.7, 'mu_0': 4, 'L': 10},\n",
    "        'ksvd': {'sigma_min': 1e-3, 'sigma_decrease_factor': 0.3, 'mu_0': 4, 'L': 3}\n",
    "    }\n",
    "\n",
    "    # Dictionary learning parameters ranges to test\n",
    "    redundancy_values = [1, 2, 3]\n",
    "    num_iterations_values = [1, 10, 20, 50]\n",
    "    L_values = [2, 4, 8]\n",
    "    preserve_dc_atom_values = [0]  # Test values for K-SVD's preserve_dc_atom\n",
    "\n",
    "\n",
    "    # Initialize list to store results\n",
    "    results_list = []\n",
    "\n",
    "    # Load the data\n",
    "    data0 = scipy.io.loadmat('100m.mat')\n",
    "    signal = data0['val'][0]  # MLII ECG data\n",
    "    signal = signal[360*0:360*10*1]  # Define portion of signal for testing\n",
    "\n",
    "    # Instantiate the compressed sensing class with the 'DBBD' matrix type\n",
    "    cs = compressedSensing(signal=signal, matrix_type='DBBD')\n",
    "\n",
    "    # Divide the signal into training and testing\n",
    "    cs.divide_signal(training_percentage=2/3)\n",
    "\n",
    "    # Compress the test set\n",
    "    cs.compress_test_set()\n",
    "\n",
    "    # Iterate over dictionary types\n",
    "    for dictionary in ['mod', 'ksvd']:\n",
    "        \n",
    "        # Iterate over redundancy, num_iterations, and L values\n",
    "        for redundancy in redundancy_values:\n",
    "            for num_iterations in num_iterations_values:\n",
    "                for L in L_values:\n",
    "                    \n",
    "                    # If K-SVD, iterate over preserve_dc_atom values\n",
    "                    if dictionary == 'ksvd':\n",
    "                        for preserve_dc_atom in preserve_dc_atom_values:\n",
    "                            cumulative_snr = 0  # Initialize cumulative SNR\n",
    "                            \n",
    "                            # Run N trials for averaging\n",
    "                            for trial in range(N):\n",
    "                                dict_params = {\n",
    "                                    'redundancy': redundancy,\n",
    "                                    'num_iterations': num_iterations,\n",
    "                                    'initialization_method': 'DataElements',\n",
    "                                    'L': L,\n",
    "                                    'preserve_dc_atom': preserve_dc_atom  # Testing different values for preserve_dc_atom\n",
    "                                }\n",
    "\n",
    "                                # Generate the K-SVD dictionary\n",
    "                                cs.generate_dictionary(dictionary_type='ksvd', ksvd_params=dict_params)\n",
    "\n",
    "                                # Recover the signal using fixed SL0 parameters for K-SVD\n",
    "                                cs.recover_signal(sl0_params=sl0_params_dict[dictionary])\n",
    "\n",
    "                                # Get the SNR and accumulate for averaging\n",
    "                                snr = cs.get_snr()\n",
    "                                cumulative_snr += snr\n",
    "\n",
    "                            # Compute the average SNR over N trials\n",
    "                            average_snr = cumulative_snr / N\n",
    "\n",
    "                            # Append the averaged results to the list\n",
    "                            results_list.append({\n",
    "                                'Measurement Matrix': 'DBBD',\n",
    "                                'Dictionary': dictionary,\n",
    "                                'redundancy': redundancy,\n",
    "                                'num_iterations': num_iterations,\n",
    "                                'L_dict': L,\n",
    "                                'preserve_dc_atom': preserve_dc_atom,  # Include preserve_dc_atom for K-SVD\n",
    "                                'Average_SNR': average_snr\n",
    "                            })\n",
    "\n",
    "                    else:\n",
    "                        # For MOD, preserve_dc_atom is always 0\n",
    "                        cumulative_snr = 0  # Initialize cumulative SNR\n",
    "\n",
    "                        # Run N trials for averaging\n",
    "                        for trial in range(N):\n",
    "                            dict_params = {\n",
    "                                'redundancy': redundancy,\n",
    "                                'num_iterations': num_iterations,\n",
    "                                'initialization_method': 'DataElements',\n",
    "                                'L': L,\n",
    "                                'preserve_dc_atom': 0  # Always 0 for MOD\n",
    "                            }\n",
    "\n",
    "                            # Generate the MOD dictionary\n",
    "                            cs.generate_dictionary(dictionary_type='mod', mod_params=dict_params)\n",
    "\n",
    "                            # Recover the signal using fixed SL0 parameters for MOD\n",
    "                            cs.recover_signal(sl0_params=sl0_params_dict[dictionary])\n",
    "\n",
    "                            # Get the SNR and accumulate for averaging\n",
    "                            snr = cs.get_snr()\n",
    "                            cumulative_snr += snr\n",
    "\n",
    "                        # Compute the average SNR over N trials\n",
    "                        average_snr = cumulative_snr / N\n",
    "\n",
    "                        # Append the averaged results to the list\n",
    "                        results_list.append({\n",
    "                            'Measurement Matrix': 'DBBD',\n",
    "                            'Dictionary': dictionary,\n",
    "                            'redundancy': redundancy,\n",
    "                            'num_iterations': num_iterations,\n",
    "                            'L_dict': L,\n",
    "                            'preserve_dc_atom': 0,  # Always 0 for MOD\n",
    "                            'Average_SNR': average_snr\n",
    "                        })\n",
    "\n",
    "    # Convert the results list to a DataFrame\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    output_folder = Path('tests_results')\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save the results to the CSV file in the folder\n",
    "    output_file = output_folder / 'mod_ksvd_parameter_sweep_results.csv'\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"MOD and K-SVD parameter sweep complete and results saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for best SL0 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "if False:\n",
    "    \n",
    "    # Number of repetitions for averaging\n",
    "    N = 2\n",
    "\n",
    "    \n",
    "    # SL0 parameter ranges for testing\n",
    "    sigma_min_values = [1e-3]  # [1e-2, 1e-3, 1e-4, 1e-6]\n",
    "    sigma_decrease_factors = [0.1, 0.3, 0.5, 0.7]  # [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    mu_0_values = list(range(1, 11))  # Values from 1 to 10\n",
    "    L_values = [3, 5, 10]\n",
    "    \n",
    "\n",
    "    # Dictionary learning parameters\n",
    "    mod_params = {\n",
    "        'redundancy': 1,\n",
    "        'num_iterations': 50,\n",
    "        'initialization_method': 'DataElements',\n",
    "        'L': 4\n",
    "    }\n",
    "\n",
    "    ksvd_params = {\n",
    "        'redundancy': 1,\n",
    "        'num_iterations': 50,\n",
    "        'initialization_method': 'DataElements',\n",
    "        'L': 4,\n",
    "        'preserve_dc_atom': 0\n",
    "    }\n",
    "\n",
    "    # Initialize list to store results\n",
    "    results_list = []\n",
    "\n",
    "    # Load the data\n",
    "    data0 = scipy.io.loadmat('100m.mat')\n",
    "    signal = data0['val'][0]  # MLII ECG data\n",
    "    signal = signal[360*0:360*3*1]  # Define portion of signal for testing\n",
    "\n",
    "    # Instantiate the compressed sensing class with the 'DBBD' matrix type\n",
    "    cs = compressedSensing(signal=signal, matrix_type='DBBD')\n",
    "\n",
    "    # Divide the signal into training and testing\n",
    "    cs.divide_signal(training_percentage=2/3)\n",
    "\n",
    "    # Compress the test set\n",
    "    cs.compress_test_set()\n",
    "\n",
    "    # Iterate over sigma_min, sigma_decrease_factors, mu_0_values, and L_values\n",
    "    for sigma_min_value in sigma_min_values:\n",
    "        for sigma_decrease_factor in sigma_decrease_factors:\n",
    "            for mu_0 in mu_0_values:\n",
    "                for L in L_values:\n",
    "                    print(\"*\")\n",
    "\n",
    "                    # Initialize cumulative SNR for averaging\n",
    "                    cumulative_snr = {'dct': 0, 'mod': 0, 'ksvd': 0}\n",
    "\n",
    "                    # Run N trials for averaging\n",
    "                    for trial in range(N):\n",
    "                        sl0_params = {\n",
    "                            'sigma_min': sigma_min_value,\n",
    "                            'sigma_decrease_factor': sigma_decrease_factor,\n",
    "                            'mu_0': mu_0,\n",
    "                            'L': L,\n",
    "                            'showProgress': False\n",
    "                        }\n",
    "\n",
    "                        # Test with different dictionaries\n",
    "                        for dictionary in ['dct', 'mod', 'ksvd']:\n",
    "                            \n",
    "                            # Generate the dictionary\n",
    "                            if dictionary == 'dct':\n",
    "                                cs.generate_dictionary(dictionary_type='dct')\n",
    "                            elif dictionary == 'mod':\n",
    "                                cs.generate_dictionary(dictionary_type='mod', mod_params=mod_params)\n",
    "                            elif dictionary == 'ksvd':\n",
    "                                cs.generate_dictionary(dictionary_type='ksvd', ksvd_params=ksvd_params)\n",
    "                            \n",
    "                            # Recover the signal\n",
    "                            cs.recover_signal(sl0_params=sl0_params)\n",
    "                            \n",
    "                            # Get the SNR and accumulate for averaging\n",
    "                            snr = cs.get_snr()\n",
    "                            cumulative_snr[dictionary] += snr\n",
    "\n",
    "                    # Compute the average SNR over N trials\n",
    "                    for dictionary in ['dct', 'mod', 'ksvd']:\n",
    "                        average_snr = cumulative_snr[dictionary] / N\n",
    "                        \n",
    "                        # Append the averaged results to the list\n",
    "                        results_list.append({\n",
    "                            'Measurement Matrix': 'DBBD',\n",
    "                            'Dictionary': dictionary,\n",
    "                            'sigma_min': sigma_min_value,\n",
    "                            'sigma_decrease_factor': sigma_decrease_factor,\n",
    "                            'mu_0': mu_0,\n",
    "                            'L': L,\n",
    "                            'Average_SNR': average_snr\n",
    "                        })\n",
    "\n",
    "    # Convert the results list to a DataFrame\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    output_folder = Path('tests_results')\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save the results to the CSV file in the folder\n",
    "    output_file = output_folder / 'sl0_parameter_sweep_results.csv'\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Parameter sweep complete and results saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test best KRONECKER FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "if False:\n",
    "\n",
    "    # Number of repetitions for averaging\n",
    "    N = 2\n",
    "\n",
    "\n",
    "    # Fixed SL0 parameters (from optimal values)\n",
    "    sl0_params_dict = {\n",
    "        'dct': {'sigma_min': 1e-3, 'sigma_decrease_factor': 0.5, 'mu_0': 3, 'L': 3},\n",
    "        'mod': {'sigma_min': 1e-3, 'sigma_decrease_factor': 0.7, 'mu_0': 4, 'L': 10},\n",
    "        'ksvd': {'sigma_min': 1e-3, 'sigma_decrease_factor': 0.3, 'mu_0': 4, 'L': 3}\n",
    "    }\n",
    "\n",
    "    # Fixed MOD and K-SVD parameters\n",
    "    mod_params = {\n",
    "        'redundancy': 1,\n",
    "        'num_iterations': 50,\n",
    "        'initialization_method': 'DataElements',\n",
    "        'L': 4\n",
    "    }\n",
    "\n",
    "    ksvd_params = {\n",
    "        'redundancy': 1,\n",
    "        'num_iterations': 50,\n",
    "        'initialization_method': 'DataElements',\n",
    "        'L': 4,\n",
    "        'preserve_dc_atom': 0\n",
    "    }\n",
    "\n",
    "    # KRON_FACT values to test\n",
    "    kron_fact_values = [2, 3, 4, 6, 8, 10, 12, 16]\n",
    "\n",
    "\n",
    "    # Initialize list to store results\n",
    "    results_list = []\n",
    "\n",
    "    # Load the data\n",
    "    data0 = scipy.io.loadmat('100m.mat')\n",
    "    signal = data0['val'][0]  # MLII ECG data\n",
    "    signal = signal[360*0:360*60*8]  # Define portion of signal for testing\n",
    "    3\n",
    "    # Instantiate the compressed sensing class with the 'DBBD' matrix type\n",
    "    cs = compressedSensing(signal=signal, matrix_type='DBBD')\n",
    "\n",
    "    # Iterate over KRON_FACT values\n",
    "    for kron_fact in kron_fact_values:\n",
    "        \n",
    "        # Initialize a dictionary to store cumulative SNR for each dictionary\n",
    "        cumulative_snr = {'dct': 0, 'mod': 0, 'ksvd': 0}\n",
    "\n",
    "        # Run N trials\n",
    "        for trial in range(N):\n",
    "            # Clear the state of the class before each iteration to start fresh\n",
    "            cs.clear()\n",
    "            \n",
    "            # Divide the signal into training and testing\n",
    "            cs.divide_signal(training_percentage=2/3)\n",
    "\n",
    "            # Compress the test set\n",
    "            cs.compress_test_set()\n",
    "\n",
    "            # Activate Kronecker compression with the current KRON_FACT value\n",
    "            cs.kronecker_activate(KRON_FACT=kron_fact)\n",
    "            \n",
    "            # Iterate over dictionary types\n",
    "            for dictionary in ['dct', 'mod', 'ksvd']:\n",
    "                \n",
    "                # Generate the dictionary\n",
    "                if dictionary == 'dct':\n",
    "                    cs.generate_dictionary(dictionary_type='dct')\n",
    "                elif dictionary == 'mod':\n",
    "                    cs.generate_dictionary(dictionary_type='mod', mod_params=mod_params)\n",
    "                elif dictionary == 'ksvd':\n",
    "                    cs.generate_dictionary(dictionary_type='ksvd', ksvd_params=ksvd_params)\n",
    "                \n",
    "                # Recover the signal using fixed SL0 parameters\n",
    "                cs.recover_signal(sl0_params=sl0_params_dict[dictionary])\n",
    "\n",
    "                # Get the SNR for this trial and add it to the cumulative sum\n",
    "                snr = cs.get_snr()\n",
    "                cumulative_snr[dictionary] += snr\n",
    "\n",
    "        # After N trials, compute the average SNR for each dictionary and append to results\n",
    "        for dictionary in ['dct', 'mod', 'ksvd']:\n",
    "            average_snr = cumulative_snr[dictionary] / N\n",
    "            results_list.append({\n",
    "                'Measurement Matrix': 'DBBD',\n",
    "                'Dictionary': dictionary,\n",
    "                'KRON_FACT': kron_fact,\n",
    "                'Average_SNR': average_snr\n",
    "            })\n",
    "\n",
    "    # Convert the results list to a DataFrame\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    output_folder = Path('tests_results')\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save the results to the CSV file in the folder\n",
    "    output_file = output_folder / 'kron_factor_sweep_results.csv'\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Kronecker factor sweep complete and results saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for optimal BLOCK_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if False:\n",
    "    \n",
    "    # Define powers of 2 for BLOCK_LEN from 16 up to 1024\n",
    "    block_lens = [2**i for i in range(4, 11)]  \n",
    "\n",
    "    # Set compression ratio (CR)\n",
    "    CR = 4\n",
    "\n",
    "    # SL0 algorithm parameters (kept fixed)\n",
    "    sl0_params = {\n",
    "        'sigma_min': 1e-3,\n",
    "        'sigma_decrease_factor': 0.5,\n",
    "        'mu_0': 2,\n",
    "        'L': 3,\n",
    "        'showProgress': False\n",
    "    }\n",
    "\n",
    "    # Initialize results list for storing BLOCK_LEN, execution time for compression, dictionary creation, and recovery, and SNR\n",
    "    results_list = []\n",
    "\n",
    "    # Load the data\n",
    "    data0 = scipy.io.loadmat('100m.mat')\n",
    "    signal = data0['val'][0]  # MLII ECG data\n",
    "    signal =  signal[:]  # Define portion of signal for testing\n",
    "\n",
    "    # Loop through different BLOCK_LEN values\n",
    "    for block_len in block_lens:\n",
    "        print(f\"Testing with BLOCK_LEN = {block_len}\")\n",
    "\n",
    "        # Instantiate the compressedSensing class\n",
    "        cs = compressedSensing(signal=signal, BLOCK_LEN=block_len, CR=CR, matrix_type='DBBD')\n",
    "\n",
    "        # Divide the signal into training and testing\n",
    "        cs.divide_signal(training_percentage=0.9)\n",
    "\n",
    "        # Measure compression time\n",
    "        start_time = time.time()\n",
    "        cs.compress_test_set()\n",
    "        compression_time = time.time() - start_time\n",
    "\n",
    "        # Measure dictionary creation time\n",
    "        start_time = time.time()\n",
    "        cs.generate_dictionary(dictionary_type='dct')\n",
    "        dict_creation_time = time.time() - start_time\n",
    "\n",
    "        # Measure recovery time\n",
    "        start_time = time.time()\n",
    "        cs.recover_signal(sl0_params=sl0_params)\n",
    "        recovery_time = time.time() - start_time\n",
    "\n",
    "        # Get the SNR of the reconstructed signal\n",
    "        snr = cs.get_snr()\n",
    "\n",
    "        # Append the results (BLOCK_LEN, compression_time, dict_creation_time, recovery_time, SNR) to the list\n",
    "        results_list.append({\n",
    "            'BLOCK_LEN': block_len,\n",
    "            'Compression Time (s)': compression_time,\n",
    "            'Dictionary Creation Time (s)': dict_creation_time,\n",
    "            'Recovery Time (s)': recovery_time,\n",
    "            'SNR (dB)': snr\n",
    "        })\n",
    "\n",
    "    # Convert the results to a DataFrame\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    output_folder = Path('outputs')\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    csv_output_file = output_folder / 'block_len_vs_time_snr.csv'\n",
    "    results_df.to_csv(csv_output_file, index=False)\n",
    "\n",
    "    # Plot compression time, dictionary creation time, and recovery time as a function of BLOCK_LEN\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results_df['BLOCK_LEN'], results_df['Compression Time (s)'], marker='o', linestyle='-', label='Compression Time', color='blue')\n",
    "    plt.plot(results_df['BLOCK_LEN'], results_df['Dictionary Creation Time (s)'], marker='o', linestyle='-', label='Dictionary Creation Time', color='green')\n",
    "    plt.plot(results_df['BLOCK_LEN'], results_df['Recovery Time (s)'], marker='o', linestyle='-', label='Recovery Time', color='red')\n",
    "    plt.xlabel('BLOCK_LEN')\n",
    "    plt.ylabel('Time (s)')\n",
    "    plt.title('Compression, Dictionary Creation, and Recovery Time vs BLOCK_LEN')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(results_df['BLOCK_LEN'])\n",
    "    plt.xscale('log', base=2)  # Set x-axis to log scale for powers of 2\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the time plot\n",
    "    time_plot_file = output_folder / 'time_vs_block_len.png'\n",
    "    plt.savefig(time_plot_file)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot SNR as a function of BLOCK_LEN\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results_df['BLOCK_LEN'], results_df['SNR (dB)'], marker='o', linestyle='-', color='darkorange')\n",
    "    plt.xlabel('BLOCK_LEN')\n",
    "    plt.ylabel('SNR (dB)')\n",
    "    plt.title('SNR vs BLOCK_LEN')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(results_df['BLOCK_LEN'])\n",
    "    plt.xscale('log', base=2)  # Set x-axis to log scale for powers of 2\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the SNR plot\n",
    "    snr_plot_file = output_folder / 'snr_vs_block_len.png'\n",
    "    plt.savefig(snr_plot_file)\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Results saved to '{csv_output_file}', and plots saved to '{time_plot_file}' and '{snr_plot_file}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize and recover wfdb signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "\n",
    "if False:\n",
    "    # Set the parameters\n",
    "    record_name = '109'  # Example record from MIT-BIH Arrhythmia Database\n",
    "    duration_minutes = 60 * 2  # Duration of the signal to load (in minutes)\n",
    "    chosen_method = 'KSVD'  # Choose 'DCT', 'MOD', or 'KSVD'\n",
    "    measurement_matrix = 'unscaled_binary'  # Choose your matrix type ('DBBD', 'gaussian', 'scaled_binary', 'unscaled_binary')\n",
    "\n",
    "    # Load the signal using wfdb\n",
    "    signal, signal_name = load_signal_from_wfdb(record_name, duration_minutes=duration_minutes)\n",
    "\n",
    "\n",
    "    # Instantiate the compressedSensing class with the chosen measurement matrix\n",
    "    cs = compressedSensing(signal=signal, matrix_type=measurement_matrix)\n",
    "\n",
    "    # Divide the signal into training and testing\n",
    "    cs.divide_signal(training_percentage=0.45)\n",
    "\n",
    "    # Compress the test set\n",
    "    cs.compress_test_set()\n",
    "\n",
    "    # Generate the dictionary based on the chosen method\n",
    "    if chosen_method == 'DCT':\n",
    "        cs.generate_dictionary(dictionary_type='dct')\n",
    "    elif chosen_method == 'MOD':\n",
    "        mod_params = {\n",
    "            'redundancy': 1, \n",
    "            'num_iterations': 10, \n",
    "            'L': 4, \n",
    "            'initialization_method': 'DataElements'  # Add initialization method here\n",
    "        }\n",
    "        cs.generate_dictionary(dictionary_type='mod', mod_params=mod_params)\n",
    "    elif chosen_method == 'KSVD':\n",
    "        ksvd_params = {\n",
    "            'redundancy': 1, \n",
    "            'num_iterations': 10, \n",
    "            'L': 4, \n",
    "            'initialization_method': 'DataElements',  # Add initialization method here\n",
    "            'preserve_dc_atom': 0  # You may also want to specify if you need this\n",
    "        }\n",
    "        cs.generate_dictionary(dictionary_type='ksvd', ksvd_params=ksvd_params)\n",
    "\n",
    "\n",
    "    # Recover the signal using SL0\n",
    "    sl0_params = {'sigma_min': 1e-3, 'sigma_decrease_factor': 0.5, 'mu_0': 2, 'L': 3, 'showProgress': False}\n",
    "    cs.recover_signal(sl0_params=sl0_params)\n",
    "\n",
    "    # Plot the reconstructed signal vs original using the built-in method\n",
    "    cs.plot_reconstructed_vs_original(\n",
    "        start_pct=0.0,\n",
    "        num_samples=2048,  # Plot a sample of 1000 points\n",
    "        reconstructed_label=f\"Reconstructed Signal ({chosen_method}) with {measurement_matrix} Matrix\"\n",
    "\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".namlVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
