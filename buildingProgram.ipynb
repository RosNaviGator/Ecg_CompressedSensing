{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script provides utility functions for formatted printing of NumPy matrices and saving matrices to CSV files.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# system imports\n",
    "import os\n",
    "\n",
    "# third party imports\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def printFormatted(matrix, decimals=4):\n",
    "    \"\"\"\n",
    "    Prints the matrix with formatted elements aligned in columns for improved readability.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    matrix : numpy array\n",
    "        The matrix to be printed.\n",
    "    decimals : int, optional (default=4)\n",
    "        The number of decimal places for formatting the elements.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    None\n",
    "        This function does not return any value; it prints the formatted matrix directly to the console.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The function aligns columns based on the maximum width needed for the formatted elements, ensuring the matrix is displayed neatly.\n",
    "    - This function is useful for visual inspection of numerical matrices, especially those with varying magnitudes.\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> matrix = np.array([[1.234567, 123.456789], [0.0001234, 1.2345]])\n",
    "    >>> print('Classic print:')\n",
    "    >>> print(matrix)\n",
    "    Classic print:\n",
    "    [[1.2345670e+00 1.2345679e+02]\n",
    "     [1.2340000e-04 1.2345000e+00]]\n",
    "     \n",
    "    >>> print('\\nFormatted print:')\n",
    "    >>> printFormatted(matrix, decimals=4)\n",
    "         1.2346  123.4568\n",
    "         0.0001    1.2345\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Determine the maximum width needed to keep alignment\n",
    "    max_width = max(len(f'{value:.{decimals}f}') for row in matrix for value in row)\n",
    "\n",
    "    # Create a formatted string for each element in the matrix, ensuring alignment\n",
    "    formatted_matrix = '\\n'.join([' '.join([f'{value:>{max_width}.{decimals}f}' for value in row]) for row in matrix])\n",
    "\n",
    "    # Print the formatted matrix\n",
    "    print(formatted_matrix)\n",
    "\n",
    "\n",
    "def py_test_csv(array):\n",
    "    \"\"\"\n",
    "    Save a numpy array as a CSV file in ./debugCsvPy/py_test.csv\n",
    "\n",
    "    Parameters:\n",
    "    array (numpy.ndarray): The input array to be saved as a CSV file.\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    output_dir = 'debugCsvPy'  # Directory where CSV files will be stored\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    py_dict_path = os.path.join(output_dir, 'py_test.csv')\n",
    "    np.savetxt(py_dict_path, array, delimiter=',', fmt='%.6f')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurement matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_DBBD_matrix(M, N):\n",
    "    \"\"\"\n",
    "    Generates a deterministic Diagonally Blocked Block Diagonal (DBBD) matrix.\n",
    "\n",
    "    A DBBD matrix is a type of block diagonal matrix where each block is a square diagonal matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    M : int\n",
    "        Number of rows in the matrix.\n",
    "    N : int\n",
    "        Number of columns in the matrix. Should be a multiple of M.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A : numpy.ndarray\n",
    "        The generated DBBD matrix of shape (M, N).\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If `N` is not a multiple of `M`.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> generate_DBDD_matrix(3, 9)\n",
    "    array([[1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
    "           [0., 0., 0., 1., 1., 1., 0., 0., 0.],\n",
    "           [0., 0., 0., 0., 0., 0., 1., 1., 1.]])\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if N % M != 0:\n",
    "        raise ValueError(\"N should be a multiple of M.\")\n",
    "    \n",
    "    Phi = np.zeros((M, N))\n",
    "    m = N // M\n",
    "    \n",
    "    for i in range(M):\n",
    "        Phi[i, i*m:(i+1)*m] = 1\n",
    "\n",
    "    return Phi\n",
    "\n",
    "\n",
    "def generate_random_matrix(M, N, matrix_type='gaussian'):\n",
    "    \"\"\"\n",
    "    Generates a random matrix based on the specified type.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    M : int\n",
    "        Number of rows in the matrix.\n",
    "    N : int\n",
    "        Number of columns in the matrix.\n",
    "    matrix_type : str, optional (default='gaussian')\n",
    "        The type of random matrix to generate. Options are:\n",
    "        - 'gaussian': A matrix with entries drawn from a normal distribution scaled by 1/M.\n",
    "        - 'scaled_binary': A matrix with binary entries (±0.5), scaled by 1/sqrt(M).\n",
    "        - 'unscaled_binary': A matrix with binary entries (±1), with no scaling.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A : numpy.ndarray\n",
    "        The generated random matrix of shape (M, N).\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If `matrix_type` is not one of the supported types.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> generate_random_matrix(2, 3, matrix_type='gaussian')\n",
    "    array([[ 0.01, -0.02,  0.03],\n",
    "           [-0.04,  0.05, -0.06]])\n",
    "\n",
    "    >>> generate_random_matrix(2, 3, matrix_type='scaled_binary')\n",
    "    array([[-0.5,  0. , -0.5],\n",
    "           [ 0.5, -0.5,  0. ]])\n",
    "    \n",
    "    >>> generate_random_matrix(2, 3, matrix_type='unscaled_binary')\n",
    "    array([[ 1., -1.,  1.],\n",
    "           [-1.,  1., -1.]])\n",
    "    \"\"\"\n",
    "    if matrix_type == 'gaussian':\n",
    "        A = ((1/M)**2) * np.random.randn(M, N)\n",
    "\n",
    "    elif matrix_type == 'scaled_binary':\n",
    "        A = np.random.binomial(1, 0.5, size=(M, N)) - 0.5\n",
    "        A = (1/np.sqrt(M)) * A\n",
    "\n",
    "    elif matrix_type == 'unscaled_binary':\n",
    "        A = np.random.binomial(1, 0.5, size=(M, N)) * 2 - 1\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported matrix type. Choose either 'gaussian', 'scaled_binary', or 'unscaled_binary'.\")\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script provides utility functions for generating a Discrete Cosine Transform (DCT) \n",
    "orthonormal basis matrix and for testing various properties of matrices such as independence \n",
    "of columns, normalization, and coherence.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import scipy.fftpack as fftpack\n",
    "import pywt\n",
    "\n",
    "def dct_dictionary(N):\n",
    "    \"\"\"\n",
    "    Generates a Discrete Cosine Transform (DCT) orthonormal basis matrix.\n",
    "\n",
    "    The DCT basis is commonly used in signal processing and data compression. \n",
    "    It transforms a signal into a sum of cosine functions oscillating at different frequencies. \n",
    "    The resulting matrix can be used for orthogonal transformations of signals.\n",
    "    \n",
    "    DCT basis is sparifying.\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    N : int\n",
    "        The size of the dictionary (i.e., the length of the signal).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict_matrix : numpy.ndarray\n",
    "        The generated DCT dictionary matrix of shape (N, N), where each column represents \n",
    "        a DCT basis vector.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> generate_dct_dictionary(4)\n",
    "    array([[ 0.5       ,  0.5       ,  0.5       ,  0.5       ],\n",
    "           [ 0.65328148,  0.27059805, -0.27059805, -0.65328148],\n",
    "           [ 0.5       , -0.5       , -0.5       ,  0.5       ],\n",
    "           [ 0.27059805, -0.65328148,  0.65328148, -0.27059805]])\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate a DCT basis dictionary\n",
    "    dict_matrix = fftpack.dct(np.eye(N), norm='ortho')    \n",
    "    return dict_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive dictionary learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Orthogonal Matching Pursuit (OMP) algorithm for sparse coding.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def OMP(dictio, sig, max_coeff):\n",
    "    \"\"\"\n",
    "    Orthogonal Matching Pursuit (OMP) algorithm for sparse coding.\n",
    "\n",
    "    This function implements the OMP algorithm, which is used to find the sparse\n",
    "    representation of a signal over a given dictionary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dictio : numpy.ndarray\n",
    "        The dictionary to use for sparse coding. It should be a matrix of size (n x K), \n",
    "        where n is the signal dimension and K is the number of atoms in the dictionary.\n",
    "        (its columns MUST be normalized).\n",
    "    \n",
    "    sig : numpy.ndarray\n",
    "        The signals to represent using the dictionary. \n",
    "        It should be a matrix of size (n x N), where N is the number of signals.\n",
    "    \n",
    "    max_coeff : int\n",
    "        The maximum number of coefficients to use for representing each signal.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    s : numpy.ndarray\n",
    "        The sparse representation of the signals over the dictionary.\n",
    "        It should be a matrix of size (K x N).\n",
    "    \"\"\"\n",
    "\n",
    "    [n, p] = sig.shape\n",
    "    [_, key] = dictio.shape\n",
    "    s = np.zeros((key, p))\n",
    "    for k in range(p):\n",
    "        x = sig[:, k]\n",
    "        residual = x.copy()\n",
    "        indx = np.array([], dtype=int)\n",
    "        current_atoms = np.empty((n, 0))\n",
    "        norm_x = np.linalg.norm(x)\n",
    "        for j in range(max_coeff):\n",
    "            proj = dictio.T @ residual\n",
    "            pos = np.argmax(np.abs(proj))\n",
    "            indx = np.append(indx, pos)\n",
    "            # Update selected atoms matrix\n",
    "            current_atoms = np.column_stack((current_atoms, dictio[:, pos]))\n",
    "            # Solve least squares problem using QR decomposition for stability\n",
    "            q, r = np.linalg.qr(current_atoms)\n",
    "            a = np.linalg.solve(r, q.T @ x)\n",
    "            residual = x - current_atoms @ a\n",
    "            # Break if norm of residual is suff small (relative to original signal)\n",
    "            if np.linalg.norm(residual) < 1e-6 * norm_x:\n",
    "                break\n",
    "        temp = np.zeros((key,))\n",
    "        temp[indx] = a\n",
    "        s[:, k] = temp\n",
    "\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MOD (Method of Optimal Directions) algorithm for dictionary learning with improved numerical stability.\n",
    "\"\"\"\n",
    "\n",
    "# system imports\n",
    "import os\n",
    "\n",
    "# third party imports\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.linalg import solve\n",
    "\n",
    "\n",
    "\n",
    "def I_findDistanceBetweenDictionaries(original, new):\n",
    "    \"\"\"\n",
    "    Calculates the distance between two dictionaries.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    original : numpy.ndarray\n",
    "        The original dictionary.\n",
    "\n",
    "    new : numpy.ndarray\n",
    "        The new dictionary.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    catchCounter : int\n",
    "        The number of elements that satisfy the condition errorOfElement < 0.01.\n",
    "    totalDistances : float\n",
    "        The sum of all errorOfElement values.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # first: all the columns in the original start with positive values\n",
    "    catchCounter = 0\n",
    "    totalDistances = 0\n",
    "\n",
    "    for i in range(new.shape[1]):\n",
    "        new[:,i] = np.sign(new[0,i]) * new[:,i]\n",
    "\n",
    "    for i in range(original.shape[1]):\n",
    "        d = np.sign(original[0,i]) * original[:,i]\n",
    "        distances = np.sum(new - np.tile(d, (1, new.shape[1])), axis=0)\n",
    "        index = np.argmin(distances)\n",
    "        errorOfElement = 1 - np.abs(new[:,index].T @ d)\n",
    "        totalDistances += errorOfElement\n",
    "        catchCounter += errorOfElement < 0.01\n",
    "\n",
    "    ratio = catchCounter / original.shape[1]\n",
    "    return ratio, totalDistances\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def MOD(data, parameters):\n",
    "    \"\"\"\n",
    "    Method of Optimal Directions (MOD) algorithm for dictionary learning .\n",
    "\n",
    "    The MOD algorithm is a method for learning a dictionary for sparse representation of signals.\n",
    "    It iteratively updates the dictionary to best represent the input data with sparse coefficients\n",
    "    using the Orthogonal Matching Pursuit (OMP) algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        An (n x N) matrix containing N signals, each of dimension n.\n",
    "    \n",
    "    parameters : dict\n",
    "        A dictionary containing the parameters for the MOD algorithm:\n",
    "            - K : int\n",
    "                The number of dictionary elements (columns) to train.\n",
    "            \n",
    "            - num_iterations : int\n",
    "                The number of iterations to perform for dictionary learning.\n",
    "            \n",
    "            - initialization_method : str\n",
    "                Method to initialize the dictionary. Options are:\n",
    "                * 'DataElements' - Initializes the dictionary using the first K data signals.\n",
    "                * 'GivenMatrix' - Initializes the dictionary using a provided matrix \n",
    "                  (requires 'initial_dictionary' key).\n",
    "\n",
    "            - initial_dictionary : numpy.ndarray, optional\n",
    "                The initial dictionary matrix to use if 'initialization_method' is \n",
    "                set to 'GivenMatrix'. It should be of size (n x K).\n",
    "\n",
    "            - L : int\n",
    "                The number of non-zero coefficients to use in OMP for sparse\n",
    "                representation of each signal.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dictionary : numpy.ndarray\n",
    "        The trained dictionary of size (n x K), where each column is a dictionary element.\n",
    "\n",
    "    coef_matrix : numpy.ndarray\n",
    "        The coefficient matrix of size (K x N), representing the sparse representation\n",
    "        of the input data using the trained dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the number of signals is smaller than the dictionary size\n",
    "    if data.shape[1] < parameters['K']:\n",
    "        print(\"MOD: number of training signals is smaller than the dictionary size. Returning trivial solution...\")\n",
    "        dictionary = data[:, :data.shape[1]]\n",
    "        coef_matrix = np.eye(data.shape[1])  # Trivial coefficients\n",
    "        return dictionary, coef_matrix\n",
    "\n",
    "    # Initialize dictionary based on the specified method\n",
    "    if parameters['initialization_method'] == 'DataElements':\n",
    "        dictionary = data[:, :parameters['K']]\n",
    "    elif parameters['initialization_method'] == 'GivenMatrix':\n",
    "        if 'initial_dictionary' not in parameters:\n",
    "            raise ValueError(\"initial_dictionary parameter is required when \"\n",
    "                             \"initialization_method is set to 'GivenMatrix'.\")\n",
    "        dictionary = parameters['initial_dictionary']\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Invalid value for initialization_method. Choose 'DataElements' or 'GivenMatrix'.\")\n",
    "\n",
    "    # Convert to float64 for precision\n",
    "    dictionary = dictionary.astype(np.float64)\n",
    "\n",
    "    # Normalize dictionary columns and avoid division by zero\n",
    "    column_norms = np.linalg.norm(dictionary, axis=0)\n",
    "    column_norms[column_norms < 1e-10] = 1  # Prevent division by zero\n",
    "    dictionary /= column_norms\n",
    "\n",
    "    # Ensure positive first elements\n",
    "    dictionary *= np.sign(dictionary[0, :])\n",
    "\n",
    "    prev_dictionary = dictionary.copy()\n",
    "\n",
    "    # Run MOD algorithm\n",
    "    for iter_num in range(parameters['num_iterations']):\n",
    "        # Step 1: Sparse coding using OMP\n",
    "        coef_matrix = OMP(dictionary, data, parameters['L'])\n",
    "\n",
    "        # Step 2: Update the dictionary\n",
    "        regularization_term = 1e-7 * sp.eye(coef_matrix.shape[0])\n",
    "        matrix_a = coef_matrix @ coef_matrix.T + regularization_term.toarray()\n",
    "\n",
    "        # Use pinv for numerical stability: \"lstsq\" or \"regularizations\" could also work\n",
    "        dictionary = data @ coef_matrix.T @ np.linalg.pinv(matrix_a)\n",
    "\n",
    "\n",
    "        # Normalize dictionary columns and avoid division by zero\n",
    "        column_norms = np.linalg.norm(dictionary, axis=0)\n",
    "        column_norms[column_norms < 1e-10] = 1  # Prevent division by zero\n",
    "        dictionary /= column_norms\n",
    "\n",
    "        # Ensure positive first elements\n",
    "        dictionary *= np.sign(dictionary[0, :])\n",
    "\n",
    "        # Convergence check\n",
    "        if np.linalg.norm(dictionary - prev_dictionary) < 1e-5:\n",
    "            print(f\"MOD converged after {iter_num + 1} iterations.\")\n",
    "            break\n",
    "\n",
    "        prev_dictionary = dictionary.copy()\n",
    "\n",
    "    return dictionary, coef_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "K-SVD algorithm for dictionary learning and sparse coding using Orthogonal Matching Pursuit (OMP).\n",
    "Includes functions for updating dictionary elements, handling singular value decomposition (SVD)\n",
    "for vectors, and clearing redundant dictionary elements.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "\n",
    "def svds_vector(v):\n",
    "    \"\"\"\n",
    "    Handle SVD for a vector or a 2D matrix with one dimension equal to 1.\n",
    "    \"\"\"\n",
    "    v = np.asarray(v)\n",
    "    \n",
    "    if v.ndim == 1:\n",
    "        v = v.reshape(-1, 1)\n",
    "    elif v.ndim == 2 and (v.shape[0] == 1 or v.shape[1] == 1):\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a vector or a 2D array with one dimension equal to 1.\")\n",
    "    \n",
    "    s = np.linalg.norm(v)\n",
    "    if s > 0:\n",
    "        u = v / s\n",
    "    else:\n",
    "        u = np.zeros_like(v)\n",
    "    \n",
    "    vt = np.array([[1]])\n",
    "\n",
    "    return u, s, vt\n",
    "\n",
    "def I_findBetterDictionaryElement(data, dictionary, j, coeff_matrix, numCoefUsed=1):\n",
    "    \"\"\"\n",
    "    Update the j-th dictionary element.\n",
    "    \"\"\"\n",
    "    relevantDataIndices = np.nonzero(coeff_matrix[j, :])[0]\n",
    "    if relevantDataIndices.size == 0:\n",
    "        errorMat = data - dictionary @ coeff_matrix\n",
    "        errorNormVec = np.sum(errorMat ** 2, axis=0)\n",
    "        i = np.argmax(errorNormVec)\n",
    "        betterDictionaryElement = data[:, i] / np.linalg.norm(data[:, i])\n",
    "        betterDictionaryElement *= np.sign(betterDictionaryElement[0])\n",
    "        coeff_matrix[j, :] = 0\n",
    "        newVectAdded = 1\n",
    "        return betterDictionaryElement, coeff_matrix, newVectAdded\n",
    "    \n",
    "    newVectAdded = 0\n",
    "    tmpCoefMatrix = coeff_matrix[:, relevantDataIndices]\n",
    "    tmpCoefMatrix[j, :] = 0\n",
    "    errors = data[:, relevantDataIndices] - dictionary @ tmpCoefMatrix\n",
    "\n",
    "    if np.min(errors.shape) <= 1:\n",
    "        u, s, vt = svds_vector(errors)\n",
    "        betterDictionaryElement = u\n",
    "        singularValue = s\n",
    "        betaVector = vt\n",
    "    else:\n",
    "        u, s, vt = svds(errors, k=1)\n",
    "        betterDictionaryElement = u[:, 0]\n",
    "        singularValue = s[0]\n",
    "        betaVector = vt[0, :]\n",
    "\n",
    "    coeff_matrix[j, relevantDataIndices] = singularValue * betaVector.T\n",
    "\n",
    "    return betterDictionaryElement, coeff_matrix, newVectAdded\n",
    "\n",
    "def I_clearDictionary(dictionary, coeff_matrix, data):\n",
    "    \"\"\"\n",
    "    Clear or replace redundant dictionary elements.\n",
    "    \"\"\"\n",
    "    T2 = 0.99\n",
    "    T1 = 3\n",
    "    K = dictionary.shape[1]\n",
    "    Er = np.sum((data - dictionary @ coeff_matrix) ** 2, axis=0)\n",
    "    G = dictionary.T @ dictionary\n",
    "    G -= np.diag(np.diag(G))\n",
    "    for jj in range(K):\n",
    "        if np.max(G[jj, :]) > T2 or np.count_nonzero(np.abs(coeff_matrix[jj, :]) > 1e-7) <= T1:\n",
    "            pos = np.argmax(Er)\n",
    "            Er[pos] = 0\n",
    "            dictionary[:, jj] = data[:, pos] / np.linalg.norm(data[:, pos])\n",
    "            G = dictionary.T @ dictionary\n",
    "            G -= np.diag(np.diag(G))\n",
    "    return dictionary\n",
    "\n",
    "def KSVD(data, param):\n",
    "    \"\"\"\n",
    "    K-SVD algorithm for dictionary learning.\n",
    "    \"\"\"\n",
    "    if param['preserve_dc_atom'] > 0:\n",
    "        fixedDictElem = np.zeros((data.shape[0], 1))  \n",
    "        fixedDictElem[:data.shape[0], 0] = 1 / np.sqrt(data.shape[0])\n",
    "    else:\n",
    "        fixedDictElem = np.empty((0, 0))\n",
    "\n",
    "    if data.shape[1] < param['K']:\n",
    "        print('KSVD: number of training data is smaller than the dictionary size. Trivial solution...')\n",
    "        dictionary = data[:, :data.shape[1]]\n",
    "        coef_matrix = np.eye(data.shape[1])\n",
    "        return dictionary, coef_matrix\n",
    "    \n",
    "    dictionary = np.zeros((data.shape[0], param['K']), dtype=np.float64)    \n",
    "    if param['initialization_method'] == 'DataElements':\n",
    "        dictionary[:, :param['K'] - param['preserve_dc_atom']] = \\\n",
    "            data[:, :param['K'] - param['preserve_dc_atom']]\n",
    "    elif param['initialization_method'] == 'GivenMatrix':\n",
    "        dictionary[:, :param['K'] - param['preserve_dc_atom']] = \\\n",
    "            param['initial_dictionary'][:, :param['K'] - param['preserve_dc_atom']]\n",
    "\n",
    "    if param['preserve_dc_atom']:\n",
    "        tmpMat = np.linalg.lstsq(dictionary + 1e-7 * np.eye(dictionary.shape[1]), fixedDictElem, rcond=None)[0]\n",
    "        dictionary -= fixedDictElem @ tmpMat\n",
    "\n",
    "    column_norms = np.sqrt(np.sum(dictionary ** 2, axis=0))\n",
    "    column_norms[column_norms < 1e-10] = 1\n",
    "    dictionary /= column_norms\n",
    "    dictionary *= np.sign(dictionary[0, :])\n",
    "\n",
    "    for iterNum in range(param['num_iterations']):\n",
    "        coef_matrix = OMP(\n",
    "            np.hstack((fixedDictElem, dictionary)) if fixedDictElem.size > 0 else dictionary,\n",
    "            data,\n",
    "            param['L']\n",
    "        )\n",
    "        \n",
    "        rand_perm = np.random.permutation(dictionary.shape[1])\n",
    "        for j in rand_perm:\n",
    "            betterDictElem, coef_matrix, newVectAdded = I_findBetterDictionaryElement(\n",
    "                data,\n",
    "                np.hstack((fixedDictElem, dictionary)) if fixedDictElem.size > 0 else dictionary,\n",
    "                j + fixedDictElem.shape[1],\n",
    "                coef_matrix,\n",
    "                param['L']\n",
    "            )\n",
    "\n",
    "            dictionary[:, j] = betterDictElem.ravel()\n",
    "            if param['preserve_dc_atom']:\n",
    "                tmpCoeff = np.linalg.lstsq(betterDictElem + 1e-7, fixedDictElem, rcond=None)[0]\n",
    "                dictionary[:, j] -= fixedDictElem @ tmpCoeff\n",
    "                dictionary[:, j] /= np.linalg.norm(dictionary[:, j])\n",
    "\n",
    "        dictionary = I_clearDictionary(dictionary, coef_matrix[fixedDictElem.shape[1]:, :], data)\n",
    "\n",
    "    dictionary = np.hstack((fixedDictElem, dictionary)) if fixedDictElem.size > 0 else dictionary\n",
    "    \n",
    "    return dictionary, coef_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## ------------------------------------------------------------------------------------------------\n",
    "## REST OF THE FUNCTIONS ARE FOR TESTING PURPOSES\n",
    "## ------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "def compute_independent_columns(A, tol=1e-10):\n",
    "    \"\"\"\n",
    "    Computes the independent columns of a matrix using the QR decomposition.\n",
    "\n",
    "    The function identifies independent columns of a given matrix `A` by performing a QR \n",
    "    decomposition. It selects columns corresponding to non-zero diagonal elements of the \n",
    "    `R` matrix, which are considered linearly independent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : numpy.ndarray\n",
    "        The matrix for which to compute the independent columns.\n",
    "    tol : float, optional (default=1e-10)\n",
    "        The tolerance value for considering diagonal elements of `R` as non-zero.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ind_cols : numpy.ndarray\n",
    "        A matrix containing the independent columns of `A`.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The QR decomposition is used to determine the rank of the matrix `A`.\n",
    "    - Columns corresponding to non-zero diagonal elements of the `R` matrix are considered independent.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "    >>> compute_independent_columns(A)\n",
    "    array([[1, 2],\n",
    "           [4, 5],\n",
    "           [7, 8]])\n",
    "    \"\"\"\n",
    "    # Perform the QR decomposition\n",
    "    Q, R = np.linalg.qr(A)\n",
    "\n",
    "    # Find the independent columns based on the rank of R\n",
    "    rank = np.sum(np.abs(np.diagonal(R)) > tol)\n",
    "    ind_cols = A[:, :rank]\n",
    "\n",
    "    return ind_cols\n",
    "\n",
    "def check_normalization(A):\n",
    "    \"\"\"\n",
    "    Checks if the columns of a matrix are normalized (i.e., each column has a unit norm).\n",
    "\n",
    "    The function calculates the norm of each column in the matrix `A` and checks if all \n",
    "    column norms are close to 1.0, which indicates normalization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : numpy.ndarray\n",
    "        The matrix to check for normalization.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    is_normalized : bool\n",
    "        True if all columns of `A` are normalized, False otherwise.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> A = np.array([[1, 0], [0, 1]])\n",
    "    >>> check_normalization(A)\n",
    "    True\n",
    "    \"\"\"\n",
    "    column_norms = np.linalg.norm(A, axis=0)\n",
    "    is_normalized = np.allclose(column_norms, 1.0)\n",
    "    return is_normalized\n",
    "\n",
    "\n",
    "def compute_coherence(matrix):\n",
    "    \"\"\"\n",
    "    Computes the coherence of the given matrix.\n",
    "\n",
    "    Coherence is a measure of the maximum correlation between any two columns of a matrix. \n",
    "    It is useful in various applications, such as signal processing and compressed sensing, \n",
    "    to assess the degree of similarity between different columns of the matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : numpy.ndarray\n",
    "        An N x M matrix where coherence is to be calculated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    coherence : float\n",
    "        The coherence of the matrix, defined as the maximum absolute value of the off-diagonal \n",
    "        elements in the Gram matrix of the column-normalized input matrix.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> matrix = np.array([[1, 0], [0, 1]])\n",
    "    >>> compute_coherence(matrix)\n",
    "    0.0\n",
    "    \"\"\"\n",
    "    # Normalize the columns of the matrix\n",
    "    normalized_matrix = matrix / np.linalg.norm(matrix, axis=0, keepdims=True)\n",
    "    \n",
    "    # Compute the Gram matrix (inner products between all pairs of columns)\n",
    "    gram_matrix = np.dot(normalized_matrix.T, normalized_matrix)\n",
    "    \n",
    "    # Remove the diagonal elements (which are all 1's) to only consider distinct columns\n",
    "    np.fill_diagonal(gram_matrix, 0)\n",
    "    \n",
    "    # Compute the coherence as the maximum absolute value of the off-diagonal elements\n",
    "    coherence = np.max(np.abs(gram_matrix))\n",
    "    \n",
    "    return coherence\n",
    "\n",
    "\n",
    "def check_matrix_properties(A):\n",
    "    \"\"\"\n",
    "    Checks various properties of a matrix.\n",
    "\n",
    "    The function checks if the matrix `A` is full rank, if its columns and rows are normalized,\n",
    "    and computes the coherence of the matrix. It also prints the results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : numpy.ndarray\n",
    "        The matrix to check.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> A = np.array([[1, 2], [3, 4]])\n",
    "    >>> check_matrix_properties(A)\n",
    "    \"\"\"\n",
    "    # Check if the matrix is full rank\n",
    "    is_full_rank = np.linalg.matrix_rank(A) == min(A.shape)\n",
    "\n",
    "    # Check if the columns are normalized\n",
    "    is_columns_normalized = check_normalization(A)\n",
    "\n",
    "    # Check if the rows are normalized\n",
    "    is_rows_normalized = check_normalization(A.T)\n",
    "\n",
    "    # Compute the coherence of the matrix\n",
    "    coherence = compute_coherence(A)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Is full rank:\", is_full_rank)\n",
    "    print(\"Are columns normalized:\", is_columns_normalized)\n",
    "    print(\"Are rows normalized:\", is_rows_normalized)\n",
    "    print(\"Coherence:\", coherence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovery Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SL0 Algorithm Implementation\n",
    "\n",
    "This file contains an implementation of the Smoothed L0 (SL0) algorithm for sparse signal recovery.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def SL0(y, A, sigma_min, sigma_decrease_factor=0.5, mu_0=2, L=3, A_pinv=None, showProgress=False):\n",
    "    \"\"\"\n",
    "    Returns the sparsest vector `s` that satisfies the underdetermined system of \n",
    "    linear equations `A @ s = y`, using the Smoothed L0 (SL0) algorithm.\n",
    "\n",
    "    Requires:\n",
    "    --------\n",
    "    - numpy as np\n",
    "    \n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    y : numpy array\n",
    "        The observed vector (Mx1), where M is the number of rows in `A`.\n",
    "    \n",
    "    A : numpy array\n",
    "        The measurement matrix (MxN), which should be 'wide', meaning it has more \n",
    "        columns than rows (N > M). The number of rows in `A` must match the length \n",
    "        of `y`.\n",
    "    \n",
    "    sigma_min : float\n",
    "        The minimum value of `sigma`, which determines the stopping criterion for \n",
    "        the algorithm. It should be chosen based on the noise level or desired \n",
    "        accuracy.\n",
    "    \n",
    "    sigma_decrease_factor : float, optional (default=0.5)\n",
    "        The factor by which `sigma` is decreased in each iteration. This should be \n",
    "        a positive value less than 1. Smaller values lead to quicker reduction of \n",
    "        `sigma`, possibly at the cost of accuracy for less sparse signals.\n",
    "    \n",
    "    mu_0 : float, optional (default=2)\n",
    "        The scaling factor for `mu`, where `mu = mu_0 * sigma^2`. This parameter \n",
    "        influences the convergence rate of the algorithm.\n",
    "    \n",
    "    L : int, optional (default=3)\n",
    "        The number of iterations for the inner loop (steepest descent). Increasing \n",
    "        `L` can improve the precision of the result but also increases computational \n",
    "        cost.\n",
    "    \n",
    "    A_pinv : numpy array, optional\n",
    "        The precomputed pseudoinverse of the matrix `A`. If not provided, it will be \n",
    "        calculated within the function as `np.linalg.pinv(A)`. Providing this value \n",
    "        is beneficial if the function is called repeatedly with the same `A`.\n",
    "    \n",
    "    showProgress : bool, optional (default=False)\n",
    "        If `True`, the function prints the current value of `sigma` during each \n",
    "        iteration, which helps monitor the convergence process.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    s : numpy array\n",
    "        The estimated sparse signal (Nx1) that best satisfies the equation `A @ s = y`.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The algorithm works by iteratively reducing `sigma` in a geometric sequence, \n",
    "      starting with `sigma = 2 * max(abs(s))` and ending with `sigma_min`. At each \n",
    "      step, the function adjusts `s` to minimize the L0-norm by smoothing it using \n",
    "      a Gaussian kernel.\n",
    "    \n",
    "    - The choice of `sigma_min` is crucial: for noiseless cases, a smaller `sigma_min` \n",
    "      yields a sparser solution; for noisy cases, `sigma_min` should be a few times \n",
    "      the standard deviation of the noise in `s`.\n",
    "\n",
    "    - If `A_pinv` is precomputed and passed as an argument, the function becomes \n",
    "      more efficient, especially in scenarios where it is called repeatedly with the \n",
    "      same `A`.\n",
    "\n",
    "      \n",
    "      References:\n",
    "      ----------\n",
    "    - Original authors (MATLAB): Massoud Babaie-Zadeh, Hossein Mohimani, 4 August 2008.\n",
    "    - Web-page: http://ee.sharif.ir/~SLzero\n",
    "\n",
    "    - Ported to python: RosNaviGator https://github.com/RosNaviGator, 2024\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "    if A_pinv is None:\n",
    "        A_pinv = np.linalg.pinv(A)\n",
    "        \n",
    "    # Initialize the variables\n",
    "    s = A_pinv @ y\n",
    "    sigma = 2 * max(np.abs(s))\n",
    "\n",
    "    # Define lambda function for delta\n",
    "    OurDelta = lambda s, sigma: s * np.exp(-s**2 / sigma**2)\n",
    " \n",
    "    # Main loop\n",
    "    while sigma > sigma_min:\n",
    "        for i in range(L):\n",
    "            delta = OurDelta(s, sigma)\n",
    "            s = s - mu_0 * delta\n",
    "            s = s - A_pinv @ (A @ s - y)\n",
    "        \n",
    "        if showProgress:\n",
    "            print(f'sigma: {sigma}')\n",
    "\n",
    "        sigma = sigma * sigma_decrease_factor\n",
    "\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script provides functions to calculate the Signal-to-Noise Ratio (SNR) between \n",
    "an original and a reconstructed signal, and to plot these signals together, displaying \n",
    "the SNR. It also includes functionality to save the plotted signals to a specified directory.\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def calculate_snr(signal, recovered_signal):\n",
    "    \"\"\"\n",
    "    Calculates the Signal-to-Noise Ratio (SNR) between the original signal and the recovered signal.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : numpy.ndarray\n",
    "        The original signal.\n",
    "    recovered_signal : numpy.ndarray\n",
    "        The recovered signal after some processing or recovery algorithm.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    snr : float\n",
    "        The Signal-to-Noise Ratio (SNR) in decibels (dB).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The SNR is calculated as 20 * log10(norm(original_signal) / norm(original_signal - recovered_signal)).\n",
    "    - A higher SNR value indicates a better recovery, with less error relative to the original signal.\n",
    "    \"\"\"\n",
    "    error = recovered_signal - signal\n",
    "    snr = 20 * np.log10(np.linalg.norm(signal) / np.linalg.norm(error))\n",
    "    \n",
    "    return snr\n",
    "\n",
    "\n",
    "\n",
    "def plot_signals(original_signal, reconstructed_signal, snr=None, original_name=\"Original Signal\", \n",
    "                 reconstructed_name=\"Reconstructed Signal\", save_path=None, filename=None,\n",
    "                 start_pct=0.0, num_samples=None):\n",
    "    \"\"\"\n",
    "    Plots a section of the original signal and the reconstructed signal on the same plot with the given names,\n",
    "    displays the Signal-to-Noise Ratio (SNR) in a text box, and saves the plot to a specified directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    original_signal : numpy.ndarray\n",
    "        The original signal to be plotted.\n",
    "    \n",
    "    reconstructed_signal : numpy.ndarray\n",
    "        The reconstructed signal to be plotted.\n",
    "    \n",
    "    reconstructed_name : str, optional (default=\"Reconstructed Signal\")\n",
    "        The name to display for the reconstructed signal in the plot.\n",
    "    \n",
    "    save_path : str, optional\n",
    "        The directory path where the plot should be saved. If None, the plot will not be saved.\n",
    "    \n",
    "    filename : str, optional\n",
    "        The name of the file to save the plot as. If None and save_path is provided, a default name will be generated.\n",
    "    \n",
    "    snr : float, optional (default=None)\n",
    "        The Signal-to-Noise Ratio to display. If None, it will be computed using the original and reconstructed signals.\n",
    "    \n",
    "    start_pct : float, optional (default=0.0)\n",
    "        The percentage (between 0 and 1) of the way through the signal to start plotting. For example, 0.5 means start \n",
    "        from the halfway point of the signals.\n",
    "    \n",
    "    num_samples : int, optional (default=None)\n",
    "        The number of samples to plot from the start point. If None, it will plot to the end of the signals.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the signals have the same length\n",
    "    if len(original_signal) != len(reconstructed_signal):\n",
    "        raise ValueError(\"The original signal and the reconstructed signal must have the same length.\")\n",
    "    \n",
    "    # Calculate the start index based on percentage\n",
    "    start_idx = int(start_pct * len(original_signal))\n",
    "    \n",
    "    # Determine the end index based on num_samples\n",
    "    if num_samples is not None:\n",
    "        end_idx = start_idx + num_samples\n",
    "    else:\n",
    "        end_idx = len(original_signal)\n",
    "    \n",
    "    # Check if the end index exceeds the signal length\n",
    "    if end_idx > len(original_signal):\n",
    "        raise ValueError(f\"You tried to plot from sample {start_idx} to sample {end_idx}, \"\n",
    "                         f\"but the signal only has {len(original_signal)} samples!\")\n",
    "    \n",
    "    # Slice the signals to the selected section\n",
    "    original_signal_section = original_signal[start_idx:end_idx]\n",
    "    reconstructed_signal_section = reconstructed_signal[start_idx:end_idx]\n",
    "    \n",
    "    # Calculate SNR if not provided\n",
    "    if snr is None:\n",
    "        snr = calculate_snr(original_signal_section, reconstructed_signal_section)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(original_signal_section, label=original_name, color='blue', linewidth=1.5)\n",
    "    plt.plot(reconstructed_signal_section, label=reconstructed_name, color='red', linestyle='--', linewidth=1.5)\n",
    "    \n",
    "    # Title and labels\n",
    "    plt.title(f\"{original_name} vs {reconstructed_name} (Section: {start_pct*100:.1f}% - {num_samples} samples)\")\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Amplitude')\n",
    "    \n",
    "    # Add a legend in the upper-right corner with a white background\n",
    "    plt.legend(loc='upper right', frameon=True, facecolor='white')\n",
    "    \n",
    "    # Display SNR in a text box in the top-left corner with a white background\n",
    "    plt.text(0.05, 0.95, f'SNR: {snr:.2f} dB', transform=plt.gca().transAxes,\n",
    "            fontsize=12, verticalalignment='top', horizontalalignment='left',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "    \n",
    "    # Grid and show plot\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save the plot if a save path is provided\n",
    "    if save_path is not None:\n",
    "        # Ensure the save directory exists\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        # Use provided filename or generate a default one\n",
    "        if filename is None:\n",
    "            filename = f\"{original_name}_vs_{reconstructed_name}_section.png\"\n",
    "        \n",
    "        # Define the file path to save the plot\n",
    "        file_path = os.path.join(save_path, filename)\n",
    "        plt.savefig(file_path)\n",
    "        print(f\"Plot saved to {file_path}\")\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressed Sensing class (higher level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "class compressedSensing:\n",
    "    def __init__(self, signal, BLOCK_LEN, CR, matrix_type='gaussian'):\n",
    "        \"\"\"\n",
    "        Constructor for the compressedSensing class.\n",
    "        \"\"\"\n",
    "        self.signal = signal\n",
    "        self.BLOCK_LEN = BLOCK_LEN\n",
    "        self.CR = CR\n",
    "        self.COMP_LEN = BLOCK_LEN // CR  # Compression length (number of rows in Phi)\n",
    "        \n",
    "        # Generate measurement matrix Phi based on the specified type\n",
    "        if matrix_type == 'DBBD':\n",
    "            self.Phi = generate_DBBD_matrix(self.COMP_LEN, self.BLOCK_LEN)\n",
    "        else:\n",
    "            self.Phi = generate_random_matrix(self.COMP_LEN, self.BLOCK_LEN, matrix_type=matrix_type)\n",
    "        \n",
    "        # Initialize attributes\n",
    "        self.training_set = None\n",
    "        self.test_set = None\n",
    "        self.training_matrix = None\n",
    "        self.reconstructed_signal = None\n",
    "        self.Y = None  # Compressed test signal\n",
    "        self.theta = None\n",
    "        self.theta_pinv = None\n",
    "        self.dictionary = None\n",
    "        self.coeff_matrix = None  # Coefficients from MOD or K-SVD algorithm\n",
    "\n",
    "    def divide_signal(self, training_percentage):\n",
    "        \"\"\"\n",
    "        Divides the signal into a training set and a test set based on the given percentage.\n",
    "        \"\"\"\n",
    "        training_size = int(training_percentage * len(self.signal))\n",
    "        \n",
    "        # Warning if the training set is longer than 1 hour (360 samples/min * 60 min)\n",
    "        if training_size > 360 * 60:\n",
    "            warnings.warn(\"The training set is longer than 1 hour (360 samples/min * 60 min). \"\n",
    "                          \"This warning is based on the MIT-BIH Arrhythmia Database, \"\n",
    "                          \"and may not apply to other datasets.\")\n",
    "        \n",
    "        # Warning if using more than 10% of the signal for training\n",
    "        if training_percentage > 0.1:\n",
    "            warnings.warn(\"Using more than 10% of the signal for training may not make sense.\")\n",
    "        \n",
    "        # Define the training and test sets\n",
    "        self.training_set = self.signal[:training_size]\n",
    "        self.test_set = self.signal[training_size:]\n",
    "\n",
    "        # Ensure the test set size is a multiple of BLOCK_LEN by truncating the test set\n",
    "        test_size = len(self.test_set)\n",
    "        test_size_multiple = (test_size // self.BLOCK_LEN) * self.BLOCK_LEN\n",
    "        self.test_set = self.test_set[:test_size_multiple]\n",
    "\n",
    "        # Ensure the training set size is a multiple of BLOCK_LEN\n",
    "        num_cols = training_size // self.BLOCK_LEN\n",
    "        if num_cols < self.BLOCK_LEN:\n",
    "            warnings.warn(\"The number of samples (columns) in the training matrix is shorter than \"\n",
    "                          \"the number of rows, which can cause issues with dictionary learning.\")\n",
    "\n",
    "        # Reshape the training set using Fortran-style ordering ('F')\n",
    "        self.training_matrix = self.training_set[:num_cols * self.BLOCK_LEN].reshape(self.BLOCK_LEN, num_cols, order='F')\n",
    "\n",
    "         # Print the shapes of the training set, test set, and training matrix\n",
    "        print(f\"Training set shape: {self.training_set.shape}\")\n",
    "        print(f\"Test set shape: {self.test_set.shape}\")\n",
    "        print(f\"Training matrix shape: {self.training_matrix.shape}\")\n",
    "\n",
    "    def compress_test_set(self):\n",
    "        \"\"\"\n",
    "        Compresses the test set using the measurement matrix Phi.\n",
    "        \"\"\"\n",
    "        if self.test_set is None:\n",
    "            raise ValueError(\"Test set not defined. Please divide the signal before compressing.\")\n",
    "\n",
    "        M, N = self.Phi.shape\n",
    "        SIGNAL_BLOCKS = len(self.test_set) // N\n",
    "        self.Y = np.zeros((M, SIGNAL_BLOCKS))\n",
    "\n",
    "        # Sampling phase: Compress signal block-wise\n",
    "        for i in range(SIGNAL_BLOCKS):\n",
    "            self.Y[:, i] = self.Phi @ self.test_set[i * N: (i + 1) * N]\n",
    "\n",
    "    def kronecker_activate(self, KRON_FACT):\n",
    "        \"\"\"\n",
    "        Activates Kronecker compression, adjusting BLOCK_LEN and reprocessing Phi and Y accordingly.\n",
    "        \"\"\"\n",
    "        if self.Y is None:\n",
    "            raise ValueError(\n",
    "                \"Y has not been computed. Please divide the signal in training_set and test_set, then compress the test set first.\")\n",
    "\n",
    "        self.KRON_FACT = KRON_FACT\n",
    "        self.BLOCK_LEN = self.BLOCK_LEN * self.KRON_FACT\n",
    "        \n",
    "        # Compute Kronecker product for Phi\n",
    "        self.Phi = np.kron(np.eye(self.KRON_FACT), self.Phi)\n",
    "        \n",
    "        # Reprocess the training set\n",
    "        training_size = len(self.training_set)\n",
    "        num_cols = training_size // self.BLOCK_LEN\n",
    "        if num_cols < self.BLOCK_LEN:\n",
    "            warnings.warn(\"The number of samples (columns) in the training matrix is shorter than \"\n",
    "                          \"the number of rows, which can cause issues with dictionary learning.\")\n",
    "        \n",
    "        self.training_matrix = self.training_set[:num_cols * self.BLOCK_LEN].reshape(self.BLOCK_LEN, num_cols, order='F')\n",
    "        \n",
    "        print(f\"Training set shape (Kronecker): {self.training_set.shape}\")\n",
    "        print(f\"Test set shape (Kronecker): {self.test_set.shape}\")\n",
    "        print(f\"Training matrix shape (Kronecker): {self.training_matrix.shape}\")\n",
    "        \n",
    "        # Generate Y_kron from Y by concatenating KRON_FACT consecutive columns\n",
    "        M, SIGNAL_BLOCKS = self.Y.shape\n",
    "        SIGNAL_BLOCKS_KRON = len(self.test_set) // self.BLOCK_LEN\n",
    "        temp_y = np.zeros((M * self.KRON_FACT, SIGNAL_BLOCKS_KRON))\n",
    "\n",
    "        for i in range(SIGNAL_BLOCKS_KRON):\n",
    "            temp_y[:, i] = self.Y[:, i * self.KRON_FACT: (i + 1) * self.KRON_FACT].flatten(order='F')\n",
    "        \n",
    "        self.Y = temp_y\n",
    "\n",
    "\n",
    "    def generate_dictionary(self, dictionary_type='dct', mod_params=None, ksvd_params=None):\n",
    "        \"\"\"\n",
    "        Generates the dictionary based on the specified type. Supports DCT, MOD, and K-SVD.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        dictionary_type : str\n",
    "            The type of dictionary to generate ('dct', 'mod', 'ksvd').\n",
    "        mod_params : dict, optional\n",
    "            Dictionary of parameters for MOD algorithm if using MOD.\n",
    "        ksvd_params : dict, optional\n",
    "            Dictionary of parameters for K-SVD algorithm if using K-SVD.\n",
    "        \"\"\"\n",
    "        if dictionary_type == 'dct':\n",
    "            self.dictionary = dct_dictionary(self.BLOCK_LEN)\n",
    "        elif dictionary_type == 'mod':\n",
    "            if self.training_matrix is None:\n",
    "                raise ValueError(\"Training matrix not defined. Please divide the signal before running MOD.\")\n",
    "            if mod_params is None:\n",
    "                raise ValueError(\"MOD parameters not provided.\")\n",
    "            # Run MOD algorithm with training matrix and mod_params\n",
    "            self.dictionary, self.coeff_matrix = MOD(self.training_matrix, mod_params)\n",
    "        elif dictionary_type == 'ksvd':\n",
    "            if self.training_matrix is None:\n",
    "                raise ValueError(\"Training matrix not defined. Please divide the signal before running K-SVD.\")\n",
    "            if ksvd_params is None:\n",
    "                raise ValueError(\"K-SVD parameters not provided.\")\n",
    "            # Run K-SVD algorithm with training matrix and ksvd_params\n",
    "            self.dictionary, self.coeff_matrix = KSVD(self.training_matrix, ksvd_params)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported dictionary type. Use 'dct', 'mod', or 'ksvd'.\")\n",
    "\n",
    "    def recover_signal(self, sigma_min=1e-4, sigma_decrease_factor=0.5, mu_0=2, L=3, showProgress=False):\n",
    "        \"\"\"\n",
    "        Recovers the original signal using the SL0 algorithm, after compressing.\n",
    "        \"\"\"\n",
    "        if self.Y is None:\n",
    "            raise ValueError(\"Test set has not been compressed. Please compress the signal first.\")\n",
    "        if self.dictionary is None:\n",
    "            raise ValueError(\"Dictionary has not been generated. Please generate a dictionary before recovery.\")\n",
    "        \n",
    "        M, N = self.Phi.shape\n",
    "        SIGNAL_BLOCKS = self.Y.shape[1]\n",
    "        reconstructed_signal = np.zeros(N * SIGNAL_BLOCKS)\n",
    "\n",
    "        # Precompute theta and theta_pinv\n",
    "        self.theta = self.Phi @ self.dictionary\n",
    "        self.theta_pinv = np.linalg.pinv(self.theta)\n",
    "\n",
    "        # SL0 recovery for each block\n",
    "        for i in range(SIGNAL_BLOCKS):\n",
    "            y = self.Y[:, i]\n",
    "\n",
    "            # SL0: Sparse reconstruction\n",
    "            xp = SL0(y, self.theta, sigma_min, sigma_decrease_factor, mu_0, L, self.theta_pinv, showProgress=showProgress)\n",
    "\n",
    "            # Recovery Phase: Reconstruct the original signal\n",
    "            reconstructed_signal[i * N : (i + 1) * N] = self.dictionary @ xp\n",
    "\n",
    "        # Store the reconstructed signal as an attribute\n",
    "        self.reconstructed_signal = reconstructed_signal\n",
    "\n",
    "    def plot_reconstructed_vs_original(self, save_path=None, filename=None, start_pct=0.0, num_samples=None):\n",
    "        \"\"\"\n",
    "        Plots the original test set against the reconstructed signal using external plotting functions.\n",
    "        \"\"\"\n",
    "        if self.reconstructed_signal is None:\n",
    "            raise ValueError(\"Reconstructed signal not found. Please call recover_signal() first.\")\n",
    "        \n",
    "        if self.test_set is None:\n",
    "            raise ValueError(\"Test set not found. Please divide the signal before plotting.\")\n",
    "\n",
    "        # print len of the two\n",
    "        print(f\"Original signal length: {len(self.test_set)}\")\n",
    "        print(f\"Reconstructed signal length: {len(self.reconstructed_signal)}\")\n",
    "\n",
    "        # Calculate the start index based on    percentage\n",
    "        total_samples = min(len(self.test_set), len(self.reconstructed_signal))\n",
    "        start_idx = int(start_pct * total_samples)\n",
    "\n",
    "        # Determine the end index based on num_samples or plot till the end if num_samples is None\n",
    "        if num_samples is None:\n",
    "            end_idx = total_samples\n",
    "        else:\n",
    "            end_idx = min(start_idx + num_samples, total_samples)\n",
    "\n",
    "        # Slice the signals for plotting\n",
    "        original_signal_section = self.test_set[start_idx:end_idx]\n",
    "        reconstructed_signal_section = self.reconstructed_signal[start_idx:end_idx]\n",
    "\n",
    "        # Calculate SNR between the original test set and the reconstructed signals\n",
    "        snr = calculate_snr(original_signal_section, reconstructed_signal_section)\n",
    "\n",
    "        # Plot the selected section of the signals and display SNR\n",
    "        plot_signals(\n",
    "            original_signal_section, \n",
    "            reconstructed_signal_section, \n",
    "            snr=snr, \n",
    "            original_name=\"Original Test Set\", \n",
    "            reconstructed_name=\"Reconstructed Signal\", \n",
    "            save_path=save_path, \n",
    "            filename=filename,\n",
    "            start_pct=start_pct,\n",
    "            num_samples=len(original_signal_section)  # Update plot title with actual number of samples being plotted\n",
    "        )\n",
    "\n",
    "    def get_measurement_matrix(self):\n",
    "        \"\"\"Retrieves the measurement matrix Phi.\"\"\"\n",
    "        return self.Phi\n",
    "\n",
    "    def get_compressed_signal(self):\n",
    "        \"\"\"Retrieves the compressed signal Y.\"\"\"\n",
    "        return self.Y\n",
    "\n",
    "    def get_dictionary(self):\n",
    "        \"\"\"Retrieves the generated dictionary.\"\"\"\n",
    "        return self.dictionary\n",
    "\n",
    "    def get_coeff_matrix(self):\n",
    "        \"\"\"Retrieves the coefficient matrix from MOD or K-SVD algorithm.\"\"\"\n",
    "        if self.coeff_matrix is None:\n",
    "            raise ValueError(\"The coefficient matrix has not been generated yet. Call generate_dictionary() with MOD or K-SVD first.\")\n",
    "        return self.coeff_matrix\n",
    "\n",
    "    def get_reconstructed_signal(self):\n",
    "        \"\"\"Retrieves the reconstructed signal after applying SL0.\"\"\"\n",
    "        if self.reconstructed_signal is None:\n",
    "            raise ValueError(\"The signal has not been reconstructed yet. Call recover_signal() first.\")\n",
    "        return self.reconstructed_signal\n",
    "\n",
    "    def get_original_signal(self):\n",
    "        \"\"\"Retrieves the original test signal that was passed to the class.\"\"\"\n",
    "        return self.test_set\n",
    "\n",
    "    def get_theta(self):\n",
    "        \"\"\"Retrieves theta (Phi @ dictionary).\"\"\"\n",
    "        if self.theta is None:\n",
    "            raise ValueError(\"theta has not been computed yet. Call recover_signal() first.\")\n",
    "        return self.theta\n",
    "\n",
    "    def get_theta_pinv(self):\n",
    "        \"\"\"Retrieves theta_pinv (pseudoinverse of Phi @ dictionary).\"\"\"\n",
    "        if self.theta_pinv is None:\n",
    "            raise ValueError(\"theta_pinv has not been computed yet. Call recover_signal() first.\")\n",
    "        return self.theta_pinv\n",
    "    \n",
    "    def get_test_set(self):\n",
    "        \"\"\"Retrieves the test set.\"\"\"\n",
    "        if self.test_set is None:\n",
    "            raise ValueError(\"Test set not defined. Please divide the signal before retrieving.\")\n",
    "        return self.test_set\n",
    "\n",
    "    def get_training_set(self):\n",
    "        \"\"\"Retrieves the training set.\"\"\"\n",
    "        if self.training_set is None:\n",
    "            raise ValueError(\"Training set not defined. Please divide the signal before retrieving.\")\n",
    "        return self.training_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (False):\n",
    "        \n",
    "    # Load the data\n",
    "    import scipy.io\n",
    "\n",
    "    data0 = scipy.io.loadmat('100m.mat')\n",
    "    # Extract the ECG data\n",
    "    signal = data0['val'][0]  # MLII ECG data\n",
    "    # Define the portion of the signal you want to use (from 10 to 100 minutes)\n",
    "    signal = signal[360*0:360*60*2]  # Taking a portion of the signal for testing, sample_frequency  = 360 samples/min\n",
    "\n",
    "    # Define parameters\n",
    "    BLOCK_LEN = 16\n",
    "    CR = 4\n",
    "    matrix_type = 'DBBD'  # Use 'DBBD' 'gaussian' 'scaled_binary' 'unscaled_binary'\n",
    "\n",
    "    # Instantiate the class\n",
    "    cs = compressedSensing(signal, BLOCK_LEN, CR, matrix_type)\n",
    "\n",
    "    # Divide the signal (e.g., use 5% of signal for training)\n",
    "    cs.divide_signal(training_percentage=0.4)\n",
    "\n",
    "    # Plot the test_set\n",
    "    print(\"Plotting the original ECG signal...\")\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    test_set = cs.get_test_set()\n",
    "    plt.plot(test_set, color='blue')\n",
    "    plt.title(\"Original ECG Signal\")\n",
    "    plt.xlabel(\"Sample Index\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Compress the test set\n",
    "    cs.compress_test_set()\n",
    "\n",
    "    # ----------------- Without Kronecker Compression -----------------\n",
    "    print(\"Running without Kronecker method...\")\n",
    "\n",
    "    # ----------------- DCT-Based Dictionary Recovery -----------------\n",
    "\n",
    "    # Generate the DCT dictionary\n",
    "    cs.generate_dictionary(dictionary_type='dct')\n",
    "\n",
    "    # Recover the original signal using DCT dictionary\n",
    "    cs.recover_signal()\n",
    "\n",
    "    # Plot the reconstructed vs original signal (DCT-based)\n",
    "    print(\"Plotting DCT-based reconstruction...\")\n",
    "    cs.plot_reconstructed_vs_original(start_pct=0.0, num_samples=None)\n",
    "\n",
    "    # ----------------- MOD-Based Dictionary Recovery -----------------\n",
    "\n",
    "    # MOD dictionary learning parameters\n",
    "    mod_params = {\n",
    "        'K': BLOCK_LEN,  # Number of dictionary elements\n",
    "        'num_iterations': 50,\n",
    "        'initialization_method': 'DataElements',  # Use 'DataElements' for initialization\n",
    "        'L': 4  # Number of non-zero coefficients to use in OMP\n",
    "    }\n",
    "\n",
    "    # Generate the dictionary using MOD\n",
    "    cs.generate_dictionary(dictionary_type='mod', mod_params=mod_params)\n",
    "\n",
    "    # Recover the original signal using MOD dictionary\n",
    "    cs.recover_signal()\n",
    "\n",
    "    # Plot the reconstructed vs original signal (MOD-based)\n",
    "    print(\"Plotting MOD-based reconstruction...\")\n",
    "    cs.plot_reconstructed_vs_original(start_pct=0.0, num_samples=None)\n",
    "\n",
    "    # ----------------- K-SVD-Based Dictionary Recovery -----------------\n",
    "\n",
    "    # K-SVD dictionary learning parameters\n",
    "    ksvd_params = {\n",
    "        'K': BLOCK_LEN,  # Number of dictionary elements\n",
    "        'num_iterations': 50,\n",
    "        'initialization_method': 'DataElements',  # Use 'DataElements' for initialization\n",
    "        'L': 4,  # Number of non-zero coefficients to use in OMP\n",
    "        'preserve_dc_atom': 0  # Whether to preserve DC atom (0 for no)\n",
    "    }\n",
    "\n",
    "    # Generate the dictionary using K-SVD\n",
    "    cs.generate_dictionary(dictionary_type='ksvd', ksvd_params=ksvd_params)\n",
    "\n",
    "    # Recover the original signal using K-SVD dictionary\n",
    "    cs.recover_signal()\n",
    "\n",
    "    # Plot the reconstructed vs original signal (K-SVD-based)\n",
    "    print(\"Plotting K-SVD-based reconstruction...\")\n",
    "    cs.plot_reconstructed_vs_original(start_pct=0.0, num_samples=None)\n",
    "\n",
    "\n",
    "    # ----------------- Activate Kronecker Compression -----------------\n",
    "    print(\"Running with Kronecker method...\")\n",
    "\n",
    "    # Activate Kronecker compression with KRON_FACT (Kronecker factor)\n",
    "    KRON_FACT = 8  # Example Kronecker factor\n",
    "    cs.kronecker_activate(KRON_FACT)\n",
    "\n",
    "    # ----------------- DCT-Based Dictionary Recovery with Kronecker -----------------\n",
    "\n",
    "    # Generate the DCT dictionary (Kronecker-activated)\n",
    "    cs.generate_dictionary(dictionary_type='dct')\n",
    "\n",
    "    # Recover the original signal using DCT dictionary (Kronecker-activated)\n",
    "    cs.recover_signal()\n",
    "\n",
    "    # Plot the reconstructed vs original signal (DCT-based with Kronecker)\n",
    "    print(\"Plotting DCT-based reconstruction with Kronecker...\")\n",
    "    cs.plot_reconstructed_vs_original(start_pct=0.0, num_samples=None)\n",
    "\n",
    "    # ----------------- MOD-Based Dictionary Recovery with Kronecker -----------------\n",
    "\n",
    "    # MOD dictionary learning parameters\n",
    "    mod_params = {\n",
    "        'K': KRON_FACT * BLOCK_LEN,  # Number of dictionary elements\n",
    "        'num_iterations': 50,\n",
    "        'initialization_method': 'DataElements',  # Use 'DataElements' for initialization\n",
    "        'L': 4  # Number of non-zero coefficients to use in OMP\n",
    "    }\n",
    "\n",
    "    # Generate the dictionary using MOD (Kronecker-activated)\n",
    "    cs.generate_dictionary(dictionary_type='mod', mod_params=mod_params)\n",
    "\n",
    "    # Recover the original signal using MOD dictionary (Kronecker-activated)\n",
    "    cs.recover_signal()\n",
    "\n",
    "    # Plot the reconstructed vs original signal (MOD-based with Kronecker)\n",
    "    print(\"Plotting MOD-based reconstruction with Kronecker...\")\n",
    "    cs.plot_reconstructed_vs_original(start_pct=0.0, num_samples=None)\n",
    "\n",
    "    # ----------------- K-SVD-Based Dictionary Recovery with Kronecker -----------------\n",
    "\n",
    "    # K-SVD dictionary learning parameters\n",
    "    ksvd_params = {\n",
    "        'K': KRON_FACT * BLOCK_LEN,  # Number of dictionary elements\n",
    "        'num_iterations': 50,\n",
    "        'initialization_method': 'DataElements',  # Use 'DataElements' for initialization\n",
    "        'L': 4,  # Number of non-zero coefficients to use in OMP\n",
    "        'preserve_dc_atom': 0  # Whether to preserve DC atom (0 for no)\n",
    "    }\n",
    "\n",
    "    # Generate the dictionary using K-SVD (Kronecker-activated)\n",
    "    cs.generate_dictionary(dictionary_type='ksvd', ksvd_params=ksvd_params)\n",
    "\n",
    "    # Recover the original signal using K-SVD dictionary (Kronecker-activated)\n",
    "    cs.recover_signal()\n",
    "\n",
    "    # Plot the reconstructed vs original signal (K-SVD-based with Kronecker)\n",
    "    print(\"Plotting K-SVD-based reconstruction with Kronecker...\")\n",
    "    cs.plot_reconstructed_vs_original(start_pct=0.0, num_samples=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".namlVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
