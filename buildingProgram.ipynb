{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script provides utility functions for formatted printing of NumPy matrices and saving matrices to CSV files.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# system imports\n",
    "import os\n",
    "\n",
    "# third party imports\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def printFormatted(matrix, decimals=4):\n",
    "    \"\"\"\n",
    "    Prints the matrix with formatted elements aligned in columns for improved readability.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    matrix : numpy array\n",
    "        The matrix to be printed.\n",
    "    decimals : int, optional (default=4)\n",
    "        The number of decimal places for formatting the elements.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    None\n",
    "        This function does not return any value; it prints the formatted matrix directly to the console.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The function aligns columns based on the maximum width needed for the formatted elements, ensuring the matrix is displayed neatly.\n",
    "    - This function is useful for visual inspection of numerical matrices, especially those with varying magnitudes.\n",
    "    \n",
    "    Example:\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> matrix = np.array([[1.234567, 123.456789], [0.0001234, 1.2345]])\n",
    "    >>> print('Classic print:')\n",
    "    >>> print(matrix)\n",
    "    Classic print:\n",
    "    [[1.2345670e+00 1.2345679e+02]\n",
    "     [1.2340000e-04 1.2345000e+00]]\n",
    "     \n",
    "    >>> print('\\nFormatted print:')\n",
    "    >>> printFormatted(matrix, decimals=4)\n",
    "         1.2346  123.4568\n",
    "         0.0001    1.2345\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Determine the maximum width needed to keep alignment\n",
    "    max_width = max(len(f'{value:.{decimals}f}') for row in matrix for value in row)\n",
    "\n",
    "    # Create a formatted string for each element in the matrix, ensuring alignment\n",
    "    formatted_matrix = '\\n'.join([' '.join([f'{value:>{max_width}.{decimals}f}' for value in row]) for row in matrix])\n",
    "\n",
    "    # Print the formatted matrix\n",
    "    print(formatted_matrix)\n",
    "\n",
    "\n",
    "def py_test_csv(array):\n",
    "    \"\"\"\n",
    "    Save a numpy array as a CSV file in ./debugCsvPy/py_test.csv\n",
    "\n",
    "    Parameters:\n",
    "    array (numpy.ndarray): The input array to be saved as a CSV file.\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    output_dir = 'debugCsvPy'  # Directory where CSV files will be stored\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    py_dict_path = os.path.join(output_dir, 'py_test.csv')\n",
    "    np.savetxt(py_dict_path, array, delimiter=',', fmt='%.6f')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurement matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_DBBD_matrix(M, N):\n",
    "    \"\"\"\n",
    "    Generates a deterministic Diagonally Blocked Block Diagonal (DBBD) matrix.\n",
    "\n",
    "    A DBBD matrix is a type of block diagonal matrix where each block is a square diagonal matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    M : int\n",
    "        Number of rows in the matrix.\n",
    "    N : int\n",
    "        Number of columns in the matrix. Should be a multiple of M.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A : numpy.ndarray\n",
    "        The generated DBBD matrix of shape (M, N).\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If `N` is not a multiple of `M`.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> generate_DBDD_matrix(3, 9)\n",
    "    array([[1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
    "           [0., 0., 0., 1., 1., 1., 0., 0., 0.],\n",
    "           [0., 0., 0., 0., 0., 0., 1., 1., 1.]])\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    if N % M != 0:\n",
    "        raise ValueError(\"N should be a multiple of M.\")\n",
    "    \n",
    "    Phi = np.zeros((M, N))\n",
    "    m = N // M\n",
    "    \n",
    "    for i in range(M):\n",
    "        Phi[i, i*m:(i+1)*m] = 1\n",
    "\n",
    "    return Phi\n",
    "\n",
    "\n",
    "def generate_random_matrix(M, N, matrix_type='gaussian'):\n",
    "    \"\"\"\n",
    "    Generates a random matrix based on the specified type.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    M : int\n",
    "        Number of rows in the matrix.\n",
    "    N : int\n",
    "        Number of columns in the matrix.\n",
    "    matrix_type : str, optional (default='gaussian')\n",
    "        The type of random matrix to generate. Options are:\n",
    "        - 'gaussian': A matrix with entries drawn from a normal distribution scaled by 1/M.\n",
    "        - 'scaled_binary': A matrix with binary entries (±0.5), scaled by 1/sqrt(M).\n",
    "        - 'unscaled_binary': A matrix with binary entries (±1), with no scaling.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A : numpy.ndarray\n",
    "        The generated random matrix of shape (M, N).\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If `matrix_type` is not one of the supported types.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> generate_random_matrix(2, 3, matrix_type='gaussian')\n",
    "    array([[ 0.01, -0.02,  0.03],\n",
    "           [-0.04,  0.05, -0.06]])\n",
    "\n",
    "    >>> generate_random_matrix(2, 3, matrix_type='scaled_binary')\n",
    "    array([[-0.5,  0. , -0.5],\n",
    "           [ 0.5, -0.5,  0. ]])\n",
    "    \n",
    "    >>> generate_random_matrix(2, 3, matrix_type='unscaled_binary')\n",
    "    array([[ 1., -1.,  1.],\n",
    "           [-1.,  1., -1.]])\n",
    "    \"\"\"\n",
    "    if matrix_type == 'gaussian':\n",
    "        A = ((1/M)**2) * np.random.randn(M, N)\n",
    "\n",
    "    elif matrix_type == 'scaled_binary':\n",
    "        A = np.random.binomial(1, 0.5, size=(M, N)) - 0.5\n",
    "        A = (1/np.sqrt(M)) * A\n",
    "\n",
    "    elif matrix_type == 'unscaled_binary':\n",
    "        A = np.random.binomial(1, 0.5, size=(M, N)) * 2 - 1\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported matrix type. Choose either 'gaussian', 'scaled_binary', or 'unscaled_binary'.\")\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script provides utility functions for generating a Discrete Cosine Transform (DCT) \n",
    "orthonormal basis matrix and for testing various properties of matrices such as independence \n",
    "of columns, normalization, and coherence.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import scipy.fftpack as fftpack\n",
    "import pywt\n",
    "\n",
    "def dct_dictionary(N):\n",
    "    \"\"\"\n",
    "    Generates a Discrete Cosine Transform (DCT) orthonormal basis matrix.\n",
    "\n",
    "    The DCT basis is commonly used in signal processing and data compression. \n",
    "    It transforms a signal into a sum of cosine functions oscillating at different frequencies. \n",
    "    The resulting matrix can be used for orthogonal transformations of signals.\n",
    "    \n",
    "    DCT basis is sparifying.\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    N : int\n",
    "        The size of the dictionary (i.e., the length of the signal).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict_matrix : numpy.ndarray\n",
    "        The generated DCT dictionary matrix of shape (N, N), where each column represents \n",
    "        a DCT basis vector.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> generate_dct_dictionary(4)\n",
    "    array([[ 0.5       ,  0.5       ,  0.5       ,  0.5       ],\n",
    "           [ 0.65328148,  0.27059805, -0.27059805, -0.65328148],\n",
    "           [ 0.5       , -0.5       , -0.5       ,  0.5       ],\n",
    "           [ 0.27059805, -0.65328148,  0.65328148, -0.27059805]])\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate a DCT basis dictionary\n",
    "    dict_matrix = fftpack.dct(np.eye(N), norm='ortho')    \n",
    "    return dict_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive dictionary learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Orthogonal Matching Pursuit (OMP) algorithm for sparse coding.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def OMP(dictio, sig, max_coeff):\n",
    "    \"\"\"\n",
    "    Orthogonal Matching Pursuit (OMP) algorithm for sparse coding.\n",
    "\n",
    "    This function implements the OMP algorithm, which is used to find the sparse\n",
    "    representation of a signal over a given dictionary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dictio : numpy.ndarray\n",
    "        The dictionary to use for sparse coding. It should be a matrix of size (n x K), \n",
    "        where n is the signal dimension and K is the number of atoms in the dictionary.\n",
    "        (its columns MUST be normalized).\n",
    "    \n",
    "    sig : numpy.ndarray\n",
    "        The signals to represent using the dictionary. \n",
    "        It should be a matrix of size (n x N), where N is the number of signals.\n",
    "    \n",
    "    max_coeff : int\n",
    "        The maximum number of coefficients to use for representing each signal.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    s : numpy.ndarray\n",
    "        The sparse representation of the signals over the dictionary.\n",
    "        It should be a matrix of size (K x N).\n",
    "    \"\"\"\n",
    "\n",
    "    [n, p] = sig.shape\n",
    "    [_, key] = dictio.shape\n",
    "    s = np.zeros((key, p))\n",
    "    for k in range(p):\n",
    "        x = sig[:, k]\n",
    "        residual = x.copy()\n",
    "        indx = np.array([], dtype=int)\n",
    "        current_atoms = np.empty((n, 0))\n",
    "        norm_x = np.linalg.norm(x)\n",
    "        for j in range(max_coeff):\n",
    "            proj = dictio.T @ residual\n",
    "            pos = np.argmax(np.abs(proj))\n",
    "            indx = np.append(indx, pos)\n",
    "            # Update selected atoms matrix\n",
    "            current_atoms = np.column_stack((current_atoms, dictio[:, pos]))\n",
    "            # Solve least squares problem using QR decomposition for stability\n",
    "            q, r = np.linalg.qr(current_atoms)\n",
    "            a = np.linalg.solve(r, q.T @ x)\n",
    "            residual = x - current_atoms @ a\n",
    "            # Break if norm of residual is suff small (relative to original signal)\n",
    "            if np.linalg.norm(residual) < 1e-6 * norm_x:\n",
    "                break\n",
    "        temp = np.zeros((key,))\n",
    "        temp[indx] = a\n",
    "        s[:, k] = temp\n",
    "\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MOD (Method of Optimal Directions) algorithm for dictionary learning with improved numerical stability.\n",
    "\"\"\"\n",
    "\n",
    "# system imports\n",
    "import os\n",
    "\n",
    "# third party imports\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.linalg import solve\n",
    "\n",
    "\n",
    "\n",
    "def I_findDistanceBetweenDictionaries(original, new):\n",
    "    \"\"\"\n",
    "    Calculates the distance between two dictionaries.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    original : numpy.ndarray\n",
    "        The original dictionary.\n",
    "\n",
    "    new : numpy.ndarray\n",
    "        The new dictionary.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    catchCounter : int\n",
    "        The number of elements that satisfy the condition errorOfElement < 0.01.\n",
    "    totalDistances : float\n",
    "        The sum of all errorOfElement values.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # first: all the columns in the original start with positive values\n",
    "    catchCounter = 0\n",
    "    totalDistances = 0\n",
    "\n",
    "    for i in range(new.shape[1]):\n",
    "        new[:,i] = np.sign(new[0,i]) * new[:,i]\n",
    "\n",
    "    for i in range(original.shape[1]):\n",
    "        d = np.sign(original[0,i]) * original[:,i]\n",
    "        distances = np.sum(new - np.tile(d, (1, new.shape[1])), axis=0)\n",
    "        index = np.argmin(distances)\n",
    "        errorOfElement = 1 - np.abs(new[:,index].T @ d)\n",
    "        totalDistances += errorOfElement\n",
    "        catchCounter += errorOfElement < 0.01\n",
    "\n",
    "    ratio = catchCounter / original.shape[1]\n",
    "    return ratio, totalDistances\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def MOD(data, parameters):\n",
    "    \"\"\"\n",
    "    Method of Optimal Directions (MOD) algorithm for dictionary learning .\n",
    "\n",
    "    The MOD algorithm is a method for learning a dictionary for sparse representation of signals.\n",
    "    It iteratively updates the dictionary to best represent the input data with sparse coefficients\n",
    "    using the Orthogonal Matching Pursuit (OMP) algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : numpy.ndarray\n",
    "        An (n x N) matrix containing N signals, each of dimension n.\n",
    "    \n",
    "    parameters : dict\n",
    "        A dictionary containing the parameters for the MOD algorithm:\n",
    "            - K : int\n",
    "                The number of dictionary elements (columns) to train.\n",
    "            \n",
    "            - num_iterations : int\n",
    "                The number of iterations to perform for dictionary learning.\n",
    "            \n",
    "            - initialization_method : str\n",
    "                Method to initialize the dictionary. Options are:\n",
    "                * 'DataElements' - Initializes the dictionary using the first K data signals.\n",
    "                * 'GivenMatrix' - Initializes the dictionary using a provided matrix \n",
    "                  (requires 'initial_dictionary' key).\n",
    "\n",
    "            - initial_dictionary : numpy.ndarray, optional\n",
    "                The initial dictionary matrix to use if 'initialization_method' is \n",
    "                set to 'GivenMatrix'. It should be of size (n x K).\n",
    "\n",
    "            - L : int\n",
    "                The number of non-zero coefficients to use in OMP for sparse\n",
    "                representation of each signal.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dictionary : numpy.ndarray\n",
    "        The trained dictionary of size (n x K), where each column is a dictionary element.\n",
    "\n",
    "    coef_matrix : numpy.ndarray\n",
    "        The coefficient matrix of size (K x N), representing the sparse representation\n",
    "        of the input data using the trained dictionary.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the number of signals is smaller than the dictionary size\n",
    "    if data.shape[1] < parameters['K']:\n",
    "        print(\"MOD: number of training signals is smaller than the dictionary size. Returning trivial solution...\")\n",
    "        dictionary = data[:, :data.shape[1]]\n",
    "        coef_matrix = np.eye(data.shape[1])  # Trivial coefficients\n",
    "        return dictionary, coef_matrix\n",
    "\n",
    "    # Initialize dictionary based on the specified method\n",
    "    if parameters['initialization_method'] == 'DataElements':\n",
    "        dictionary = data[:, :parameters['K']]\n",
    "    elif parameters['initialization_method'] == 'GivenMatrix':\n",
    "        if 'initial_dictionary' not in parameters:\n",
    "            raise ValueError(\"initial_dictionary parameter is required when \"\n",
    "                             \"initialization_method is set to 'GivenMatrix'.\")\n",
    "        dictionary = parameters['initial_dictionary']\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Invalid value for initialization_method. Choose 'DataElements' or 'GivenMatrix'.\")\n",
    "\n",
    "    # Convert to float64 for precision\n",
    "    dictionary = dictionary.astype(np.float64)\n",
    "\n",
    "    # Normalize dictionary columns and avoid division by zero\n",
    "    column_norms = np.linalg.norm(dictionary, axis=0)\n",
    "    column_norms[column_norms < 1e-10] = 1  # Prevent division by zero\n",
    "    dictionary /= column_norms\n",
    "\n",
    "    # Ensure positive first elements\n",
    "    dictionary *= np.sign(dictionary[0, :])\n",
    "\n",
    "    prev_dictionary = dictionary.copy()\n",
    "\n",
    "    # Run MOD algorithm\n",
    "    for iter_num in range(parameters['num_iterations']):\n",
    "        # Step 1: Sparse coding using OMP\n",
    "        coef_matrix = OMP(dictionary, data, parameters['L'])\n",
    "\n",
    "        # Step 2: Update the dictionary\n",
    "        regularization_term = 1e-7 * sp.eye(coef_matrix.shape[0])\n",
    "        matrix_a = coef_matrix @ coef_matrix.T + regularization_term.toarray()\n",
    "\n",
    "        # Use pinv for numerical stability: \"lstsq\" or \"regularizations\" could also work\n",
    "        dictionary = data @ coef_matrix.T @ np.linalg.pinv(matrix_a)\n",
    "\n",
    "\n",
    "        # Normalize dictionary columns and avoid division by zero\n",
    "        column_norms = np.linalg.norm(dictionary, axis=0)\n",
    "        column_norms[column_norms < 1e-10] = 1  # Prevent division by zero\n",
    "        dictionary /= column_norms\n",
    "\n",
    "        # Ensure positive first elements\n",
    "        dictionary *= np.sign(dictionary[0, :])\n",
    "\n",
    "        # Convergence check\n",
    "        if np.linalg.norm(dictionary - prev_dictionary) < 1e-5:\n",
    "            print(f\"MOD converged after {iter_num + 1} iterations.\")\n",
    "            break\n",
    "\n",
    "        prev_dictionary = dictionary.copy()\n",
    "\n",
    "    return dictionary, coef_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "K-SVD algorithm for dictionary learning and sparse coding using Orthogonal Matching Pursuit (OMP).\n",
    "Includes functions for updating dictionary elements, handling singular value decomposition (SVD)\n",
    "for vectors, and clearing redundant dictionary elements.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "\n",
    "def svds_vector(v):\n",
    "    \"\"\"\n",
    "    Handle SVD for a vector or a 2D matrix with one dimension equal to 1.\n",
    "    \"\"\"\n",
    "    v = np.asarray(v)\n",
    "    \n",
    "    if v.ndim == 1:\n",
    "        v = v.reshape(-1, 1)\n",
    "    elif v.ndim == 2 and (v.shape[0] == 1 or v.shape[1] == 1):\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a vector or a 2D array with one dimension equal to 1.\")\n",
    "    \n",
    "    s = np.linalg.norm(v)\n",
    "    if s > 0:\n",
    "        u = v / s\n",
    "    else:\n",
    "        u = np.zeros_like(v)\n",
    "    \n",
    "    vt = np.array([[1]])\n",
    "\n",
    "    return u, s, vt\n",
    "\n",
    "def I_findBetterDictionaryElement(data, dictionary, j, coeff_matrix, numCoefUsed=1):\n",
    "    \"\"\"\n",
    "    Update the j-th dictionary element.\n",
    "    \"\"\"\n",
    "    relevantDataIndices = np.nonzero(coeff_matrix[j, :])[0]\n",
    "    if relevantDataIndices.size == 0:\n",
    "        errorMat = data - dictionary @ coeff_matrix\n",
    "        errorNormVec = np.sum(errorMat ** 2, axis=0)\n",
    "        i = np.argmax(errorNormVec)\n",
    "        betterDictionaryElement = data[:, i] / np.linalg.norm(data[:, i])\n",
    "        betterDictionaryElement *= np.sign(betterDictionaryElement[0])\n",
    "        coeff_matrix[j, :] = 0\n",
    "        newVectAdded = 1\n",
    "        return betterDictionaryElement, coeff_matrix, newVectAdded\n",
    "    \n",
    "    newVectAdded = 0\n",
    "    tmpCoefMatrix = coeff_matrix[:, relevantDataIndices]\n",
    "    tmpCoefMatrix[j, :] = 0\n",
    "    errors = data[:, relevantDataIndices] - dictionary @ tmpCoefMatrix\n",
    "\n",
    "    if np.min(errors.shape) <= 1:\n",
    "        u, s, vt = svds_vector(errors)\n",
    "        betterDictionaryElement = u\n",
    "        singularValue = s\n",
    "        betaVector = vt\n",
    "    else:\n",
    "        u, s, vt = svds(errors, k=1)\n",
    "        betterDictionaryElement = u[:, 0]\n",
    "        singularValue = s[0]\n",
    "        betaVector = vt[0, :]\n",
    "\n",
    "    coeff_matrix[j, relevantDataIndices] = singularValue * betaVector.T\n",
    "\n",
    "    return betterDictionaryElement, coeff_matrix, newVectAdded\n",
    "\n",
    "def I_clearDictionary(dictionary, coeff_matrix, data):\n",
    "    \"\"\"\n",
    "    Clear or replace redundant dictionary elements.\n",
    "    \"\"\"\n",
    "    T2 = 0.99\n",
    "    T1 = 3\n",
    "    K = dictionary.shape[1]\n",
    "    Er = np.sum((data - dictionary @ coeff_matrix) ** 2, axis=0)\n",
    "    G = dictionary.T @ dictionary\n",
    "    G -= np.diag(np.diag(G))\n",
    "    for jj in range(K):\n",
    "        if np.max(G[jj, :]) > T2 or np.count_nonzero(np.abs(coeff_matrix[jj, :]) > 1e-7) <= T1:\n",
    "            pos = np.argmax(Er)\n",
    "            Er[pos] = 0\n",
    "            dictionary[:, jj] = data[:, pos] / np.linalg.norm(data[:, pos])\n",
    "            G = dictionary.T @ dictionary\n",
    "            G -= np.diag(np.diag(G))\n",
    "    return dictionary\n",
    "\n",
    "def KSVD(data, param):\n",
    "    \"\"\"\n",
    "    K-SVD algorithm for dictionary learning.\n",
    "    \"\"\"\n",
    "    if param['preserve_dc_atom'] > 0:\n",
    "        fixedDictElem = np.zeros((data.shape[0], 1))  \n",
    "        fixedDictElem[:data.shape[0], 0] = 1 / np.sqrt(data.shape[0])\n",
    "    else:\n",
    "        fixedDictElem = np.empty((0, 0))\n",
    "\n",
    "    if data.shape[1] < param['K']:\n",
    "        print('KSVD: number of training data is smaller than the dictionary size. Trivial solution...')\n",
    "        dictionary = data[:, :data.shape[1]]\n",
    "        coef_matrix = np.eye(data.shape[1])\n",
    "        return dictionary, coef_matrix\n",
    "    \n",
    "    dictionary = np.zeros((data.shape[0], param['K']), dtype=np.float64)    \n",
    "    if param['initialization_method'] == 'DataElements':\n",
    "        dictionary[:, :param['K'] - param['preserve_dc_atom']] = \\\n",
    "            data[:, :param['K'] - param['preserve_dc_atom']]\n",
    "    elif param['initialization_method'] == 'GivenMatrix':\n",
    "        dictionary[:, :param['K'] - param['preserve_dc_atom']] = \\\n",
    "            param['initial_dictionary'][:, :param['K'] - param['preserve_dc_atom']]\n",
    "\n",
    "    if param['preserve_dc_atom']:\n",
    "        tmpMat = np.linalg.lstsq(dictionary + 1e-7 * np.eye(dictionary.shape[1]), fixedDictElem, rcond=None)[0]\n",
    "        dictionary -= fixedDictElem @ tmpMat\n",
    "\n",
    "    column_norms = np.sqrt(np.sum(dictionary ** 2, axis=0))\n",
    "    column_norms[column_norms < 1e-10] = 1\n",
    "    dictionary /= column_norms\n",
    "    dictionary *= np.sign(dictionary[0, :])\n",
    "\n",
    "    for iterNum in range(param['num_iterations']):\n",
    "        coef_matrix = OMP(\n",
    "            np.hstack((fixedDictElem, dictionary)) if fixedDictElem.size > 0 else dictionary,\n",
    "            data,\n",
    "            param['L']\n",
    "        )\n",
    "        \n",
    "        rand_perm = np.random.permutation(dictionary.shape[1])\n",
    "        for j in rand_perm:\n",
    "            betterDictElem, coef_matrix, newVectAdded = I_findBetterDictionaryElement(\n",
    "                data,\n",
    "                np.hstack((fixedDictElem, dictionary)) if fixedDictElem.size > 0 else dictionary,\n",
    "                j + fixedDictElem.shape[1],\n",
    "                coef_matrix,\n",
    "                param['L']\n",
    "            )\n",
    "\n",
    "            dictionary[:, j] = betterDictElem.ravel()\n",
    "            if param['preserve_dc_atom']:\n",
    "                tmpCoeff = np.linalg.lstsq(betterDictElem + 1e-7, fixedDictElem, rcond=None)[0]\n",
    "                dictionary[:, j] -= fixedDictElem @ tmpCoeff\n",
    "                dictionary[:, j] /= np.linalg.norm(dictionary[:, j])\n",
    "\n",
    "        dictionary = I_clearDictionary(dictionary, coef_matrix[fixedDictElem.shape[1]:, :], data)\n",
    "\n",
    "    dictionary = np.hstack((fixedDictElem, dictionary)) if fixedDictElem.size > 0 else dictionary\n",
    "    \n",
    "    return dictionary, coef_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## ------------------------------------------------------------------------------------------------\n",
    "## REST OF THE FUNCTIONS ARE FOR TESTING PURPOSES\n",
    "## ------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "def compute_independent_columns(A, tol=1e-10):\n",
    "    \"\"\"\n",
    "    Computes the independent columns of a matrix using the QR decomposition.\n",
    "\n",
    "    The function identifies independent columns of a given matrix `A` by performing a QR \n",
    "    decomposition. It selects columns corresponding to non-zero diagonal elements of the \n",
    "    `R` matrix, which are considered linearly independent.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : numpy.ndarray\n",
    "        The matrix for which to compute the independent columns.\n",
    "    tol : float, optional (default=1e-10)\n",
    "        The tolerance value for considering diagonal elements of `R` as non-zero.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ind_cols : numpy.ndarray\n",
    "        A matrix containing the independent columns of `A`.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The QR decomposition is used to determine the rank of the matrix `A`.\n",
    "    - Columns corresponding to non-zero diagonal elements of the `R` matrix are considered independent.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "    >>> compute_independent_columns(A)\n",
    "    array([[1, 2],\n",
    "           [4, 5],\n",
    "           [7, 8]])\n",
    "    \"\"\"\n",
    "    # Perform the QR decomposition\n",
    "    Q, R = np.linalg.qr(A)\n",
    "\n",
    "    # Find the independent columns based on the rank of R\n",
    "    rank = np.sum(np.abs(np.diagonal(R)) > tol)\n",
    "    ind_cols = A[:, :rank]\n",
    "\n",
    "    return ind_cols\n",
    "\n",
    "def check_normalization(A):\n",
    "    \"\"\"\n",
    "    Checks if the columns of a matrix are normalized (i.e., each column has a unit norm).\n",
    "\n",
    "    The function calculates the norm of each column in the matrix `A` and checks if all \n",
    "    column norms are close to 1.0, which indicates normalization.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : numpy.ndarray\n",
    "        The matrix to check for normalization.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    is_normalized : bool\n",
    "        True if all columns of `A` are normalized, False otherwise.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> A = np.array([[1, 0], [0, 1]])\n",
    "    >>> check_normalization(A)\n",
    "    True\n",
    "    \"\"\"\n",
    "    column_norms = np.linalg.norm(A, axis=0)\n",
    "    is_normalized = np.allclose(column_norms, 1.0)\n",
    "    return is_normalized\n",
    "\n",
    "\n",
    "def compute_coherence(matrix):\n",
    "    \"\"\"\n",
    "    Computes the coherence of the given matrix.\n",
    "\n",
    "    Coherence is a measure of the maximum correlation between any two columns of a matrix. \n",
    "    It is useful in various applications, such as signal processing and compressed sensing, \n",
    "    to assess the degree of similarity between different columns of the matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : numpy.ndarray\n",
    "        An N x M matrix where coherence is to be calculated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    coherence : float\n",
    "        The coherence of the matrix, defined as the maximum absolute value of the off-diagonal \n",
    "        elements in the Gram matrix of the column-normalized input matrix.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> matrix = np.array([[1, 0], [0, 1]])\n",
    "    >>> compute_coherence(matrix)\n",
    "    0.0\n",
    "    \"\"\"\n",
    "    # Normalize the columns of the matrix\n",
    "    normalized_matrix = matrix / np.linalg.norm(matrix, axis=0, keepdims=True)\n",
    "    \n",
    "    # Compute the Gram matrix (inner products between all pairs of columns)\n",
    "    gram_matrix = np.dot(normalized_matrix.T, normalized_matrix)\n",
    "    \n",
    "    # Remove the diagonal elements (which are all 1's) to only consider distinct columns\n",
    "    np.fill_diagonal(gram_matrix, 0)\n",
    "    \n",
    "    # Compute the coherence as the maximum absolute value of the off-diagonal elements\n",
    "    coherence = np.max(np.abs(gram_matrix))\n",
    "    \n",
    "    return coherence\n",
    "\n",
    "\n",
    "def check_matrix_properties(A):\n",
    "    \"\"\"\n",
    "    Checks various properties of a matrix.\n",
    "\n",
    "    The function checks if the matrix `A` is full rank, if its columns and rows are normalized,\n",
    "    and computes the coherence of the matrix. It also prints the results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    A : numpy.ndarray\n",
    "        The matrix to check.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> A = np.array([[1, 2], [3, 4]])\n",
    "    >>> check_matrix_properties(A)\n",
    "    \"\"\"\n",
    "    # Check if the matrix is full rank\n",
    "    is_full_rank = np.linalg.matrix_rank(A) == min(A.shape)\n",
    "\n",
    "    # Check if the columns are normalized\n",
    "    is_columns_normalized = check_normalization(A)\n",
    "\n",
    "    # Check if the rows are normalized\n",
    "    is_rows_normalized = check_normalization(A.T)\n",
    "\n",
    "    # Compute the coherence of the matrix\n",
    "    coherence = compute_coherence(A)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Is full rank:\", is_full_rank)\n",
    "    print(\"Are columns normalized:\", is_columns_normalized)\n",
    "    print(\"Are rows normalized:\", is_rows_normalized)\n",
    "    print(\"Coherence:\", coherence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recovery Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SL0 Algorithm Implementation\n",
    "\n",
    "This file contains an implementation of the Smoothed L0 (SL0) algorithm for sparse signal recovery.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def SL0(y, A, sigma_min, sigma_decrease_factor=0.5, mu_0=2, L=3, A_pinv=None, showProgress=False):\n",
    "    \"\"\"\n",
    "    Returns the sparsest vector `s` that satisfies the underdetermined system of \n",
    "    linear equations `A @ s = y`, using the Smoothed L0 (SL0) algorithm.\n",
    "\n",
    "    Requires:\n",
    "    --------\n",
    "    - numpy as np\n",
    "    \n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    y : numpy array\n",
    "        The observed vector (Mx1), where M is the number of rows in `A`.\n",
    "    \n",
    "    A : numpy array\n",
    "        The measurement matrix (MxN), which should be 'wide', meaning it has more \n",
    "        columns than rows (N > M). The number of rows in `A` must match the length \n",
    "        of `y`.\n",
    "    \n",
    "    sigma_min : float\n",
    "        The minimum value of `sigma`, which determines the stopping criterion for \n",
    "        the algorithm. It should be chosen based on the noise level or desired \n",
    "        accuracy.\n",
    "    \n",
    "    sigma_decrease_factor : float, optional (default=0.5)\n",
    "        The factor by which `sigma` is decreased in each iteration. This should be \n",
    "        a positive value less than 1. Smaller values lead to quicker reduction of \n",
    "        `sigma`, possibly at the cost of accuracy for less sparse signals.\n",
    "    \n",
    "    mu_0 : float, optional (default=2)\n",
    "        The scaling factor for `mu`, where `mu = mu_0 * sigma^2`. This parameter \n",
    "        influences the convergence rate of the algorithm.\n",
    "    \n",
    "    L : int, optional (default=3)\n",
    "        The number of iterations for the inner loop (steepest descent). Increasing \n",
    "        `L` can improve the precision of the result but also increases computational \n",
    "        cost.\n",
    "    \n",
    "    A_pinv : numpy array, optional\n",
    "        The precomputed pseudoinverse of the matrix `A`. If not provided, it will be \n",
    "        calculated within the function as `np.linalg.pinv(A)`. Providing this value \n",
    "        is beneficial if the function is called repeatedly with the same `A`.\n",
    "    \n",
    "    showProgress : bool, optional (default=False)\n",
    "        If `True`, the function prints the current value of `sigma` during each \n",
    "        iteration, which helps monitor the convergence process.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    s : numpy array\n",
    "        The estimated sparse signal (Nx1) that best satisfies the equation `A @ s = y`.\n",
    "\n",
    "    Notes:\n",
    "    -----\n",
    "    - The algorithm works by iteratively reducing `sigma` in a geometric sequence, \n",
    "      starting with `sigma = 2 * max(abs(s))` and ending with `sigma_min`. At each \n",
    "      step, the function adjusts `s` to minimize the L0-norm by smoothing it using \n",
    "      a Gaussian kernel.\n",
    "    \n",
    "    - The choice of `sigma_min` is crucial: for noiseless cases, a smaller `sigma_min` \n",
    "      yields a sparser solution; for noisy cases, `sigma_min` should be a few times \n",
    "      the standard deviation of the noise in `s`.\n",
    "\n",
    "    - If `A_pinv` is precomputed and passed as an argument, the function becomes \n",
    "      more efficient, especially in scenarios where it is called repeatedly with the \n",
    "      same `A`.\n",
    "\n",
    "      \n",
    "      References:\n",
    "      ----------\n",
    "    - Original authors (MATLAB): Massoud Babaie-Zadeh, Hossein Mohimani, 4 August 2008.\n",
    "    - Web-page: http://ee.sharif.ir/~SLzero\n",
    "\n",
    "    - Ported to python: RosNaviGator https://github.com/RosNaviGator, 2024\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "    if A_pinv is None:\n",
    "        A_pinv = np.linalg.pinv(A)\n",
    "        \n",
    "    # Initialize the variables\n",
    "    s = A_pinv @ y\n",
    "    sigma = 2 * max(np.abs(s))\n",
    "\n",
    "    # Define lambda function for delta\n",
    "    OurDelta = lambda s, sigma: s * np.exp(-s**2 / sigma**2)\n",
    " \n",
    "    # Main loop\n",
    "    while sigma > sigma_min:\n",
    "        for i in range(L):\n",
    "            delta = OurDelta(s, sigma)\n",
    "            s = s - mu_0 * delta\n",
    "            s = s - A_pinv @ (A @ s - y)\n",
    "        \n",
    "        if showProgress:\n",
    "            print(f'sigma: {sigma}')\n",
    "\n",
    "        sigma = sigma * sigma_decrease_factor\n",
    "\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script provides functions to calculate the Signal-to-Noise Ratio (SNR) between \n",
    "an original and a reconstructed signal, and to plot these signals together, displaying \n",
    "the SNR. It also includes functionality to save the plotted signals to a specified directory.\n",
    "\"\"\"\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def calculate_snr(signal, recovered_signal):\n",
    "    \"\"\"\n",
    "    Calculates the Signal-to-Noise Ratio (SNR) between the original signal and the recovered signal.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    signal : numpy.ndarray\n",
    "        The original signal.\n",
    "    recovered_signal : numpy.ndarray\n",
    "        The recovered signal after some processing or recovery algorithm.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    snr : float\n",
    "        The Signal-to-Noise Ratio (SNR) in decibels (dB).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The SNR is calculated as 20 * log10(norm(original_signal) / norm(original_signal - recovered_signal)).\n",
    "    - A higher SNR value indicates a better recovery, with less error relative to the original signal.\n",
    "    \"\"\"\n",
    "    error = recovered_signal - signal\n",
    "    snr = 20 * np.log10(np.linalg.norm(signal) / np.linalg.norm(error))\n",
    "    \n",
    "    return snr\n",
    "\n",
    "\n",
    "\n",
    "def plot_signals(original_signal, reconstructed_signal, snr=None, original_name=\"Original Signal\", \n",
    "                 reconstructed_name=\"Reconstructed Signal\", save_path=None, filename=None,\n",
    "                 start_pct=0.0, num_samples=None):\n",
    "    \"\"\"\n",
    "    Plots a section of the original signal and the reconstructed signal on the same plot with the given names,\n",
    "    displays the Signal-to-Noise Ratio (SNR) in a text box, and saves the plot to a specified directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    original_signal : numpy.ndarray\n",
    "        The original signal to be plotted.\n",
    "    \n",
    "    reconstructed_signal : numpy.ndarray\n",
    "        The reconstructed signal to be plotted.\n",
    "    \n",
    "    reconstructed_name : str, optional (default=\"Reconstructed Signal\")\n",
    "        The name to display for the reconstructed signal in the plot.\n",
    "    \n",
    "    save_path : str, optional\n",
    "        The directory path where the plot should be saved. If None, the plot will not be saved.\n",
    "    \n",
    "    filename : str, optional\n",
    "        The name of the file to save the plot as. If None and save_path is provided, a default name will be generated.\n",
    "    \n",
    "    snr : float, optional (default=None)\n",
    "        The Signal-to-Noise Ratio to display. If None, it will be computed using the original and reconstructed signals.\n",
    "    \n",
    "    start_pct : float, optional (default=0.0)\n",
    "        The percentage (between 0 and 1) of the way through the signal to start plotting. For example, 0.5 means start \n",
    "        from the halfway point of the signals.\n",
    "    \n",
    "    num_samples : int, optional (default=None)\n",
    "        The number of samples to plot from the start point. If None, it will plot to the end of the signals.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure the signals have the same length\n",
    "    if len(original_signal) != len(reconstructed_signal):\n",
    "        raise ValueError(\"The original signal and the reconstructed signal must have the same length.\")\n",
    "    \n",
    "    # Calculate the start index based on percentage\n",
    "    start_idx = int(start_pct * len(original_signal))\n",
    "    \n",
    "    # Determine the end index based on num_samples\n",
    "    if num_samples is not None:\n",
    "        end_idx = start_idx + num_samples\n",
    "    else:\n",
    "        end_idx = len(original_signal)\n",
    "    \n",
    "    # Check if the end index exceeds the signal length\n",
    "    if end_idx > len(original_signal):\n",
    "        raise ValueError(f\"You tried to plot from sample {start_idx} to sample {end_idx}, \"\n",
    "                         f\"but the signal only has {len(original_signal)} samples!\")\n",
    "    \n",
    "    # Slice the signals to the selected section\n",
    "    original_signal_section = original_signal[start_idx:end_idx]\n",
    "    reconstructed_signal_section = reconstructed_signal[start_idx:end_idx]\n",
    "    \n",
    "    # Calculate SNR if not provided\n",
    "    if snr is None:\n",
    "        snr = calculate_snr(original_signal_section, reconstructed_signal_section)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(original_signal_section, label=original_name, color='blue', linewidth=1.5)\n",
    "    plt.plot(reconstructed_signal_section, label=reconstructed_name, color='red', linestyle='--', linewidth=1.5)\n",
    "    \n",
    "    # Title and labels\n",
    "    plt.title(f\"{original_name} vs {reconstructed_name} (Section: {start_pct*100:.1f}% - {num_samples} samples)\")\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Amplitude')\n",
    "    \n",
    "    # Add a legend in the upper-right corner with a white background\n",
    "    plt.legend(loc='upper right', frameon=True, facecolor='white')\n",
    "    \n",
    "    # Display SNR in a text box in the top-left corner with a white background\n",
    "    plt.text(0.05, 0.95, f'SNR: {snr:.2f} dB', transform=plt.gca().transAxes,\n",
    "            fontsize=12, verticalalignment='top', horizontalalignment='left',\n",
    "            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "    \n",
    "    # Grid and show plot\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Save the plot if a save path is provided\n",
    "    if save_path is not None:\n",
    "        # Ensure the save directory exists\n",
    "        os.makedirs(save_path, exist_ok=True)\n",
    "        \n",
    "        # Use provided filename or generate a default one\n",
    "        if filename is None:\n",
    "            filename = f\"{original_name}_vs_{reconstructed_name}_section.png\"\n",
    "        \n",
    "        # Define the file path to save the plot\n",
    "        file_path = os.path.join(save_path, filename)\n",
    "        plt.savefig(file_path)\n",
    "        print(f\"Plot saved to {file_path}\")\n",
    "    \n",
    "    # Display the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressed Sensing class (higher level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "\n",
    "class compressedSensing:\n",
    "    def __init__(self, signal, BLOCK_LEN=16, CR=4, matrix_type='gaussian'):\n",
    "        \"\"\"\n",
    "        Constructor for the compressedSensingNormal class.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        signal : numpy.ndarray\n",
    "            The input signal, must be a valid array of real numbers.\n",
    "        BLOCK_LEN : int\n",
    "            The number of rows in the measurement matrix Phi. Default is 16.\n",
    "        CR : int, optional (default=4)\n",
    "            Compression ratio (controls the number of rows in Phi). Must be a positive integer and BLOCK_LEN / CR > 1.\n",
    "        matrix_type : str, optional (default='gaussian')\n",
    "            Type of the matrix to generate ('gaussian', 'DBBD', etc.).\n",
    "        \"\"\"\n",
    "        # Check if signal is valid\n",
    "        if signal is None:\n",
    "            raise ValueError(\"A signal must be provided.\")\n",
    "        \n",
    "        # Ensure signal is a vector-like structure (array or list of real numbers)\n",
    "        if not (isinstance(signal, (list, np.ndarray)) and np.issubdtype(np.array(signal).dtype, np.number)):\n",
    "            raise ValueError(\"The signal must be a valid array or list of numerical values.\")\n",
    "        \n",
    "        self.signal = np.array(signal)  # Convert to numpy array if it isn't already\n",
    "\n",
    "        # Check that BLOCK_LEN and CR are valid\n",
    "        if not isinstance(BLOCK_LEN, int) or BLOCK_LEN <= 0:\n",
    "            raise ValueError(\"BLOCK_LEN must be a positive integer.\")\n",
    "        if not isinstance(CR, int) or CR <= 2:\n",
    "            raise ValueError(\"CR must be a positive integer greater than 2.\")\n",
    "        if BLOCK_LEN % CR != 0 or BLOCK_LEN // CR <= 1:\n",
    "            raise ValueError(\"BLOCK_LEN must be divisible by CR, and BLOCK_LEN / CR must be greater than 1.\")\n",
    "        \n",
    "        self.BLOCK_LEN = BLOCK_LEN\n",
    "        self.CR = CR\n",
    "        self.COMP_LEN = BLOCK_LEN // CR  # Compression length (number of rows in Phi)\n",
    "\n",
    "        # Generate measurement matrix Phi based on the specified type\n",
    "        if matrix_type == 'DBBD':\n",
    "            self.Phi = generate_DBBD_matrix(self.COMP_LEN, self.BLOCK_LEN)\n",
    "        else:\n",
    "            self.Phi = generate_random_matrix(self.COMP_LEN, self.BLOCK_LEN, matrix_type=matrix_type)\n",
    "\n",
    "        # we save original phi for model output purposes in case kronecker is activated\n",
    "        self.original_phi = self.Phi\n",
    "\n",
    "        # Initialize other attributes\n",
    "        self.training_set = None\n",
    "        self.training_matrix = None\n",
    "        self.reconstructed_signal = None\n",
    "        self.Y = None  # Compressed test signal\n",
    "        self.theta = None\n",
    "        self.theta_pinv = None\n",
    "        self.coeff_matrix = None  # Coefficients from MOD or K-SVD algorithm\n",
    "        self.is_kron = False\n",
    "\n",
    "\n",
    "    def divide_signal(self, training_percentage):\n",
    "        \"\"\"\n",
    "        Divides the signal into a training set and a test set based on the given percentage.\n",
    "        \"\"\"\n",
    "        training_size = int(training_percentage * len(self.signal))\n",
    "        \n",
    "        # Calculate the time duration in hours and minutes for both training and testing sets\n",
    "        training_minutes = training_size / 360\n",
    "        testing_minutes = (len(self.signal) - training_size) / 360\n",
    "        \n",
    "        training_hours = int(training_minutes // 60)\n",
    "        training_minutes = int(training_minutes % 60)\n",
    "        \n",
    "        testing_hours = int(testing_minutes // 60)\n",
    "        testing_minutes = int(testing_minutes % 60)\n",
    "\n",
    "        # Print the duration for training and testing sets in hours and minutes\n",
    "        print(f\"Training set duration: {training_hours} hour(s) and {training_minutes} minute(s)\")\n",
    "        print(f\"Testing set duration: {testing_hours} hour(s) and {testing_minutes} minute(s)\")\n",
    "        \n",
    "        # Define the training and test sets\n",
    "        self.training_set = self.signal[:training_size]\n",
    "        self.test_set = self.signal[training_size:]\n",
    "\n",
    "        # Ensure the test set size is a multiple of BLOCK_LEN by truncating the test set\n",
    "        test_size = len(self.test_set)\n",
    "        test_size_multiple = (test_size // self.BLOCK_LEN) * self.BLOCK_LEN\n",
    "        self.test_set = self.test_set[:test_size_multiple]\n",
    "\n",
    "        # Ensure the training set size is a multiple of BLOCK_LEN\n",
    "        num_cols = training_size // self.BLOCK_LEN\n",
    "        if num_cols < self.BLOCK_LEN:\n",
    "            warnings.warn(\"The number of samples (columns) in the training matrix is shorter than \"\n",
    "                        \"the number of rows, which can cause issues with dictionary learning.\")\n",
    "\n",
    "        # Reshape the training set using Fortran-style ordering ('F')\n",
    "        self.training_matrix = self.training_set[:num_cols * self.BLOCK_LEN].reshape(self.BLOCK_LEN, num_cols, order='F')\n",
    "\n",
    "        # print training matrix shape\n",
    "        print(f\"Training matrix shape: {self.training_matrix.shape}\")\n",
    "\n",
    "\n",
    "    def compress_test_set(self):\n",
    "        \"\"\"\n",
    "        Compresses the test set using the original measurement matrix (original_phi).\n",
    "        Raises an error if the test set has already been compressed.\n",
    "        \"\"\"\n",
    "        if self.Y is not None:\n",
    "            raise RuntimeError(\"Test set has already been compressed. Recompression is not allowed.\")\n",
    "        \n",
    "        if self.test_set is None:\n",
    "            raise ValueError(\"Test set not defined. Please divide the signal before compressing.\")\n",
    "\n",
    "        M, N = self.original_phi.shape  # Use original_phi instead of Phi\n",
    "        SIGNAL_BLOCKS = len(self.test_set) // N\n",
    "        self.Y = np.zeros((M, SIGNAL_BLOCKS))\n",
    "\n",
    "        # Sampling phase: Compress signal block-wise\n",
    "        for i in range(SIGNAL_BLOCKS):\n",
    "            self.Y[:, i] = self.original_phi @ self.test_set[i * N: (i + 1) * N]  # Use original_phi here\n",
    "\n",
    "\n",
    "\n",
    "    def Y_kron(self):\n",
    "        \"\"\"\n",
    "        Reshapes the compressed signal Y into its Kronecker version by concatenating KRON_FACT consecutive columns.\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'KRON_FACT') or self.KRON_FACT is None:\n",
    "            raise ValueError(\"KRON_FACT has not been set. Please activate the Kronecker method before reshaping Y.\")\n",
    "        \n",
    "        # Generate Y_kron from Y by concatenating KRON_FACT consecutive columns\n",
    "        M, SIGNAL_BLOCKS = self.Y.shape\n",
    "        SIGNAL_BLOCKS_KRON = len(self.test_set) // self.BLOCK_LEN  # BLOCK_LEN must be already \"the kronecker one\"\n",
    "        temp_y = np.zeros((M * self.KRON_FACT, SIGNAL_BLOCKS_KRON))\n",
    "\n",
    "        for i in range(SIGNAL_BLOCKS_KRON):\n",
    "            temp_y[:, i] = self.Y[:, i * self.KRON_FACT: (i + 1) * self.KRON_FACT].flatten(order='F')\n",
    "        \n",
    "        self.Y = temp_y\n",
    "\n",
    "\n",
    "    def kronecker_activate(self, KRON_FACT):\n",
    "        \"\"\"\n",
    "        Activates Kronecker compression, adjusting BLOCK_LEN and reprocessing Phi and Y accordingly.\n",
    "        \"\"\"\n",
    "        if hasattr(self, 'is_kron') and self.is_kron:\n",
    "            raise ValueError(\"Kronecker compression has already been activated. Cannot activate again.\")\n",
    "\n",
    "        if self.Y is None:\n",
    "            raise ValueError(\"Y has not been computed. Please compress the signal before activating the kronecker method.\")\n",
    "        \n",
    "        self.KRON_FACT = KRON_FACT\n",
    "        self.BLOCK_LEN = self.BLOCK_LEN * self.KRON_FACT\n",
    "\n",
    "        # Compute Kronecker product for Phi\n",
    "        self.Phi = np.kron(np.eye(self.KRON_FACT), self.Phi)\n",
    "\n",
    "        # Reprocess the training set if it exists\n",
    "        if self.training_set is not None:\n",
    "            training_size = len(self.training_set)\n",
    "            num_cols = training_size // self.BLOCK_LEN\n",
    "            if num_cols < self.BLOCK_LEN:\n",
    "                warnings.warn(\"The number of samples (columns) in the training matrix is shorter than \"\n",
    "                            \"the number of rows, which can cause issues with dictionary learning.\")\n",
    "            \n",
    "            self.training_matrix = self.training_set[:num_cols * self.BLOCK_LEN].reshape(self.BLOCK_LEN, num_cols, order='F')\n",
    "\n",
    "            # print training matrix shape\n",
    "            print(f\"KRONECKER ACTIVATE Training matrix shape: {self.training_matrix.shape}\")\n",
    "        \n",
    "        self.Y_kron()\n",
    "\n",
    "        # Set the is_kron flag to True\n",
    "        self.is_kron = True\n",
    "\n",
    "\n",
    "    def generate_dictionary(self, dictionary_type='dct', mod_params=None, ksvd_params=None):\n",
    "        \"\"\"\n",
    "        Generates the dictionary based on the specified type. Supports DCT, MOD, and K-SVD.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        dictionary_type : str\n",
    "            The type of dictionary to generate ('dct', 'mod', 'ksvd').\n",
    "        mod_params : dict, optional\n",
    "            Dictionary of parameters for MOD algorithm if using MOD, should include `redundancy`.\n",
    "        ksvd_params : dict, optional\n",
    "            Dictionary of parameters for K-SVD algorithm if using K-SVD, should include `redundancy`.\n",
    "        \"\"\"\n",
    "        if dictionary_type == 'dct':\n",
    "            self.dictionary = dct_dictionary(self.BLOCK_LEN)\n",
    "        elif dictionary_type == 'mod':\n",
    "            if self.training_matrix is None:\n",
    "                raise ValueError(\"Training matrix not defined. Please divide the signal before running MOD.\")\n",
    "            if mod_params is None:\n",
    "                raise ValueError(\"MOD parameters not provided.\")\n",
    "            \n",
    "            # Compute 'K' using redundancy factor and BLOCK_LEN\n",
    "            mod_params['K'] = mod_params['redundancy'] * self.BLOCK_LEN\n",
    "            \n",
    "            # Run MOD algorithm with training matrix and mod_params\n",
    "            self.dictionary, self.coeff_matrix = MOD(self.training_matrix, mod_params)\n",
    "        elif dictionary_type == 'ksvd':\n",
    "            if self.training_matrix is None:\n",
    "                raise ValueError(\"Training matrix not defined. Please divide the signal before running K-SVD.\")\n",
    "            if ksvd_params is None:\n",
    "                raise ValueError(\"K-SVD parameters not provided.\")\n",
    "            \n",
    "            # Compute 'K' using redundancy factor and BLOCK_LEN\n",
    "            ksvd_params['K'] = ksvd_params['redundancy'] * self.BLOCK_LEN\n",
    "            \n",
    "            # Run K-SVD algorithm with training matrix and ksvd_params\n",
    "            self.dictionary, self.coeff_matrix = KSVD(self.training_matrix, ksvd_params)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported dictionary type. Use 'dct', 'mod', or 'ksvd'.\")\n",
    "\n",
    "    def recover_signal(self, sl0_params=None):\n",
    "        \"\"\"\n",
    "        Recovers the original signal using the SL0 algorithm, after compressing.\n",
    "        \"\"\"\n",
    "        if self.Y is None:\n",
    "            raise ValueError(\"Test set has not been compressed. Please compress the signal first.\")\n",
    "        if self.dictionary is None:\n",
    "            raise ValueError(\"Dictionary has not been generated. Please generate a dictionary before recovery.\")\n",
    "        \n",
    "        M, N = self.Phi.shape\n",
    "        SIGNAL_BLOCKS = self.Y.shape[1]\n",
    "        reconstructed_signal = np.zeros(N * SIGNAL_BLOCKS)\n",
    "\n",
    "        # Precompute theta and theta_pinv\n",
    "        self.theta = self.Phi @ self.dictionary\n",
    "        self.theta_pinv = np.linalg.pinv(self.theta)\n",
    "\n",
    "        # Set default SL0 parameters and update with user-provided values\n",
    "        default_sl0_params = {\n",
    "            'sigma_min': 1e-4,\n",
    "            'sigma_decrease_factor': 0.5,\n",
    "            'mu_0': 2,\n",
    "            'L': 3,\n",
    "            'showProgress': False\n",
    "        }\n",
    "        \n",
    "        if sl0_params is not None:\n",
    "            default_sl0_params.update(sl0_params)\n",
    "\n",
    "        # SL0 recovery for each block\n",
    "        for i in range(SIGNAL_BLOCKS):\n",
    "            y = self.Y[:, i]\n",
    "\n",
    "            # SL0: Sparse reconstruction using the parameters\n",
    "            xp = SL0(\n",
    "                y, self.theta,\n",
    "                sigma_min=default_sl0_params['sigma_min'],\n",
    "                sigma_decrease_factor=default_sl0_params['sigma_decrease_factor'],\n",
    "                mu_0=default_sl0_params['mu_0'],\n",
    "                L=default_sl0_params['L'],\n",
    "                A_pinv=self.theta_pinv,\n",
    "                showProgress=default_sl0_params['showProgress']\n",
    "            )\n",
    "\n",
    "            # Recovery Phase: Reconstruct the original signal\n",
    "            reconstructed_signal[i * N : (i + 1) * N] = self.dictionary @ xp\n",
    "\n",
    "        # Store the reconstructed signal as an attribute\n",
    "        self.reconstructed_signal = reconstructed_signal\n",
    "\n",
    "\n",
    "\n",
    "    def plot_reconstructed_vs_original(self, save_path=None, filename=None, start_pct=0.0, num_samples=None, reconstructed_label=\"Reconstructed Signal\"):\n",
    "        \"\"\"\n",
    "        Plots the original test set against the reconstructed signal.\n",
    "        \"\"\"\n",
    "        if self.reconstructed_signal is None:\n",
    "            raise ValueError(\"Reconstructed signal not found. Please call recover_signal() first.\")\n",
    "        \n",
    "        if self.test_set is None:\n",
    "            raise ValueError(\"Test set not found. Please divide the signal before plotting.\")\n",
    "\n",
    "        # Check if the lengths of the signals match\n",
    "        if len(self.test_set) != len(self.reconstructed_signal):\n",
    "            warnings.warn(\"The original and reconstructed signals have different lengths. \"\n",
    "                        \"They will both be plotted up to the length of the shorter signal.\")\n",
    "\n",
    "        # Calculate the start index based on percentage\n",
    "        total_samples = min(len(self.test_set), len(self.reconstructed_signal))\n",
    "        start_idx = int(start_pct * total_samples)\n",
    "\n",
    "        # Determine the end index based on num_samples or plot till the end if num_samples is None\n",
    "        if num_samples is None:\n",
    "            end_idx = total_samples\n",
    "        else:\n",
    "            end_idx = min(start_idx + num_samples, total_samples)\n",
    "\n",
    "        # Slice the signals for plotting\n",
    "        original_signal_section = self.test_set[start_idx:end_idx]\n",
    "        reconstructed_signal_section = self.reconstructed_signal[start_idx:end_idx]\n",
    "\n",
    "        # Calculate SNR between the original test set and the reconstructed signals\n",
    "        snr = calculate_snr(original_signal_section, reconstructed_signal_section)\n",
    "\n",
    "        # Plot the selected section of the signals and display SNR\n",
    "        plot_signals(\n",
    "            original_signal_section, \n",
    "            reconstructed_signal_section, \n",
    "            snr=snr, \n",
    "            original_name=\"Original Signal\",  # Fixed label for original signal\n",
    "            reconstructed_name=reconstructed_label,  # Custom label for reconstructed signal\n",
    "            save_path=save_path, \n",
    "            filename=filename,\n",
    "            start_pct=start_pct,\n",
    "            num_samples=len(original_signal_section)  # Update plot title with actual number of samples being plotted\n",
    "        )\n",
    "\n",
    "    def get_measurement_matrix(self):\n",
    "        \"\"\"Retrieves the measurement matrix Phi.\"\"\"\n",
    "        return self.Phi\n",
    "\n",
    "    def get_compressed_signal(self):\n",
    "        \"\"\"Retrieves the compressed signal Y.\"\"\"\n",
    "        return self.Y\n",
    "\n",
    "    def get_dictionary(self):\n",
    "        \"\"\"Retrieves the generated dictionary.\"\"\"\n",
    "        return self.dictionary\n",
    "\n",
    "    def get_coeff_matrix(self):\n",
    "        \"\"\"Retrieves the coefficient matrix from MOD or K-SVD algorithm.\"\"\"\n",
    "        if self.coeff_matrix is None:\n",
    "            raise ValueError(\"The coefficient matrix has not been generated yet. Call generate_dictionary() with MOD or K-SVD first.\")\n",
    "        return self.coeff_matrix\n",
    "\n",
    "    def get_reconstructed_signal(self):\n",
    "        \"\"\"Retrieves the reconstructed signal after applying SL0.\"\"\"\n",
    "        if self.reconstructed_signal is None:\n",
    "            raise ValueError(\"The signal has not been reconstructed yet. Call recover_signal() first.\")\n",
    "        return self.reconstructed_signal\n",
    "\n",
    "    def get_original_signal(self):\n",
    "        \"\"\"Retrieves the original test signal that was passed to the class.\"\"\"\n",
    "        return self.test_set\n",
    "\n",
    "    def get_theta(self):\n",
    "        \"\"\"Retrieves theta (Phi @ dictionary).\"\"\"\n",
    "        if self.theta is None:\n",
    "            raise ValueError(\"theta has not been computed yet. Call recover_signal() first.\")\n",
    "        return self.theta\n",
    "\n",
    "    def get_theta_pinv(self):\n",
    "        \"\"\"Retrieves theta_pinv (pseudoinverse of Phi @ dictionary).\"\"\"\n",
    "        if self.theta_pinv is None:\n",
    "            raise ValueError(\"theta_pinv has not been computed yet. Call recover_signal() first.\")\n",
    "        return self.theta_pinv\n",
    "    \n",
    "    def get_test_set(self):\n",
    "        \"\"\"Retrieves the test set.\"\"\"\n",
    "        if self.test_set is None:\n",
    "            raise ValueError(\"Test set not defined. Please divide the signal before retrieving.\")\n",
    "        return self.test_set\n",
    "\n",
    "    def get_training_set(self):\n",
    "        \"\"\"Retrieves the training set.\"\"\"\n",
    "        if self.training_set is None:\n",
    "            raise ValueError(\"Training set not defined. Please divide the signal before retrieving.\")\n",
    "        return self.training_set\n",
    "\n",
    "    def get_snr(self):\n",
    "        \"\"\"\n",
    "        Computes and returns the Signal-to-Noise Ratio (SNR) between the original test set and the reconstructed signal.\n",
    "        \"\"\"\n",
    "        if self.test_set is None:\n",
    "            raise ValueError(\"Test set not found. Please divide the signal before computing SNR.\")\n",
    "        \n",
    "        if self.reconstructed_signal is None:\n",
    "            raise ValueError(\"Reconstructed signal not found. Please call recover_signal() first.\")\n",
    "        \n",
    "        # Ensure both signals have the same length by truncating to the shorter one\n",
    "        total_samples = min(len(self.test_set), len(self.reconstructed_signal))\n",
    "        \n",
    "        original_signal_section = self.test_set[:total_samples]\n",
    "        reconstructed_signal_section = self.reconstructed_signal[:total_samples]\n",
    "        \n",
    "        return calculate_snr(original_signal_section, reconstructed_signal_section)\n",
    "\n",
    "    def extract_model(self):\n",
    "        \"\"\"\n",
    "        Extracts the current state of the model for future recovery.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        model : dict\n",
    "            A dictionary containing 'phi', 'dict', 'is_kron', and 'original_phi'.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'phi': self.Phi,\n",
    "            'dict': self.dictionary,\n",
    "            'is_kron': self.is_kron,  # Use the is_kron flag directly\n",
    "            'original_phi': self.original_phi  # Always return the original version of Phi\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Number of repetitions\n",
    "REPS = 1\n",
    "\n",
    "# Define matrix type (can be changed)\n",
    "matrix_type = 'DBBD'  # Use 'DBBD', 'gaussian', 'scaled_binary', 'unscaled_binary'\n",
    "\n",
    "# MOD and K-SVD dictionary learning parameters\n",
    "mod_params = {\n",
    "    'redundancy': 1,  # This will translate to 'K = redundancy * BLOCK_LEN'\n",
    "    'num_iterations': 50,\n",
    "    'initialization_method': 'DataElements',  # Use 'DataElements' for initialization\n",
    "    'L': 4  # Number of non-zero coefficients to use in OMP\n",
    "}\n",
    "\n",
    "ksvd_params = {\n",
    "    'redundancy': 1,  # This will translate to 'K = redundancy * BLOCK_LEN'\n",
    "    'num_iterations': 50,\n",
    "    'initialization_method': 'DataElements',  # Use 'DataElements' for initialization\n",
    "    'L': 4,  # Number of non-zero coefficients to use in OMP\n",
    "    'preserve_dc_atom': 0  # Whether to preserve DC atom (0 for no)\n",
    "}\n",
    "\n",
    "# SL0 algorithm parameters\n",
    "sl0_params = {\n",
    "    'sigma_min': 1e-4,\n",
    "    'sigma_decrease_factor': 0.5,\n",
    "    'mu_0': 2,\n",
    "    'L': 3,\n",
    "    'showProgress': False\n",
    "}\n",
    "\n",
    "# Initialize SNR accumulators\n",
    "snr_dct_total = 0\n",
    "snr_mod_total = 0\n",
    "snr_ksvd_total = 0\n",
    "snr_dct_kron_total = 0\n",
    "snr_mod_kron_total = 0\n",
    "snr_ksvd_kron_total = 0\n",
    "\n",
    "# Loop through the number of repetitions\n",
    "for rep in range(REPS):\n",
    "    print(f\"Iteration {rep} just started\")\n",
    "    \n",
    "    # Load the data\n",
    "    data0 = scipy.io.loadmat('100m.mat')\n",
    "    signal = data0['val'][0]  # MLII ECG data\n",
    "    signal = signal[360*0:360*60*1]  # Define portion of signal for testing \n",
    "    # it has to be long enough so that training matrix has not less columns than rows\n",
    "\n",
    "    # Instantiate the class\n",
    "    cs = compressedSensing(signal=signal, matrix_type=matrix_type)\n",
    "\n",
    "    # Divide the signal\n",
    "    cs.divide_signal(training_percentage=0.9)\n",
    "\n",
    "    # Compress the test set\n",
    "    cs.compress_test_set()\n",
    "\n",
    "    # ----------------- Without Kronecker Compression -----------------\n",
    "\n",
    "    # ----------------- DCT-Based Dictionary Recovery -----------------\n",
    "    cs.generate_dictionary(dictionary_type='dct')\n",
    "    cs.recover_signal(sl0_params=sl0_params)  # Pass SL0 parameters here\n",
    "    snr_dct = cs.get_snr()\n",
    "    snr_dct_total += snr_dct\n",
    "\n",
    "    if REPS == 1:\n",
    "        cs.plot_reconstructed_vs_original(start_pct=0.0, num_samples=None, reconstructed_label=f\"{matrix_type}-DCT\")\n",
    "\n",
    "    # ----------------- MOD-Based Dictionary Recovery -----------------\n",
    "    cs.generate_dictionary(dictionary_type='mod', mod_params=mod_params)\n",
    "    cs.recover_signal(sl0_params=sl0_params)  # Pass SL0 parameters here\n",
    "    snr_mod = cs.get_snr()\n",
    "    snr_mod_total += snr_mod\n",
    "\n",
    "    if REPS == 1:\n",
    "        cs.plot_reconstructed_vs_original(start_pct=0.0, num_samples=None, reconstructed_label=f\"{matrix_type}-MOD\")\n",
    "\n",
    "    # ----------------- K-SVD-Based Dictionary Recovery -----------------\n",
    "    cs.generate_dictionary(dictionary_type='ksvd', ksvd_params=ksvd_params)\n",
    "    cs.recover_signal(sl0_params=sl0_params)  # Pass SL0 parameters here\n",
    "    snr_ksvd = cs.get_snr()\n",
    "    snr_ksvd_total += snr_ksvd\n",
    "\n",
    "    if REPS == 1:\n",
    "        cs.plot_reconstructed_vs_original(start_pct=0.0, num_samples=None, reconstructed_label=f\"{matrix_type}-KSVD\")\n",
    "\n",
    "    # ----------------- Activate Kronecker Compression -----------------\n",
    "    KRON_FACT = 8  # Example Kronecker factor\n",
    "    cs.kronecker_activate(KRON_FACT)\n",
    "\n",
    "    # ----------------- DCT-Based Dictionary Recovery with Kronecker -----------------\n",
    "    cs.generate_dictionary(dictionary_type='dct')\n",
    "    cs.recover_signal(sl0_params=sl0_params)  # Pass SL0 parameters here\n",
    "    snr_dct_kron = cs.get_snr()\n",
    "    snr_dct_kron_total += snr_dct_kron\n",
    "\n",
    "    if REPS == 1:\n",
    "        cs.plot_reconstructed_vs_original(start_pct=0.0, num_samples=None, reconstructed_label=f\"{matrix_type}-DCT-KRON\")\n",
    "\n",
    "    # ----------------- MOD-Based Dictionary Recovery with Kronecker -----------------\n",
    "    cs.generate_dictionary(dictionary_type='mod', mod_params=mod_params)\n",
    "    cs.recover_signal(sl0_params=sl0_params)  # Pass SL0 parameters here\n",
    "    snr_mod_kron = cs.get_snr()\n",
    "    snr_mod_kron_total += snr_mod_kron\n",
    "\n",
    "    if REPS == 1:\n",
    "        cs.plot_reconstructed_vs_original(start_pct=0.0, num_samples=None, reconstructed_label=f\"{matrix_type}-MOD-KRON\")\n",
    "\n",
    "    # ----------------- K-SVD-Based Dictionary Recovery with Kronecker -----------------\n",
    "    cs.generate_dictionary(dictionary_type='ksvd', ksvd_params=ksvd_params)\n",
    "    cs.recover_signal(sl0_params=sl0_params)  # Pass SL0 parameters here\n",
    "    snr_ksvd_kron = cs.get_snr()\n",
    "    snr_ksvd_kron_total += snr_ksvd_kron\n",
    "\n",
    "    if REPS == 1:\n",
    "        cs.plot_reconstructed_vs_original(start_pct=0.0, num_samples=None, reconstructed_label=f\"{matrix_type}-KSVD-KRON\")\n",
    "\n",
    "if REPS:\n",
    "    # Compute and print the average SNR for each method after all repetitions\n",
    "    avg_snr_dct = snr_dct_total / REPS\n",
    "    avg_snr_mod = snr_mod_total / REPS\n",
    "    avg_snr_ksvd = snr_ksvd_total / REPS\n",
    "    avg_snr_dct_kron = snr_dct_kron_total / REPS\n",
    "    avg_snr_mod_kron = snr_mod_kron_total / REPS\n",
    "    avg_snr_ksvd_kron = snr_ksvd_kron_total / REPS\n",
    "\n",
    "    print(\"\\n\\nAverage SNR Results after\", REPS, \"iterations:\")\n",
    "    print(f\"{matrix_type}-DCT: {avg_snr_dct:.2f} dB\")\n",
    "    print(f\"{matrix_type}-MOD: {avg_snr_mod:.2f} dB\")\n",
    "    print(f\"{matrix_type}-KSVD: {avg_snr_ksvd:.2f} dB\")\n",
    "    print(f\"{matrix_type}-DCT-KRON: {avg_snr_dct_kron:.2f} dB\")\n",
    "    print(f\"{matrix_type}-MOD-KRON: {avg_snr_mod_kron:.2f} dB\")\n",
    "    print(f\"{matrix_type}-KSVD-KRON: {avg_snr_ksvd_kron:.2f} dB\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for best MOD and KSVD parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set duration: 0 hour(s) and 6 minute(s)\n",
      "Testing set duration: 0 hour(s) and 3 minute(s)\n",
      "Training matrix shape: (16, 150)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[147], line 87\u001b[0m\n\u001b[1;32m     78\u001b[0m dict_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mredundancy\u001b[39m\u001b[38;5;124m'\u001b[39m: redundancy,\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_iterations\u001b[39m\u001b[38;5;124m'\u001b[39m: num_iterations,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreserve_dc_atom\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m  \u001b[38;5;66;03m# Always 0 for MOD\u001b[39;00m\n\u001b[1;32m     84\u001b[0m }\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Generate the MOD dictionary\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[43mcs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_dictionary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdictionary_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmod\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdict_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Recover the signal using fixed SL0 parameters for MOD\u001b[39;00m\n\u001b[1;32m     90\u001b[0m cs\u001b[38;5;241m.\u001b[39mrecover_signal(sl0_params\u001b[38;5;241m=\u001b[39msl0_params_dict[dictionary])\n",
      "Cell \u001b[0;32mIn[145], line 203\u001b[0m, in \u001b[0;36mcompressedSensing.generate_dictionary\u001b[0;34m(self, dictionary_type, mod_params, ksvd_params)\u001b[0m\n\u001b[1;32m    200\u001b[0m     mod_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mK\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m mod_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mredundancy\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBLOCK_LEN\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m# Run MOD algorithm with training matrix and mod_params\u001b[39;00m\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdictionary, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoeff_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mMOD\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmod_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dictionary_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mksvd\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_matrix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn[140], line 139\u001b[0m, in \u001b[0;36mMOD\u001b[0;34m(data, parameters)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Run MOD algorithm\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iter_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(parameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_iterations\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# Step 1: Sparse coding using OMP\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m     coef_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mOMP\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# Step 2: Update the dictionary\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     regularization_term \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1e-7\u001b[39m \u001b[38;5;241m*\u001b[39m sp\u001b[38;5;241m.\u001b[39meye(coef_matrix\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "Cell \u001b[0;32mIn[139], line 47\u001b[0m, in \u001b[0;36mOMP\u001b[0;34m(dictio, sig, max_coeff)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_coeff):\n\u001b[1;32m     46\u001b[0m     proj \u001b[38;5;241m=\u001b[39m dictio\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m@\u001b[39m residual\n\u001b[0;32m---> 47\u001b[0m     pos \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproj\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     48\u001b[0m     indx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(indx, pos)\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# Update selected atoms matrix\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if True:\n",
    "    # Fixed SL0 parameters (from optimal values)\n",
    "    sl0_params_dict = {\n",
    "        'dct': {'sigma_min': 1e-3, 'sigma_decrease_factor': 0.5, 'mu_0': 3, 'L': 3},\n",
    "        'mod': {'sigma_min': 1e-3, 'sigma_decrease_factor': 0.7, 'mu_0': 4, 'L': 10},\n",
    "        'ksvd': {'sigma_min': 1e-3, 'sigma_decrease_factor': 0.3, 'mu_0': 4, 'L': 3}\n",
    "    }\n",
    "\n",
    "    # Dictionary learning parameters ranges to test\n",
    "    redundancy_values = [1, 2, 3]\n",
    "    num_iterations_values = [1, 10, 20, 50]\n",
    "    L_values = [2, 4, 8]\n",
    "    preserve_dc_atom_values = [0]  # Test values for K-SVD's preserve_dc_atom\n",
    "\n",
    "    # Initialize list to store results\n",
    "    results_list = []\n",
    "\n",
    "    # Load the data\n",
    "    data0 = scipy.io.loadmat('100m.mat')\n",
    "    signal = data0['val'][0]  # MLII ECG data\n",
    "    signal = signal[360*0:360*10*1]  # Define portion of signal for testing\n",
    "\n",
    "    # Instantiate the compressed sensing class with the 'DBBD' matrix type\n",
    "    cs = compressedSensing(signal=signal, matrix_type='DBBD')\n",
    "\n",
    "    # Divide the signal into training and testing\n",
    "    cs.divide_signal(training_percentage=2/3)\n",
    "\n",
    "    # Compress the test set\n",
    "    cs.compress_test_set()\n",
    "\n",
    "    # Iterate over dictionary types\n",
    "    for dictionary in ['mod', 'ksvd']:\n",
    "        \n",
    "        # Iterate over redundancy, num_iterations, and L values\n",
    "        for redundancy in redundancy_values:\n",
    "            for num_iterations in num_iterations_values:\n",
    "                for L in L_values:\n",
    "                    \n",
    "                    # If K-SVD, we need to iterate over preserve_dc_atom values\n",
    "                    if dictionary == 'ksvd':\n",
    "                        for preserve_dc_atom in preserve_dc_atom_values:\n",
    "                            dict_params = {\n",
    "                                'redundancy': redundancy,\n",
    "                                'num_iterations': num_iterations,\n",
    "                                'initialization_method': 'DataElements',\n",
    "                                'L': L,\n",
    "                                'preserve_dc_atom': preserve_dc_atom  # Testing different values for preserve_dc_atom\n",
    "                            }\n",
    "\n",
    "                            # Generate the K-SVD dictionary\n",
    "                            cs.generate_dictionary(dictionary_type='ksvd', ksvd_params=dict_params)\n",
    "\n",
    "                            # Recover the signal using fixed SL0 parameters for K-SVD\n",
    "                            cs.recover_signal(sl0_params=sl0_params_dict[dictionary])\n",
    "\n",
    "                            # Get the SNR\n",
    "                            snr = cs.get_snr()\n",
    "\n",
    "                            # Append the results to the list\n",
    "                            results_list.append({\n",
    "                                'Measurement Matrix': 'DBBD',\n",
    "                                'Dictionary': dictionary,\n",
    "                                'redundancy': redundancy,\n",
    "                                'num_iterations': num_iterations,\n",
    "                                'L_dict': L,\n",
    "                                'preserve_dc_atom': preserve_dc_atom,  # Include preserve_dc_atom for K-SVD\n",
    "                                'SNR': snr\n",
    "                            })\n",
    "\n",
    "                    else:\n",
    "                        # For MOD, preserve_dc_atom is always 0\n",
    "                        dict_params = {\n",
    "                            'redundancy': redundancy,\n",
    "                            'num_iterations': num_iterations,\n",
    "                            'initialization_method': 'DataElements',\n",
    "                            'L': L,\n",
    "                            'preserve_dc_atom': 0  # Always 0 for MOD\n",
    "                        }\n",
    "\n",
    "                        # Generate the MOD dictionary\n",
    "                        cs.generate_dictionary(dictionary_type='mod', mod_params=dict_params)\n",
    "\n",
    "                        # Recover the signal using fixed SL0 parameters for MOD\n",
    "                        cs.recover_signal(sl0_params=sl0_params_dict[dictionary])\n",
    "\n",
    "                        # Get the SNR\n",
    "                        snr = cs.get_snr()\n",
    "\n",
    "                        # Append the results to the list\n",
    "                        results_list.append({\n",
    "                            'Measurement Matrix': 'DBBD',\n",
    "                            'Dictionary': dictionary,\n",
    "                            'redundancy': redundancy,\n",
    "                            'num_iterations': num_iterations,\n",
    "                            'L_dict': L,\n",
    "                            'preserve_dc_atom': 0,  # Always 0 for MOD\n",
    "                            'SNR': snr\n",
    "                        })\n",
    "\n",
    "    # Convert the results list to a DataFrame\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    output_folder = Path('tests_results')\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save the results to the CSV file in the folder\n",
    "    output_file = output_folder / 'mod_ksvd_parameter_sweep_results.csv'\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"MOD and K-SVD parameter sweep complete and results saved to '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test for best SL0 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set duration: 0 hour(s) and 2 minute(s)\n",
      "Testing set duration: 0 hour(s) and 1 minute(s)\n",
      "Training matrix shape: (16, 45)\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "*\n",
      "Parameter sweep complete and results saved to 'tests_results/sl0_parameter_sweep_results.csv'\n"
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "if True:\n",
    "    # SL0 parameter ranges for testing\n",
    "    sigma_min_values = [1e-3]  # [1e-2, 1e-3, 1e-4, 1e-6]\n",
    "    sigma_decrease_factors = [0.1, 0.3, 0.5, 0.7]  # [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    mu_0_values = list(range(1, 11))  # Values from 1 to 10\n",
    "    L_values = [3, 5, 10]\n",
    "\n",
    "    # Dictionary learning parameters\n",
    "    mod_params = {\n",
    "        'redundancy': 1,\n",
    "        'num_iterations': 50,\n",
    "        'initialization_method': 'DataElements',\n",
    "        'L': 4\n",
    "    }\n",
    "\n",
    "    ksvd_params = {\n",
    "        'redundancy': 1,\n",
    "        'num_iterations': 50,\n",
    "        'initialization_method': 'DataElements',\n",
    "        'L': 4,\n",
    "        'preserve_dc_atom': 0\n",
    "    }\n",
    "\n",
    "    # Initialize list to store results\n",
    "    results_list = []\n",
    "\n",
    "    # Load the data\n",
    "    data0 = scipy.io.loadmat('100m.mat')\n",
    "    signal = data0['val'][0]  # MLII ECG data\n",
    "    signal = signal[360*0:360*3*1]  # Define portion of signal for testing\n",
    "\n",
    "    # Instantiate the compressed sensing class with the 'DBBD' matrix type\n",
    "    cs = compressedSensing(signal=signal, matrix_type='DBBD')\n",
    "\n",
    "    # Divide the signal into training and testing\n",
    "    cs.divide_signal(training_percentage=2/3)\n",
    "\n",
    "    # Compress the test set\n",
    "    cs.compress_test_set()\n",
    "\n",
    "    # Iterate over sigma_min, sigma_decrease_factors, mu_0_values, and L_values\n",
    "    for sigma_min_value in sigma_min_values:\n",
    "        for sigma_decrease_factor in sigma_decrease_factors:\n",
    "            for mu_0 in mu_0_values:\n",
    "                for L in L_values:\n",
    "                    print(\"*\")\n",
    "                    sl0_params = {\n",
    "                        'sigma_min': sigma_min_value,\n",
    "                        'sigma_decrease_factor': sigma_decrease_factor,\n",
    "                        'mu_0': mu_0,\n",
    "                        'L': L,\n",
    "                        'showProgress': False\n",
    "                    }\n",
    "                    \n",
    "                    # Test with different dictionaries\n",
    "                    for dictionary in ['dct', 'mod', 'ksvd']:\n",
    "                        \n",
    "                        # Generate the dictionary\n",
    "                        if dictionary == 'dct':\n",
    "                            cs.generate_dictionary(dictionary_type='dct')\n",
    "                        elif dictionary == 'mod':\n",
    "                            cs.generate_dictionary(dictionary_type='mod', mod_params=mod_params)\n",
    "                        elif dictionary == 'ksvd':\n",
    "                            cs.generate_dictionary(dictionary_type='ksvd', ksvd_params=ksvd_params)\n",
    "                        \n",
    "                        # Recover the signal\n",
    "                        cs.recover_signal(sl0_params=sl0_params)\n",
    "                        \n",
    "                        # Get the SNR\n",
    "                        snr = cs.get_snr()\n",
    "                        \n",
    "                        # Append the results to the list\n",
    "                        results_list.append({\n",
    "                            'Measurement Matrix': 'DBBD',\n",
    "                            'Dictionary': dictionary,\n",
    "                            'sigma_min': sigma_min_value,\n",
    "                            'sigma_decrease_factor': sigma_decrease_factor,\n",
    "                            'mu_0': mu_0,\n",
    "                            'L': L,\n",
    "                            'SNR': snr\n",
    "                        })\n",
    "\n",
    "    # Convert the results list to a DataFrame\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "\n",
    "    # Ensure the output folder exists\n",
    "    output_folder = Path('tests_results')\n",
    "    output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save the results to the CSV file in the folder\n",
    "    output_file = output_folder / 'sl0_parameter_sweep_results.csv'\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Parameter sweep complete and results saved to '{output_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test best KRONECKER FACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set duration: 0 hour(s) and 2 minute(s)\n",
      "Testing set duration: 0 hour(s) and 1 minute(s)\n",
      "Training matrix shape: (16, 45)\n",
      "KRONECKER ACTIVATE Training matrix shape: (32, 22)\n",
      "MOD: number of training signals is smaller than the dictionary size. Returning trivial solution...\n",
      "KSVD: number of training data is smaller than the dictionary size. Trivial solution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8787/1054745558.py:159: UserWarning: The number of samples (columns) in the training matrix is shorter than the number of rows, which can cause issues with dictionary learning.\n",
      "  warnings.warn(\"The number of samples (columns) in the training matrix is shorter than \"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Kronecker compression has already been activated. Cannot activate again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[124], line 52\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Iterate over KRON_FACT values\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m kron_fact \u001b[38;5;129;01min\u001b[39;00m kron_fact_values:\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# Activate Kronecker compression with the current KRON_FACT value\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     \u001b[43mcs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkronecker_activate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mKRON_FACT\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkron_fact\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;66;03m# Iterate over dictionary types\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m dictionary \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdct\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmod\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mksvd\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m     56\u001b[0m         \n\u001b[1;32m     57\u001b[0m         \u001b[38;5;66;03m# Generate the dictionary\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[120], line 143\u001b[0m, in \u001b[0;36mcompressedSensing.kronecker_activate\u001b[0;34m(self, KRON_FACT)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03mActivates Kronecker compression, adjusting BLOCK_LEN and reprocessing Phi and Y accordingly.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_kron\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_kron:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKronecker compression has already been activated. Cannot activate again.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mY has not been computed. Please compress the signal before activating the kronecker method.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Kronecker compression has already been activated. Cannot activate again."
     ]
    }
   ],
   "source": [
    "import scipy.io\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Fixed SL0 parameters (from optimal values)\n",
    "sl0_params_dict = {\n",
    "    'dct': {'sigma_min': 1e-3, 'sigma_decrease_factor': 0.5, 'mu_0': 3, 'L': 3},\n",
    "    'mod': {'sigma_min': 1e-3, 'sigma_decrease_factor': 0.7, 'mu_0': 4, 'L': 10},\n",
    "    'ksvd': {'sigma_min': 1e-3, 'sigma_decrease_factor': 0.3, 'mu_0': 4, 'L': 3}\n",
    "}\n",
    "\n",
    "# Fixed MOD and K-SVD parameters\n",
    "mod_params = {\n",
    "    'redundancy': 1,\n",
    "    'num_iterations': 50,\n",
    "    'initialization_method': 'DataElements',\n",
    "    'L': 4\n",
    "}\n",
    "\n",
    "ksvd_params = {\n",
    "    'redundancy': 1,\n",
    "    'num_iterations': 50,\n",
    "    'initialization_method': 'DataElements',\n",
    "    'L': 4,\n",
    "    'preserve_dc_atom': 0\n",
    "}\n",
    "\n",
    "# KRON_FACT values to test\n",
    "kron_fact_values = [2, 3, 4, 6, 8, 10, 12, 16]\n",
    "\n",
    "# Initialize list to store results\n",
    "results_list = []\n",
    "\n",
    "# Load the data\n",
    "data0 = scipy.io.loadmat('100m.mat')\n",
    "signal = data0['val'][0]  # MLII ECG data\n",
    "signal = signal[360*0:360*3*1]  # Define portion of signal for testing\n",
    "\n",
    "# Instantiate the compressed sensing class with the 'DBBD' matrix type\n",
    "cs = compressedSensing(signal=signal, matrix_type='DBBD')\n",
    "\n",
    "# Divide the signal into training and testing\n",
    "cs.divide_signal(training_percentage=2/3)\n",
    "\n",
    "# Compress the test set\n",
    "cs.compress_test_set()\n",
    "\n",
    "# Iterate over KRON_FACT values\n",
    "for kron_fact in kron_fact_values:\n",
    "    # Activate Kronecker compression with the current KRON_FACT value\n",
    "    cs.kronecker_activate(KRON_FACT=kron_fact)\n",
    "    \n",
    "    # Iterate over dictionary types\n",
    "    for dictionary in ['dct', 'mod', 'ksvd']:\n",
    "        \n",
    "        # Generate the dictionary\n",
    "        if dictionary == 'dct':\n",
    "            cs.generate_dictionary(dictionary_type='dct')\n",
    "        elif dictionary == 'mod':\n",
    "            cs.generate_dictionary(dictionary_type='mod', mod_params=mod_params)\n",
    "        elif dictionary == 'ksvd':\n",
    "            cs.generate_dictionary(dictionary_type='ksvd', ksvd_params=ksvd_params)\n",
    "        \n",
    "        # Recover the signal using fixed SL0 parameters\n",
    "        cs.recover_signal(sl0_params=sl0_params_dict[dictionary])\n",
    "\n",
    "        # Get the SNR\n",
    "        snr = cs.get_snr()\n",
    "\n",
    "        # Append the results to the list\n",
    "        results_list.append({\n",
    "            'Measurement Matrix': 'DBBD',\n",
    "            'Dictionary': dictionary,\n",
    "            'KRON_FACT': kron_fact,\n",
    "            'SNR': snr\n",
    "        })\n",
    "\n",
    "# Convert the results list to a DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Ensure the output folder exists\n",
    "output_folder = Path('tests_results')\n",
    "output_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the results to the CSV file in the folder\n",
    "output_file = output_folder / 'kron_factor_sweep_results.csv'\n",
    "results_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Kronecker factor sweep complete and results saved to '{output_file}'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".namlVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
