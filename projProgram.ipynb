{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code implementation (python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "__Sampling phase__\n",
    "- $y = \\Phi x$, simulate what would happen on device (_compute block by block, later concatenate results_)\n",
    "\n",
    "---\n",
    "\n",
    "__Recovery phase__\n",
    "- Reconstruct $\\hat{s}$, approximation of $s$ _sparse representation_ of $x$\n",
    "- Use dictionary $\\Psi$ to retrieve $\\hat{x}$ approximation of $x$ \n",
    "\n",
    "--- \n",
    "\n",
    "__Evaluate result__\n",
    "1. Sampling faster for smaller $\\Phi$?\n",
    "2. Which dictionary are the best? (How fast, how accurate)\n",
    "3. With Kronecker vs without\n",
    "4. ~~Which recovery method is best? (How fast, how accurate)~~ only use Smooth-L0\n",
    "5. ~~Robustness to noise?~~\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: sampling phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurement matrix $\\Phi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_measurement_matrix(rows, cols):\n",
    "    \"\"\"\n",
    "    Generates a random measurement matrix with the specified number of rows and columns.\n",
    "\n",
    "    Parameters:\n",
    "    - rows (int): The number of rows in the measurement matrix.\n",
    "    - cols (int): The number of columns in the measurement matrix.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The generated measurement matrix with shape (rows, cols).\n",
    "\n",
    "    Description:\n",
    "    This function generates a random measurement matrix with the specified number of rows and columns. \n",
    "    The measurement matrix is created by randomly choosing either -1 or 1 for each element in the matrix.\n",
    "    The resulting matrix is returned as a numpy array.\n",
    "    \"\"\"\n",
    "\n",
    "    return np.random.choice([-1, 1], size=(rows, cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal Sampling $y = \\Phi x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_compressed_measurement(signal, signal_block_dimension):\n",
    "    \"\"\"\n",
    "    Computes the compressed measurement of a signal using compressed sensing.\n",
    "\n",
    "    Parameters:\n",
    "    - signal (numpy.ndarray): The input signal.\n",
    "    - signal_block_dimension (int): The dimension of each signal block.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: The compressed measurement of the signal.\n",
    "    - numpy.ndarray: The measurement matrix used for compression.\n",
    "\n",
    "    Description:\n",
    "    This function computes the compressed measurement of a signal using compressed sensing.\n",
    "    It generates a measurement matrix with the specified number of columns based on the signal block dimension.\n",
    "    Then, it computes the compressed measurement block by block and concatenates the results to obtain the compressed measurement of the whole signal.\n",
    "    The resulting compressed measurement and measurement matrix are returned as numpy arrays.\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of blocks\n",
    "    num_blocks = len(signal) // signal_block_dimension\n",
    "\n",
    "    # To a achieve 75% compression rate: the number of rows in the measurement matrix is set to 1/4 of the signal block dimension\n",
    "    NUM_OF_ROWS = signal_block_dimension // 4\n",
    "\n",
    "    # Generate measurement matrix\n",
    "    measurement_matrix = generate_measurement_matrix( NUM_OF_ROWS, signal_block_dimension)\n",
    "\n",
    "    # Initialize the compressed measurement\n",
    "    compressed_measurement = np.empty(0)\n",
    "\n",
    "    # Compute the compressed measurement block by block\n",
    "    for i in range(num_blocks):\n",
    "        \n",
    "        # Get the current block of the signal\n",
    "        block = signal[i * signal_block_dimension : (i + 1) * signal_block_dimension]\n",
    "\n",
    "        # Compute the compressed measurement block\n",
    "        compressed_measurement_block = np.dot(measurement_matrix, block)\n",
    "\n",
    "        # Concatenate the compressed measurement block to the compressed measurement\n",
    "        compressed_measurement = np.concatenate((compressed_measurement, compressed_measurement_block))\n",
    "\n",
    "    return compressed_measurement, measurement_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: recovery phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only two things needed out of the _sampling phase_ are $y$ and $\\Phi$\n",
    "\n",
    "Next step is to solve:\n",
    "$$\n",
    "\\hat{s} = \\arg_{s} \\min \\|s\\|_1 \\text{ subject to } y = \\Phi \\Psi \\alpha\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix $\\Theta = \\Phi \\Psi$ for fixed dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's necessary to compute $\\Theta = \\Phi \\Psi$\n",
    "- First possibility is to actually compute the dictionary $\\Psi$\n",
    "- It's not efficient to compute the whole base $\\Psi$, it's preferable a _matrix-free_ approach\n",
    "- We should allow choosing various __fixed dictionaries__\n",
    "- We need to store what transform we used in order to successfully reconstruct in later steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.fftpack import idct\n",
    "from pywt import wavedec, waverec\n",
    "\n",
    "def compute_theta(phi, transform_type='dwt', wavelet='db4'):\n",
    "    \"\"\"\n",
    "    Computes the matrix Theta = Phi * Psi using a matrix-free approach with the DCT or DWT as the basis for Psi.\n",
    "    Also returns the transform type and wavelet type (if applicable).\n",
    "\n",
    "    Parameters:\n",
    "    - phi (numpy.ndarray): The measurement matrix Phi of shape (m, n).\n",
    "    - transform_type (str): The type of transform to use. Can be 'dwt' (default) or 'dct'.\n",
    "    - wavelet (str): The type of wavelet to use for DWT. Default is 'db4'.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (Theta, transform_type, wavelet_type)\n",
    "      - Theta (numpy.ndarray): The matrix Theta of shape (m, n), where Theta = Phi * Psi.\n",
    "      - transform_type (str): A string indicating the transform used ('dct' or 'dwt').\n",
    "      - wavelet_type (str or None): The wavelet name if DWT is used; None if DCT is used.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the number of columns of Phi, equal to signal block dimension\n",
    "    n = phi.shape[1]\n",
    "\n",
    "    # Initialize Theta as a zero matrix of shape (m, n)\n",
    "    m = phi.shape[0]\n",
    "    theta = np.zeros((m, n))\n",
    "\n",
    "    # Set the wavelet_type to None initially\n",
    "    wavelet_type = None\n",
    "\n",
    "    # Iteratively compute each column of Theta\n",
    "    for ii in range(n):\n",
    "        # Create the unit vector ek\n",
    "        ek = np.zeros(n)\n",
    "        ek[ii] = 1\n",
    "\n",
    "        # Compute the corresponding column of Psi using the chosen transform\n",
    "        if transform_type == 'dct':\n",
    "            psi_column = idct(ek, norm='ortho')\n",
    "        elif transform_type == 'dwt':\n",
    "            # Decompose the unit vector in the wavelet domain\n",
    "            coeffs = wavedec(ek, wavelet, level=1)\n",
    "            # Reconstruct the unit vector (basis vector) in the time domain\n",
    "            psi_column = waverec(coeffs, wavelet)[:n]\n",
    "            wavelet_type = wavelet  # Set the wavelet_type to the specified wavelet\n",
    "\n",
    "        # Compute the corresponding column of Theta\n",
    "        theta[:, ii] = np.dot(phi, psi_column)\n",
    "\n",
    "    return theta, transform_type, wavelet_type\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruction Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\ell_{1}$-minimization standard solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "from scipy.fftpack import idct\n",
    "from pywt import waverec\n",
    "\n",
    "def l1_minimization(y, Theta):\n",
    "    \"\"\"\n",
    "    Solves the L1-minimization problem to recover the sparse representation s from the compressed measurements y.\n",
    "    \n",
    "    Parameters:\n",
    "    y (numpy.ndarray): The compressed measurements.\n",
    "    Theta (numpy.ndarray): The combined matrix Theta (Phi * Psi) used during compression.\n",
    "    \n",
    "    Returns:\n",
    "    s_recovered (numpy.ndarray): The recovered sparse representation.\n",
    "    \"\"\"\n",
    "    n = Theta.shape[1]\n",
    "\n",
    "    # Objective function: Minimize the L1 norm of the sparse representation s\n",
    "    c = np.ones(2 * n)\n",
    "\n",
    "    # Equality constraints: Theta * s = y\n",
    "    A_eq = np.hstack([Theta, -Theta])\n",
    "    b_eq = y\n",
    "\n",
    "    # Variable bounds: s+ >= 0 and s- >= 0\n",
    "    bounds = [(0, None)] * (2 * n)\n",
    "\n",
    "    # Solve the linear program using linprog\n",
    "    result = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')\n",
    "\n",
    "    if result.success:\n",
    "        s_recovered = result.x[:n] - result.x[n:]\n",
    "        return s_recovered\n",
    "    else:\n",
    "        raise ValueError(\"L1-minimization did not converge\")\n",
    "\n",
    "\n",
    "def blockwise_l1_signal_reconstruction(y, Theta, transform_type, wavelet=None):\n",
    "    \"\"\"\n",
    "    Solves the L1-minimization problem block by block to find the sparsest s\n",
    "    that satisfies y = Theta * s, then inverts the transform to recover x.\n",
    "\n",
    "    Parameters:\n",
    "    - y (numpy.ndarray): The compressed measurements of the entire signal.\n",
    "    - Theta (numpy.ndarray): The measurement matrix Theta (Phi * Psi) for the blocks.\n",
    "    - transform_type (str): The type of transform used ('dct' or 'dwt').\n",
    "    - wavelet (str): The type of wavelet to use for DWT. Must be provided if 'dwt' is selected.\n",
    "\n",
    "    Returns:\n",
    "    - x_full (numpy.ndarray): The reconstructed signal x.\n",
    "    \"\"\"\n",
    "\n",
    "    if transform_type == 'dwt' and wavelet is None:\n",
    "        raise ValueError(\"Wavelet type must be provided when using 'dwt' as the transform_type.\")\n",
    "\n",
    "    num_blocks = len(y) // Theta.shape[0]\n",
    "    x_blocks = []\n",
    "\n",
    "    for i in range(num_blocks):\n",
    "        y_block = y[i * Theta.shape[0]: (i + 1) * Theta.shape[0]]\n",
    "        s_block = l1_minimization(y_block, Theta)\n",
    "\n",
    "        # Apply the inverse transform to recover the block x_block\n",
    "        if transform_type == 'dct':\n",
    "            x_block = idct(s_block, norm='ortho')\n",
    "        elif transform_type == 'dwt':\n",
    "            # Perform wavelet reconstruction\n",
    "            coeffs = [s_block] + [np.zeros_like(s_block) for _ in range(len(wavelet))]\n",
    "            x_block = waverec(coeffs, wavelet)[:len(s_block)]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown transform type: {transform_type}\")\n",
    "\n",
    "        x_blocks.append(x_block)\n",
    "\n",
    "    x_full = np.concatenate(x_blocks)\n",
    "    return x_full\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: reconstructed signal evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def calculate_prd(original_signal, reconstructed_signal):\n",
    "    \"\"\"\n",
    "    Calculate the Percentage Root Mean Square Difference (PRD).\n",
    "    \n",
    "    Parameters:\n",
    "    original_signal (np.array): The original ECG signal.\n",
    "    reconstructed_signal (np.array): The reconstructed ECG signal.\n",
    "    \n",
    "    Returns:\n",
    "    float: The PRD value as a percentage.\n",
    "    \"\"\"\n",
    "    numerator = np.sum((original_signal - reconstructed_signal) ** 2)\n",
    "    denominator = np.sum(original_signal ** 2)\n",
    "    prd = 100 * np.sqrt(numerator / denominator)\n",
    "    return prd\n",
    "\n",
    "def calculate_snr(prd):\n",
    "    \"\"\"\n",
    "    Calculate the Signal-to-Noise Ratio (SNR) based on PRD.\n",
    "    \n",
    "    Parameters:\n",
    "    prd (float): The PRD value as a percentage.\n",
    "    \n",
    "    Returns:\n",
    "    float: The SNR value in dB.\n",
    "    \"\"\"\n",
    "    snr = -20 * np.log10(prd / 100)\n",
    "    return snr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: test for best dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test all dictionaries to show with big number of data which are the one to perform better (we expect DWT to be the best of fixed, and that adaptive are better than fixed in general)\n",
    "- __#Records:__ Test on __MULTIPLE patients__ records is a MUST, especially to show that adaptive are better \n",
    "- __#Dictionaries:__ Test all dictionaries, that's what we are doing ...\n",
    "- __#ReconstructionMethods:__ Test with __only one reconstruction method__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: test for best reconstruction method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test to find which recontruction method is the best\n",
    "- __#Records:__ Test on a __single patient__ record should be fine \n",
    "- __#Dictionaries:__ Test with __a single dictionary type__\n",
    "- __#ReconstructionMethods:__ Test with __all reconstruction methods__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: test for correct block dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test to show that __sampling phase process speed is inversly proportional to block dimension__\n",
    "- __#Records:__ Test on a __single patient__ record should be fine \n",
    "- __#Dictionaries:__ Test with __a single dictionary type__ (USE BEST!)\n",
    "- __#ReconstructionMethods:__ Test with __only one reconstruction method__ (USE BEST!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code: test for noise robustness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Test if robust to noise by adding noise to the signal\n",
    "- __#Records:__ Test on a __single patient__ record should be fine \n",
    "- __#Dictionaries:__ Test with __a single dictionary type__ (USE BEST!)\n",
    "- __#ReconstructionMethods:__ Test with __only one reconstruction method__ (USE BEST!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data: MIT–BIH Arrhythmia Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MIT-BIH Arrhythmia Database\n",
    "\n",
    "__Source:__ [physionet.org](https://www.physionet.org/content/mitdb/1.0.0/)\n",
    "\n",
    "__Authors:__ George Moody, Roger Mark\n",
    "\n",
    "__Version:__ 1.0.0 (Feb. 24, 2005)\n",
    "\n",
    "__Citation Information__\n",
    "\n",
    "__Original publication:__\n",
    "Moody GB, Mark RG. *The impact of the MIT-BIH Arrhythmia Database*. IEEE Eng in Med and Biol 20(3):45-50 (May-June 2001). (PMID: 11446209)\n",
    "\n",
    "__Citation for PhysioNet:__\n",
    "Goldberger, A., Amaral, L., Glass, L., Hausdorff, J., Ivanov, P. C., Mark, R., ... & Stanley, H. E. (2000). *PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals*. Circulation [Online]. 101 (23), pp. e215–e220.\n",
    "\n",
    "__Background__\n",
    "\n",
    "Since 1975, laboratories at Boston’s Beth Israel Hospital (now the Beth Israel Deaconess Medical Center) and at MIT have supported research into arrhythmia analysis and related subjects. One of the first major products of that effort was the MIT-BIH Arrhythmia Database, completed and distributed in 1980. The database was the first generally available set of standard test material for evaluating arrhythmia detectors and has been used for that purpose as well as for basic research into cardiac dynamics at more than 500 sites worldwide. Originally, the database was distributed on 9-track half-inch digital tape at 800 and 1600 bpi, and on quarter-inch IRIG-format FM analog tape. In August 1989, a CD-ROM version of the database was produced.\n",
    "\n",
    "__Data Description__\n",
    "\n",
    "The MIT-BIH Arrhythmia Database contains:\n",
    "\n",
    "- 48 half-hour excerpts of __two-channel ambulatory ECG recordings.__\n",
    "- Data obtained from 47 subjects (1975-1979).\n",
    "- 23 recordings chosen at random from a set of 4000 24-hour ambulatory ECG recordings from Boston’s Beth Israel Hospital (inpatients 60%, outpatients 40%).\n",
    "- 25 recordings selected to include less common but clinically significant arrhythmias.\n",
    "- __Digitized at 360 samples per second per channel with 11-bit resolution over a 10 mV range.__\n",
    "- Annotations by two or more cardiologists; disagreements resolved to obtain reference annotations (approx. 110,000 annotations).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the database (command worked on August 2024, otherwise look for the database on physionet.org)\n",
    "\n",
    "# This will ignore:\n",
    "# mitdbdir: html where data is presented and explained (go on website if intrested or download without \"--exclude\" option)\n",
    "# *.xws: files that are used to visualize data on a Physionet.org tool called \"LightWave\" (go on website if intrested or download without \"--reject\" option)\n",
    "\n",
    "#!wget -r -N -c -np --reject \"*.xws\" --exclude-directories=mitdbdir https://physionet.org/files/mitdb/1.0.0/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Retrieving the Data Path\n",
    "\n",
    "To read the MITDB (MIT-BIH Arrhythmia Database) data using the `wfdb` library, we first need to retrieve the data path. In this Jupyter Notebook, the data path is stored in the `data_folder` variable.\n",
    "We can use this path to locate the specific record we want to read. In this case, the record number is stored in the `record_number` variable.\n",
    "\n",
    "To construct the full path to the record, we can use the `os.path.join()` function as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the project root directory (assuming the notebook is in the root directory)\n",
    "project_root = os.getcwd()  # Gets the current working directory, which is the root in this case\n",
    "\n",
    "# Define the relative path to the data folder\n",
    "data_folder = os.path.join(project_root, 'physionet.org', 'files', 'mitdb', '1.0.0')\n",
    "\n",
    "# Specify the record number\n",
    "record_number = '100'  # Change this to the desired record number\n",
    "\n",
    "# Construct the full path to the record\n",
    "record_path = os.path.join(data_folder, record_number)\n",
    "\n",
    "\n",
    "# print record_path\n",
    "print(record_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load the signal and annotations\n",
    "In the code above, we first import the `wfdb` module. Then, we use the `rdrecord()` function to load the signal from the specified `record_path`. We also use the `rdann()` function to load the annotations for the same record. \n",
    "\n",
    "After executing this code, the signal will be stored in the `record` variable, and the annotations will be stored in the `annotation` variable.\n",
    "\n",
    "We will then visualize the whole signal, it's unreadable on a 30 hours time frame, it's only to visually check if downloading went well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb\n",
    "\n",
    "# Load the signal and annotations\n",
    "record = wfdb.rdrecord(record_path)\n",
    "annotation = wfdb.rdann(record_name=record_path, extension='atr', shift_samps=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the signal with annotations, just to check if download went correctly\n",
    "wfdb.plot_wfdb(record=record, annotation=annotation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: moving data to Pandas dataframe\n",
    "Simply use the proper function from `wfdb` library\n",
    "\n",
    "__Annotation will not be ported to pandas dataframe__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = record.to_dataframe()\n",
    "\n",
    "print(df.head(15))\n",
    "print(\"\\n\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART II: visualize the ECG data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ECG wave consists of three main components: the P wave, the QRS complex, and the T wave.\n",
    "\n",
    "- The P wave represents atrial depolarization, which is the contraction of the atria. It is typically a small and smooth upward deflection.\n",
    "- The QRS complex represents ventricular depolarization, which is the contraction of the ventricles. It consists of three distinct waves: Q, R, and S. The Q wave is the first downward deflection, the R wave is the first upward deflection after the Q wave, and the S wave is the downward deflection after the R wave.\n",
    "- The T wave represents ventricular repolarization, which is the recovery of the ventricles. It is typically a smooth upward deflection.\n",
    "\n",
    "The typical duration of the P wave is around 80-100 milliseconds, the QRS complex lasts around 80-120 milliseconds, and the T wave lasts around 160-240 milliseconds.\n",
    "\n",
    "The distance between waves can vary depending on the heart rate. In a normal sinus rhythm, the distance between consecutive P waves (P-P interval) is usually consistent and represents the atrial rate. The distance between consecutive R waves (R-R interval) represents the ventricular rate. The normal range for the R-R interval is around 600-1000 milliseconds.\n",
    "\n",
    "When plotting hours-long ECG data, the waves become densely packed, making it difficult to interpret the waveform patterns. To overcome this, we can plot a specific interval of the ECG data by selecting a starting time and duration. This allows us to focus on a specific part of the ECG waveform and make it more interpretable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be useful when deciding portion of data to work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_ecg_interval(df, start=0, duration=1, fs=360):\n",
    "    \"\"\"\n",
    "    Plot an interval of ECG data.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the ECG data with time as index.\n",
    "    - start: Starting time in seconds for the plot.\n",
    "    - duration: Duration in seconds of the interval to plot.\n",
    "    - fs: Sampling frequency (samples per second).\n",
    "    \"\"\"\n",
    "    start_sample = int(start * fs)\n",
    "    end_sample = int((start + duration) * fs)\n",
    "    \n",
    "    interval_df = df.iloc[start_sample:end_sample]\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 8), sharex=True)\n",
    "    \n",
    "    # Plot MLII in the first subplot\n",
    "    axes[0].plot(interval_df.index, interval_df['MLII'], label='MLII')\n",
    "    axes[0].set_ylabel('Amplitude (mV)')\n",
    "    axes[0].set_title(f'ECG MLII Data from {start} to {start + duration} seconds')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # Plot V5 in the second subplot\n",
    "    axes[1].plot(interval_df.index, interval_df['V5'], label='V5', color='orange')\n",
    "    axes[1].set_xlabel('Time (seconds)')\n",
    "    axes[1].set_ylabel('Amplitude (mV)')\n",
    "    axes[1].set_title(f'ECG V5 Data from {start} to {start + duration} seconds')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test single implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose portion of record to work with\n",
    "# starting sample\n",
    "start = 302\n",
    "# duration in seconds\n",
    "duration = 1024\n",
    "\n",
    "# Extract that portion of the signal\n",
    "ecg_signal = df.iloc[start:start + duration]\n",
    "\n",
    "# Plot \"duration\" number of seconds of ecg data starting at \"start\" seconds\n",
    "plot_ecg_interval(df, start=302, duration=1) # Change the start and duration values as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the compressed measurement of the ECG signal\n",
    "y,Phi = compute_compressed_measurement(ecg_signal['MLII'].values, 16)\n",
    "\n",
    "# Compute Theta\n",
    "Theta, transform_type, wavelet_type = compute_theta(Phi, transform_type='dct')\n",
    "\n",
    "# Reconstruct the signal using blockwise L1 minimization\n",
    "reconstructed_signal = blockwise_l1_signal_reconstruction(y, Theta, transform_type)\n",
    "\n",
    "# Calculate the PRD between the original and reconstructed signals\n",
    "prd = calculate_prd(ecg_signal['MLII'].values, reconstructed_signal)\n",
    "\n",
    "# Calculate the SNR from the PRD\n",
    "snr = calculate_snr(prd)\n",
    "\n",
    "# Print the PRD and SNR values\n",
    "print(f\"PRD: {prd:.2f}%\")\n",
    "print(f\"SNR: {snr:.2f} dB\")\n",
    "\n",
    "# Plot the original and reconstructed signals\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(ecg_signal['MLII'].values, label='Original Signal', color='blue')\n",
    "plt.plot(reconstructed_signal, label='Reconstructed Signal', linestyle='--', color='red')\n",
    "plt.title('Original vs Reconstructed ECG Signal')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".namlVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
